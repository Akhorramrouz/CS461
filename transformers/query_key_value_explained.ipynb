{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Query, Key, and Value: A Deep Dive\n",
    "\n",
    "Let's understand these concepts through **multiple analogies** and **step-by-step examples**.\n",
    "\n",
    "## The Big Picture\n",
    "\n",
    "**Query, Key, and Value** are three different \"views\" or \"perspectives\" of the same words in a sentence. Think of them as three different questions we ask about each word:\n",
    "\n",
    "- **Query (Q)**: \"What am I looking for?\"\n",
    "- **Key (K)**: \"What information do I contain that others might search for?\"\n",
    "- **Value (V)**: \"What is my actual content that I'll share?\"\n",
    "\n",
    "Let's explore this through several analogies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Analogy 1: The Library System\n",
    "\n",
    "Imagine you're in a library searching for information.\n",
    "\n",
    "### The Scenario\n",
    "You walk into a library and ask: **\"I need information about machine learning\"**\n",
    "\n",
    "Here's how Q, K, V work:\n",
    "\n",
    "| Concept | Library Analogy | What It Does |\n",
    "|---------|----------------|-------------|\n",
    "| **Query** | Your search request: \"machine learning\" | Represents what you're looking for |\n",
    "| **Key** | Book titles/labels on shelves: \"AI Textbook\", \"Neural Networks Guide\", \"Cooking Recipes\" | Help you find relevant books |\n",
    "| **Value** | Actual book contents | The information you actually take away |\n",
    "\n",
    "### The Process\n",
    "\n",
    "1. **You have a Query**: \"machine learning\"\n",
    "2. **You check Keys** (book titles): \n",
    "   - \"AI Textbook\" ‚úÖ (relevant! high match)\n",
    "   - \"Neural Networks Guide\" ‚úÖ (relevant! high match)\n",
    "   - \"Cooking Recipes\" ‚ùå (not relevant, low match)\n",
    "3. **You read the Values** (contents) from books with high matches\n",
    "4. **You combine** the information proportionally:\n",
    "   - 50% from \"AI Textbook\"\n",
    "   - 50% from \"Neural Networks Guide\"\n",
    "   - 0% from \"Cooking Recipes\"\n",
    "\n",
    "### Key Insight\n",
    "**The title (Key) helps you find the book, but the content (Value) is what you actually learn from!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Analogy 2: Google Search\n",
    "\n",
    "This is even more intuitive!\n",
    "\n",
    "### When you search on Google:\n",
    "\n",
    "```\n",
    "You type: \"best pizza in New York\"  ‚Üê This is your QUERY\n",
    "```\n",
    "\n",
    "### What happens:\n",
    "\n",
    "1. **Query (Q)**: Your search term: \"best pizza in New York\"\n",
    "\n",
    "2. **Keys (K)**: Metadata/tags of each webpage:\n",
    "   - Page 1: [\"pizza\", \"New York\", \"restaurant\", \"food\"] ‚úÖ\n",
    "   - Page 2: [\"pizza\", \"Chicago\", \"deep dish\"] ‚ö†Ô∏è\n",
    "   - Page 3: [\"cars\", \"vehicles\", \"New York\"] ‚ùå\n",
    "\n",
    "3. **Matching**: Google compares your Query against Keys:\n",
    "   - Page 1: High relevance (0.9) - has \"pizza\" AND \"New York\"\n",
    "   - Page 2: Medium relevance (0.3) - has \"pizza\" but wrong city\n",
    "   - Page 3: Low relevance (0.1) - only has \"New York\"\n",
    "\n",
    "4. **Values (V)**: Actual content of webpages\n",
    "   - Page 1 Value: \"Joe's Pizza on 42nd Street has amazing slices...\"\n",
    "   - Page 2 Value: \"Chicago deep dish is characterized by...\"\n",
    "   - Page 3 Value: \"The best cars to drive in New York are...\"\n",
    "\n",
    "5. **Final Result**: Google shows you a **weighted combination**:\n",
    "   - 90% of what you see comes from Page 1 (high relevance)\n",
    "   - 9% from Page 2 (medium relevance)\n",
    "   - 1% from Page 3 (low relevance)\n",
    "\n",
    "### The Magic\n",
    "You don't search the actual content (Values) directly - that would be too slow! Instead:\n",
    "- Keys are like **quick summaries** for fast matching\n",
    "- Values are the **full content** you actually want\n",
    "- Query is **what you're looking for**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Analogy 3: Understanding a Sentence (The Real Use Case)\n",
    "\n",
    "Now let's see how this works for actual language.\n",
    "\n",
    "### Sentence: \"The cat sat on the mat\"\n",
    "\n",
    "Let's focus on the word **\"sat\"** and see what it should pay attention to.\n",
    "\n",
    "### For the word \"sat\":\n",
    "\n",
    "**Query (from \"sat\")**: \"I'm a verb. Who is performing me? Where am I happening?\"\n",
    "- This is like \"sat\" asking: \"What should I pay attention to?\"\n",
    "\n",
    "**Keys (from all words)**:\n",
    "- \"The\": \"I'm an article, not very important\" ‚ö™\n",
    "- \"cat\": \"I'm a noun, an ANIMAL, could be a subject of action!\" üü¢\n",
    "- \"sat\": \"That's me\" ‚ö™\n",
    "- \"on\": \"I'm a preposition, showing relationships\" üü°\n",
    "- \"the\": \"I'm an article\" ‚ö™\n",
    "- \"mat\": \"I'm a noun, a LOCATION where things happen!\" üü¢\n",
    "\n",
    "**Matching (Query of \"sat\" against all Keys)**:\n",
    "- \"sat\" √ó \"The\": 0.05 (low match)\n",
    "- \"sat\" √ó \"cat\": 0.50 (high match! verbs care about their subjects)\n",
    "- \"sat\" √ó \"sat\": 0.10 (words do pay some attention to themselves)\n",
    "- \"sat\" √ó \"on\": 0.15 (medium match)\n",
    "- \"sat\" √ó \"the\": 0.05 (low match)\n",
    "- \"sat\" √ó \"mat\": 0.45 (high match! verbs care about locations)\n",
    "\n",
    "**Values (the actual meaning each word contributes)**:\n",
    "- \"The\": [simple article information]\n",
    "- \"cat\": [animal, furry, pet, subject of action]\n",
    "- \"sat\": [action, past tense, resting position]\n",
    "- \"on\": [spatial relationship, above surface]\n",
    "- \"the\": [simple article information]\n",
    "- \"mat\": [object, flat surface, location]\n",
    "\n",
    "**Final representation of \"sat\" after attention**:\n",
    "```\n",
    "New \"sat\" = 0.50 √ó [cat info] + 0.45 √ó [mat info] + 0.10 √ó [sat info] + ...\n",
    "           = \"sitting action performed by a cat on a mat surface\"\n",
    "```\n",
    "\n",
    "The word \"sat\" now has a **richer representation** that includes context about WHO sat and WHERE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Let's visualize this with actual code!\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Let's build intuition with a concrete example!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Concrete Example: Step by Step\n",
    "\n",
    "Let's work through a **tiny example** with actual numbers.\n",
    "\n",
    "### Sentence: \"cat sat\"\n",
    "\n",
    "We'll use 3-dimensional vectors to keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 1: Start with word embeddings (these come from a lookup table)\n",
    "# In real transformers, these are learned 512 or 768 dimensional vectors\n",
    "# We use 3D for visualization\n",
    "\n",
    "embeddings = {\n",
    "    'cat': np.array([1.0, 0.5, 0.2]),  # represents the word \"cat\"\n",
    "    'sat': np.array([0.3, 1.0, 0.8])   # represents the word \"sat\"\n",
    "}\n",
    "\n",
    "print(\"Original Word Embeddings:\")\n",
    "print(\"cat:\", embeddings['cat'])\n",
    "print(\"sat:\", embeddings['sat'])\n",
    "print(\"\\nThese are just vector representations of words.\")\n",
    "print(\"Think of them as coordinates in 'meaning space'\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 2: Create transformation matrices\n",
    "# These are LEARNED during training\n",
    "# They transform embeddings into Query, Key, and Value spaces\n",
    "\n",
    "# Small weight matrices (3x3) for our 3D embeddings\n",
    "W_query = np.array([\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0]\n",
    "]) * 0.5\n",
    "\n",
    "W_key = np.array([\n",
    "    [0.8, 0.2, 0.0],\n",
    "    [0.2, 0.8, 0.0],\n",
    "    [0.0, 0.0, 1.0]\n",
    "]) * 0.5\n",
    "\n",
    "W_value = np.array([\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0]\n",
    "]) * 0.8\n",
    "\n",
    "print(\"Weight Matrices created!\")\n",
    "print(\"These transform embeddings into Q, K, V representations\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 3: Compute Query, Key, Value for each word\n",
    "\n",
    "Q = {}\n",
    "K = {}\n",
    "V = {}\n",
    "\n",
    "for word in ['cat', 'sat']:\n",
    "    embedding = embeddings[word]\n",
    "    \n",
    "    Q[word] = W_query @ embedding  # Matrix multiplication\n",
    "    K[word] = W_key @ embedding\n",
    "    V[word] = W_value @ embedding\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 3: Creating Q, K, V for each word\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for word in ['cat', 'sat']:\n",
    "    print(f\"\\nWord: '{word}'\")\n",
    "    print(f\"  Original embedding: {embeddings[word]}\")\n",
    "    print(f\"  Query (Q):  {Q[word]}  ‚Üê 'What am I looking for?'\")\n",
    "    print(f\"  Key (K):    {K[word]}  ‚Üê 'What do I offer?'\")\n",
    "    print(f\"  Value (V):  {V[word]}  ‚Üê 'What is my content?'\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë The Critical Insight\n",
    "\n",
    "### Why do we need THREE different representations?\n",
    "\n",
    "Each word plays **two roles** simultaneously:\n",
    "\n",
    "1. **As a searcher (Query)**: \"What information do I need from other words?\"\n",
    "2. **As a provider (Key & Value)**: \"What information can I provide to other words?\"\n",
    "\n",
    "#### Example with \"sat\":\n",
    "\n",
    "**When \"sat\" is the QUERY (searching):**\n",
    "- \"I'm a verb. I need to know who performed me (subject) and where (location)\"\n",
    "- So \"sat\"'s Query looks for subjects and locations\n",
    "\n",
    "**When \"sat\" is the KEY (being searched):**\n",
    "- \"I'm an action that happened. Other words might want to know about me.\"\n",
    "- So \"sat\"'s Key advertises: \"I'm an action, a verb, describes what happened\"\n",
    "\n",
    "**When \"sat\" is the VALUE (providing information):**\n",
    "- \"Here's the actual meaning I contribute: past-tense action, sitting position, etc.\"\n",
    "- The Value contains the rich semantic information\n",
    "\n",
    "### The Same Word, Three Perspectives!\n",
    "\n",
    "Think of it like a person at a networking event:\n",
    "- **Query**: \"I'm looking for software engineers\" (what you seek)\n",
    "- **Key**: \"Hi, I'm a software engineer!\" (how you advertise yourself)\n",
    "- **Value**: [Your actual skills, experience, knowledge] (what you offer)\n",
    "\n",
    "You're simultaneously **searching for others** AND **being found by others**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 4: Calculate attention scores\n",
    "# This determines how much each word should attend to every other word\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 4: Computing Attention (Query √ó Key matching)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Let's see how much \"sat\" should attend to each word\n",
    "print(\"\\nFocus: How much should 'sat' pay attention to each word?\")\n",
    "print(\"\\nWe compute: Query('sat') ‚Ä¢ Key(each word)\")\n",
    "print(\"The dot product measures similarity/relevance\\n\")\n",
    "\n",
    "query_sat = Q['sat']\n",
    "\n",
    "# Attention scores: how much 'sat' attends to each word\n",
    "score_sat_to_cat = np.dot(query_sat, K['cat'])\n",
    "score_sat_to_sat = np.dot(query_sat, K['sat'])\n",
    "\n",
    "print(f\"Query('sat') ‚Ä¢ Key('cat') = {score_sat_to_cat:.3f}\")\n",
    "print(f\"Query('sat') ‚Ä¢ Key('sat') = {score_sat_to_sat:.3f}\")\n",
    "print(\"\\nHigher score = more relevant = should pay more attention\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 5: Scale the scores\n",
    "d_k = 3  # dimension of our key vectors\n",
    "scaling_factor = np.sqrt(d_k)\n",
    "\n",
    "scaled_score_sat_to_cat = score_sat_to_cat / scaling_factor\n",
    "scaled_score_sat_to_sat = score_sat_to_sat / scaling_factor\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 5: Scale by ‚àöd_k\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nScaling factor: ‚àö{d_k} = {scaling_factor:.3f}\")\n",
    "print(f\"Scaled score ('sat' ‚Üí 'cat'): {scaled_score_sat_to_cat:.3f}\")\n",
    "print(f\"Scaled score ('sat' ‚Üí 'sat'): {scaled_score_sat_to_sat:.3f}\")\n",
    "print(\"\\nWhy scale? Prevents very large values that make training difficult\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 6: Apply softmax to get attention weights\n",
    "# This converts scores into probabilities that sum to 1\n",
    "\n",
    "def softmax(scores):\n",
    "    exp_scores = np.exp(scores - np.max(scores))  # numerical stability\n",
    "    return exp_scores / exp_scores.sum()\n",
    "\n",
    "attention_scores = np.array([scaled_score_sat_to_cat, scaled_score_sat_to_sat])\n",
    "attention_weights = softmax(attention_scores)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 6: Apply Softmax (convert to probabilities)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAttention weight ('sat' ‚Üí 'cat'): {attention_weights[0]:.3f}\")\n",
    "print(f\"Attention weight ('sat' ‚Üí 'sat'): {attention_weights[1]:.3f}\")\n",
    "print(f\"Sum: {attention_weights.sum():.3f} (must equal 1.0)\")\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(f\"   'sat' should focus {attention_weights[0]*100:.1f}% on 'cat'\")\n",
    "print(f\"   'sat' should focus {attention_weights[1]*100:.1f}% on itself\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 7: Compute weighted sum of Values\n",
    "# This is the final output!\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 7: Compute Weighted Sum of Values\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# New representation of 'sat' after attention\n",
    "new_sat = attention_weights[0] * V['cat'] + attention_weights[1] * V['sat']\n",
    "\n",
    "print(\"\\nOriginal 'sat' embedding:\", embeddings['sat'])\n",
    "print(\"Original 'sat' value:    \", V['sat'])\n",
    "print(\"\\nNEW 'sat' representation:\", new_sat)\n",
    "\n",
    "print(\"\\nüéØ This new representation combines:\")\n",
    "print(f\"   {attention_weights[0]*100:.1f}% information from 'cat': {V['cat']}\")\n",
    "print(f\"   {attention_weights[1]*100:.1f}% information from 'sat': {V['sat']}\")\n",
    "print(\"\\n‚ú® Result: 'sat' now has context about 'cat'!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Visual Summary\n",
    "\n",
    "Let's create a visual representation of the entire process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a comprehensive visualization\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('The Complete Query-Key-Value Process', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Original Embeddings\n",
    "ax1 = axes[0, 0]\n",
    "words = ['cat', 'sat']\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    ax1.bar([0, 1, 2], embeddings[word], alpha=0.7, label=word, color=colors[i], width=0.35 * (i - 0.5))\n",
    "\n",
    "ax1.set_xlabel('Dimension')\n",
    "ax1.set_ylabel('Value')\n",
    "ax1.set_title('1. Original Word Embeddings')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Q, K, V Representations\n",
    "ax2 = axes[0, 1]\n",
    "word = 'sat'\n",
    "x = np.arange(3)\n",
    "width = 0.25\n",
    "\n",
    "ax2.bar(x - width, Q[word], width, label='Query (Q)', alpha=0.8, color='#FF6B6B')\n",
    "ax2.bar(x, K[word], width, label='Key (K)', alpha=0.8, color='#4ECDC4')\n",
    "ax2.bar(x + width, V[word], width, label='Value (V)', alpha=0.8, color='#95E1D3')\n",
    "\n",
    "ax2.set_xlabel('Dimension')\n",
    "ax2.set_ylabel('Value')\n",
    "ax2.set_title('2. Q, K, V for \"sat\"')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Attention Weights\n",
    "ax3 = axes[1, 0]\n",
    "attention_matrix = np.array([attention_weights, [0, 0]])  # Just showing 'sat' row\n",
    "im = ax3.imshow(attention_weights.reshape(1, -1), cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\n",
    "ax3.set_xticks([0, 1])\n",
    "ax3.set_xticklabels(['cat', 'sat'])\n",
    "ax3.set_yticks([0])\n",
    "ax3.set_yticklabels(['sat'])\n",
    "ax3.set_xlabel('Attending TO (Key)')\n",
    "ax3.set_ylabel('Attending FROM (Query)')\n",
    "ax3.set_title('3. Attention Weights')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(2):\n",
    "    ax3.text(i, 0, f'{attention_weights[i]:.2f}', ha='center', va='center', \n",
    "            color='white', fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.colorbar(im, ax=ax3, label='Weight')\n",
    "\n",
    "# Plot 4: Before and After\n",
    "ax4 = axes[1, 1]\n",
    "x_pos = np.arange(3)\n",
    "\n",
    "ax4.bar(x_pos - 0.2, V['sat'], 0.4, label='Before Attention', alpha=0.7, color='#4ECDC4')\n",
    "ax4.bar(x_pos + 0.2, new_sat, 0.4, label='After Attention', alpha=0.7, color='#FF6B6B')\n",
    "\n",
    "ax4.set_xlabel('Dimension')\n",
    "ax4.set_ylabel('Value')\n",
    "ax4.set_title('4. \"sat\" Before vs After Attention')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY TAKEAWAY\")\n",
    "print(\"=\"*60)\n",
    "print(\"The 'sat' vector has been ENRICHED with information from 'cat'\")\n",
    "print(\"This is how words understand their context!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© The Final Puzzle: Why Three Separate Transformations?\n",
    "\n",
    "You might ask: **\"Why can't we just use the original embeddings? Why transform them into Q, K, V?\"**\n",
    "\n",
    "Great question! Here's why:\n",
    "\n",
    "### Without Q, K, V (using just embeddings):\n",
    "```python\n",
    "# If we just did: embedding('sat') ‚Ä¢ embedding('cat')\n",
    "# This would measure general similarity\n",
    "```\n",
    "\n",
    "**Problem**: Words that are similar in meaning might not need to attend to each other!\n",
    "\n",
    "Example: \"happy\" and \"joyful\" are similar, but when understanding a sentence, \"happy\" might need to attend to the **subject** (who is happy?) rather than other emotion words.\n",
    "\n",
    "### With Q, K, V transformations:\n",
    "\n",
    "```python\n",
    "# We do: Query('sat') ‚Ä¢ Key('cat')\n",
    "# This measures TASK-SPECIFIC relevance\n",
    "```\n",
    "\n",
    "**Solution**: The transformations learn to encode **relationships** rather than just similarity!\n",
    "\n",
    "- **Query transformation** learns: \"What relationships should I look for?\"\n",
    "- **Key transformation** learns: \"What relationships do I participate in?\"\n",
    "- **Value transformation** learns: \"What information should I contribute?\"\n",
    "\n",
    "### Concrete Example:\n",
    "\n",
    "Sentence: \"The **cat** sat on the **mat**\"\n",
    "\n",
    "- **Query('sat')** learns to look for:\n",
    "  - Noun patterns (for subjects)\n",
    "  - Location patterns (for where)\n",
    "  \n",
    "- **Key('cat')** learns to advertise:\n",
    "  - \"I'm a noun\"\n",
    "  - \"I can be a subject\"\n",
    "  \n",
    "- **Value('cat')** provides:\n",
    "  - Rich semantic meaning: [animal, furry, pet, small, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Let's Test Your Understanding!\n",
    "\n",
    "### Question 1: Database Analogy\n",
    "In a database, you're looking for \"employees with Python skills\".\n",
    "\n",
    "- What is the Query?\n",
    "- What are the Keys?\n",
    "- What are the Values?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "- **Query**: \"employees with Python skills\" (what you're searching for)\n",
    "- **Keys**: Tags/metadata on each employee record (\"Python\", \"Java\", \"Manager\", etc.)\n",
    "- **Values**: Full employee profiles (name, experience, projects, etc.)\n",
    "\n",
    "</details>\n",
    "\n",
    "### Question 2: Sentence Understanding\n",
    "Sentence: \"The dog chased the cat\"\n",
    "\n",
    "What should the word \"chased\" pay attention to?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "\"chased\" should pay high attention to:\n",
    "- **\"dog\"** (the subject performing the action)\n",
    "- **\"cat\"** (the object receiving the action)\n",
    "\n",
    "Lower attention to:\n",
    "- **\"The\"** (not semantically important)\n",
    "\n",
    "</details>\n",
    "\n",
    "### Question 3: The Key Question\n",
    "If two words have identical embeddings, will they have identical Q, K, and V?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "**YES!** Because Q, K, V are computed by multiplying the embedding by weight matrices:\n",
    "- Q = Embedding √ó W_query\n",
    "- K = Embedding √ó W_key\n",
    "- V = Embedding √ó W_value\n",
    "\n",
    "Same input (embedding) ‚Üí Same output (Q, K, V)\n",
    "\n",
    "However, in practice, even identical words in different positions will have different embeddings after adding positional encoding!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary: The Big Picture\n",
    "\n",
    "### What Happens in Attention (Simple Version):\n",
    "\n",
    "1. **Start**: You have word embeddings\n",
    "2. **Transform**: Create three views (Q, K, V) of each word\n",
    "3. **Compare**: Match Queries against Keys (\"What's relevant?\")\n",
    "4. **Combine**: Mix Values based on relevance weights\n",
    "5. **Result**: Each word now understands its context!\n",
    "\n",
    "### The Genius:\n",
    "\n",
    "- **No fixed rules**: The model **learns** what to pay attention to\n",
    "- **Flexible**: Different attention heads can learn different types of relationships\n",
    "- **Parallel**: All words processed simultaneously (unlike RNNs)\n",
    "- **Contextual**: Each word's representation depends on surrounding words\n",
    "\n",
    "### Remember:\n",
    "\n",
    "üîç **Query**: \"What should I look for?\"\n",
    "üîë **Key**: \"What do I represent for others to find?\"\n",
    "üíé **Value**: \"What do I actually contribute?\"\n",
    "\n",
    "### The Magic Formula:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Breaking it down:\n",
    "- $QK^T$: \"How relevant is each word to each other word?\"\n",
    "- $\\frac{1}{\\sqrt{d_k}}$: \"Scale it down for numerical stability\"\n",
    "- $\\text{softmax}$: \"Convert to probabilities\"\n",
    "- $\\times V$: \"Mix the values based on relevance\"\n",
    "\n",
    "And that's Query, Key, and Value explained! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Bonus: Experiment Yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Try changing these values and see what happens!\n",
    "\n",
    "# Create your own embeddings\n",
    "my_embeddings = {\n",
    "    'word1': np.array([1.0, 0.0, 0.5]),\n",
    "    'word2': np.array([0.5, 1.0, 0.2])\n",
    "}\n",
    "\n",
    "# Create simple weight matrices\n",
    "W_q = np.eye(3) * 0.5\n",
    "W_k = np.eye(3) * 0.5\n",
    "W_v = np.eye(3) * 0.8\n",
    "\n",
    "# Compute Q, K, V\n",
    "Q1 = W_q @ my_embeddings['word1']\n",
    "K1 = W_k @ my_embeddings['word1']\n",
    "K2 = W_k @ my_embeddings['word2']\n",
    "V1 = W_v @ my_embeddings['word1']\n",
    "V2 = W_v @ my_embeddings['word2']\n",
    "\n",
    "# Compute attention\n",
    "score1 = np.dot(Q1, K1)\n",
    "score2 = np.dot(Q1, K2)\n",
    "weights = softmax(np.array([score1, score2]))\n",
    "\n",
    "# Final output\n",
    "output = weights[0] * V1 + weights[1] * V2\n",
    "\n",
    "print(\"Your custom attention result:\")\n",
    "print(f\"Attention weights: {weights}\")\n",
    "print(f\"Output: {output}\")\n",
    "print(\"\\nTry changing the embeddings and see how attention changes!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Further Reading\n",
    "\n",
    "Now that you understand Q, K, V, you're ready to:\n",
    "\n",
    "1. Learn about **multi-head attention** (multiple Q, K, V transformations in parallel)\n",
    "2. Understand **self-attention vs cross-attention**\n",
    "3. Study complete **transformer architecture**\n",
    "4. Explore **real implementations** (BERT, GPT, etc.)\n",
    "\n",
    "You've got this! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
