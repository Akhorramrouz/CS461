{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Guide to Transformer Models ü§ñ\n",
    "\n",
    "## A Beginner-Friendly Tutorial for CS Students\n",
    "\n",
    "Welcome! In this tutorial, you'll learn about three major types of transformer models:\n",
    "1. **Encoder-Only Models** (like BERT)\n",
    "2. **Decoder-Only Models** (like GPT)\n",
    "3. **Encoder-Decoder Models** (like T5)\n",
    "\n",
    "By the end, you'll understand how each works and be able to use them for real tasks!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup: Installing Required Libraries\n",
    "\n",
    "First, let's install the libraries we'll need. Run this cell once at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers torch sentencepiece -q\n",
    "\n",
    "print(\"‚úÖ All libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Now let's import everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BertTokenizer, BertModel, BertForSequenceClassification,\n",
    "    GPT2Tokenizer, GPT2LMHeadModel,\n",
    "    T5Tokenizer, T5ForConditionalGeneration,\n",
    "    pipeline\n",
    ")\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"Using PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Encoder-Only Models (BERT) üîç\n",
    "\n",
    "## What are Encoder-Only Models?\n",
    "\n",
    "**Encoder-only models** read and understand text. They're great at:\n",
    "- Understanding the meaning of sentences\n",
    "- Classification tasks (spam detection, sentiment analysis)\n",
    "- Question answering\n",
    "- Finding similar sentences\n",
    "\n",
    "**Key Feature:** They can look at the *entire* sentence at once (bidirectional attention).\n",
    "\n",
    "**Popular Example:** BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "### How BERT Works:\n",
    "1. Takes in a sentence\n",
    "2. Converts words to numbers (tokenization)\n",
    "3. Processes the entire sentence at once\n",
    "4. Outputs a representation (embedding) that captures the meaning\n",
    "\n",
    "Let's see it in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1.1: Loading BERT and Getting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the BERT tokenizer and model\n",
    "print(\"Loading BERT model...\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "print(\"‚úÖ BERT model loaded!\\n\")\n",
    "\n",
    "# Step 2: Let's encode some text\n",
    "text = \"Transformers are amazing for natural language processing!\"\n",
    "print(f\"Input text: '{text}'\\n\")\n",
    "\n",
    "# Step 3: Tokenize (convert text to numbers)\n",
    "inputs = bert_tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "print(\"Tokenized input IDs:\")\n",
    "print(inputs['input_ids'])\n",
    "print(f\"\\nTokens: {bert_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\\n\")\n",
    "\n",
    "# Step 4: Get BERT's output\n",
    "with torch.no_grad():  # We don't need gradients for inference\n",
    "    outputs = bert_model(**inputs)\n",
    "\n",
    "# Step 5: Extract the embeddings\n",
    "# last_hidden_state contains embeddings for each token\n",
    "embeddings = outputs.last_hidden_state\n",
    "print(f\"Shape of embeddings: {embeddings.shape}\")\n",
    "print(f\"This means: [batch_size=1, sequence_length={embeddings.shape[1]}, hidden_size={embeddings.shape[2]}]\")\n",
    "print(\"\\nüí° Each word now has a 768-dimensional vector that captures its meaning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1.2: Sentiment Analysis with BERT\n",
    "\n",
    "Let's use BERT to classify whether movie reviews are positive or negative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a pre-trained sentiment analysis pipeline (built on BERT)\n",
    "print(\"Loading sentiment analysis model...\")\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "print(\"‚úÖ Model loaded!\\n\")\n",
    "\n",
    "# Test sentences\n",
    "sentences = [\n",
    "    \"This movie was absolutely fantastic! I loved every minute.\",\n",
    "    \"Terrible film. Waste of time and money.\",\n",
    "    \"It was okay, nothing special but not bad either.\",\n",
    "    \"Best movie I've seen this year! Highly recommend!\"\n",
    "]\n",
    "\n",
    "print(\"Analyzing sentiments...\\n\")\n",
    "for sentence in sentences:\n",
    "    result = sentiment_analyzer(sentence)[0]\n",
    "    print(f\"Text: '{sentence}'\")\n",
    "    print(f\"Sentiment: {result['label']} (confidence: {result['score']:.4f})\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1.3: Understanding Sentence Similarity\n",
    "\n",
    "BERT can help us understand which sentences are similar in meaning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def get_sentence_embedding(text):\n",
    "    \"\"\"Get the average embedding for a sentence using BERT\"\"\"\n",
    "    inputs = bert_tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    # Average all token embeddings (mean pooling)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Compare three sentences\n",
    "sentence1 = \"The cat sat on the mat.\"\n",
    "sentence2 = \"A feline rested on the rug.\"\n",
    "sentence3 = \"Python is a programming language.\"\n",
    "\n",
    "# Get embeddings\n",
    "emb1 = get_sentence_embedding(sentence1)\n",
    "emb2 = get_sentence_embedding(sentence2)\n",
    "emb3 = get_sentence_embedding(sentence3)\n",
    "\n",
    "# Calculate similarities\n",
    "sim_1_2 = cosine_similarity(emb1, emb2).item()\n",
    "sim_1_3 = cosine_similarity(emb1, emb3).item()\n",
    "sim_2_3 = cosine_similarity(emb2, emb3).item()\n",
    "\n",
    "print(\"Sentence Similarity Analysis:\\n\")\n",
    "print(f\"Sentence 1: '{sentence1}'\")\n",
    "print(f\"Sentence 2: '{sentence2}'\")\n",
    "print(f\"Sentence 3: '{sentence3}'\\n\")\n",
    "print(f\"Similarity (1 ‚Üî 2): {sim_1_2:.4f} üëç (similar meaning!)\")\n",
    "print(f\"Similarity (1 ‚Üî 3): {sim_1_3:.4f} üëé (different topics)\")\n",
    "print(f\"Similarity (2 ‚Üî 3): {sim_2_3:.4f} üëé (different topics)\")\n",
    "print(\"\\nüí° Sentences 1 and 2 have high similarity because they mean the same thing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways: Encoder-Only Models\n",
    "\n",
    "‚úÖ **Best for:** Understanding and analyzing text\n",
    "\n",
    "‚úÖ **Can see:** The entire input at once (bidirectional)\n",
    "\n",
    "‚úÖ **Common tasks:**\n",
    "- Text classification\n",
    "- Sentiment analysis\n",
    "- Named entity recognition\n",
    "- Question answering\n",
    "- Sentence similarity\n",
    "\n",
    "‚ùå **Not good for:** Generating new text\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Decoder-Only Models (GPT) üìù\n",
    "\n",
    "## What are Decoder-Only Models?\n",
    "\n",
    "**Decoder-only models** are designed to generate text. They're great at:\n",
    "- Writing stories, articles, code\n",
    "- Continuing text from a prompt\n",
    "- Conversational AI\n",
    "- Creative writing\n",
    "\n",
    "**Key Feature:** They can only look at *previous* words when predicting the next word (unidirectional/causal attention).\n",
    "\n",
    "**Popular Example:** GPT (Generative Pre-trained Transformer)\n",
    "\n",
    "### How GPT Works:\n",
    "1. You give it a starting prompt\n",
    "2. It predicts the next word based on previous words\n",
    "3. It adds that word to the sequence\n",
    "4. Repeats until it generates the desired length\n",
    "\n",
    "Let's explore!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2.1: Loading GPT-2 and Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load GPT-2 tokenizer and model\n",
    "print(\"Loading GPT-2 model...\")\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# GPT-2 needs a padding token\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "print(\"‚úÖ GPT-2 model loaded!\\n\")\n",
    "\n",
    "# Step 2: Create a prompt\n",
    "prompt = \"Once upon a time, in a land far away,\"\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "\n",
    "# Step 3: Tokenize the input\n",
    "inputs = gpt_tokenizer(prompt, return_tensors='pt')\n",
    "print(f\"Tokenized input: {inputs['input_ids']}\")\n",
    "print(f\"Tokens: {gpt_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\\n\")\n",
    "\n",
    "# Step 4: Generate text!\n",
    "print(\"Generating text...\\n\")\n",
    "with torch.no_grad():\n",
    "    outputs = gpt_model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=100,  # Maximum length of generated text\n",
    "        num_return_sequences=1,  # Number of different completions\n",
    "        temperature=0.8,  # Creativity (higher = more creative)\n",
    "        do_sample=True,  # Use sampling instead of greedy decoding\n",
    "        top_k=50,  # Consider top 50 tokens\n",
    "        pad_token_id=gpt_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Step 5: Decode and print the generated text\n",
    "generated_text = gpt_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Story:\")\n",
    "print(\"=\" * 80)\n",
    "print(generated_text)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2.2: Multiple Generation Strategies\n",
    "\n",
    "Let's see how different parameters affect text generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Strategy 1: Greedy decoding (always picks most likely word)\n",
    "print(\"\\n1Ô∏è‚É£ GREEDY DECODING (deterministic, safe)\")\n",
    "print(\"-\" * 80)\n",
    "inputs = gpt_tokenizer(prompt, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    outputs = gpt_model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=50,\n",
    "        do_sample=False,  # No sampling, always pick most likely\n",
    "        pad_token_id=gpt_tokenizer.eos_token_id\n",
    "    )\n",
    "print(gpt_tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# Strategy 2: High temperature (more creative/random)\n",
    "print(\"\\n\\n2Ô∏è‚É£ HIGH TEMPERATURE (creative, varied)\")\n",
    "print(\"-\" * 80)\n",
    "with torch.no_grad():\n",
    "    outputs = gpt_model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=50,\n",
    "        do_sample=True,\n",
    "        temperature=1.5,  # High temperature = more random\n",
    "        pad_token_id=gpt_tokenizer.eos_token_id\n",
    "    )\n",
    "print(gpt_tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# Strategy 3: Low temperature (more focused/deterministic)\n",
    "print(\"\\n\\n3Ô∏è‚É£ LOW TEMPERATURE (focused, consistent)\")\n",
    "print(\"-\" * 80)\n",
    "with torch.no_grad():\n",
    "    outputs = gpt_model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.3,  # Low temperature = more deterministic\n",
    "        pad_token_id=gpt_tokenizer.eos_token_id\n",
    "    )\n",
    "print(gpt_tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüí° Notice how temperature affects creativity and coherence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2.3: Interactive Story Generator\n",
    "\n",
    "Create your own story beginnings and see what GPT-2 generates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story(prompt, max_length=100, temperature=0.8):\n",
    "    \"\"\"Generate a story continuation from a prompt\"\"\"\n",
    "    inputs = gpt_tokenizer(prompt, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = gpt_model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=gpt_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return gpt_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Try different story prompts\n",
    "prompts = [\n",
    "    \"In the year 2150, humans discovered\",\n",
    "    \"The mysterious package arrived at midnight, containing\",\n",
    "    \"She opened the old book and found\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Story {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nPrompt: '{prompt}'\\n\")\n",
    "    story = generate_story(prompt, max_length=80)\n",
    "    print(story)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüí° Try changing the prompts above to generate your own stories!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways: Decoder-Only Models\n",
    "\n",
    "‚úÖ **Best for:** Generating new text\n",
    "\n",
    "‚úÖ **Can see:** Only previous words in the sequence (left-to-right)\n",
    "\n",
    "‚úÖ **Common tasks:**\n",
    "- Text generation\n",
    "- Story writing\n",
    "- Code completion\n",
    "- Chatbots\n",
    "- Text completion\n",
    "\n",
    "‚ùå **Not ideal for:** Understanding/analyzing text (though modern large GPT models can do this too!)\n",
    "\n",
    "**Important Parameters:**\n",
    "- `temperature`: Controls randomness (0 = deterministic, higher = more creative)\n",
    "- `top_k`: Only consider the k most likely next tokens\n",
    "- `top_p`: Nucleus sampling - consider tokens whose cumulative probability exceeds p\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Encoder-Decoder Models (T5) üîÑ\n",
    "\n",
    "## What are Encoder-Decoder Models?\n",
    "\n",
    "**Encoder-decoder models** combine the best of both worlds! They:\n",
    "- Use an encoder to understand the input\n",
    "- Use a decoder to generate the output\n",
    "- Are perfect for transformation tasks\n",
    "\n",
    "**Key Feature:** They can understand complex inputs AND generate sophisticated outputs.\n",
    "\n",
    "**Popular Example:** T5 (Text-to-Text Transfer Transformer)\n",
    "\n",
    "### How T5 Works:\n",
    "1. **Encoder** reads and understands the input\n",
    "2. Creates a representation of the input\n",
    "3. **Decoder** uses that representation to generate output\n",
    "4. Everything is framed as a text-to-text task!\n",
    "\n",
    "### Use Cases:\n",
    "- Translation\n",
    "- Summarization\n",
    "- Question answering\n",
    "- Text transformation\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3.1: Loading T5 and Understanding the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load T5 tokenizer and model\n",
    "print(\"Loading T5 model... (this might take a moment)\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "print(\"‚úÖ T5 model loaded!\\n\")\n",
    "\n",
    "# T5 uses task prefixes to know what to do\n",
    "print(\"üìö T5 uses prefixes to understand the task:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"'translate English to German: ' ‚Üí Translation\")\n",
    "print(\"'summarize: ' ‚Üí Summarization\")\n",
    "print(\"'question: ... context: ...' ‚Üí Question Answering\")\n",
    "print(\"'sentiment: ' ‚Üí Sentiment Analysis\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3.2: Text Summarization\n",
    "\n",
    "Let's use T5 to summarize a long piece of text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long article about machine learning\n",
    "article = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence that focuses on the development \n",
    "of algorithms and statistical models that enable computer systems to improve their \n",
    "performance on a specific task through experience. Unlike traditional programming, \n",
    "where explicit instructions are provided, machine learning systems learn patterns from \n",
    "data. There are three main types of machine learning: supervised learning, where the \n",
    "algorithm learns from labeled data; unsupervised learning, where the algorithm finds \n",
    "patterns in unlabeled data; and reinforcement learning, where an agent learns to make \n",
    "decisions by receiving rewards or penalties. Deep learning, a subset of machine learning, \n",
    "uses neural networks with multiple layers to learn hierarchical representations of data. \n",
    "Machine learning has revolutionized many industries, including healthcare, finance, \n",
    "transportation, and entertainment, enabling applications such as disease diagnosis, \n",
    "fraud detection, autonomous vehicles, and recommendation systems.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original Article:\")\n",
    "print(\"=\" * 80)\n",
    "print(article.strip())\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOriginal length: {len(article.split())} words\\n\")\n",
    "\n",
    "# Prepare input for T5 (add task prefix)\n",
    "input_text = \"summarize: \" + article\n",
    "inputs = t5_tokenizer(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "# Generate summary\n",
    "print(\"Generating summary...\\n\")\n",
    "with torch.no_grad():\n",
    "    summary_ids = t5_model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=100,\n",
    "        min_length=30,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,  # Beam search for better quality\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "summary = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(summary)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSummary length: {len(summary.split())} words\")\n",
    "print(f\"Compression ratio: {len(article.split()) / len(summary.split()):.1f}x shorter!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3.3: Translation\n",
    "\n",
    "T5 can translate text between languages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, target_language='German'):\n",
    "    \"\"\"Translate English text to another language using T5\"\"\"\n",
    "    # T5 format: \"translate English to [language]: [text]\"\n",
    "    input_text = f\"translate English to {target_language}: {text}\"\n",
    "    inputs = t5_tokenizer(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = t5_model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test sentences\n",
    "sentences = [\n",
    "    \"Hello, how are you today?\",\n",
    "    \"Machine learning is fascinating.\",\n",
    "    \"I love studying computer science.\"\n",
    "]\n",
    "\n",
    "print(\"English to German Translation:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for sentence in sentences:\n",
    "    translation = translate_text(sentence, 'German')\n",
    "    print(f\"\\nüá¨üáß English:  {sentence}\")\n",
    "    print(f\"üá©üá™ German:   {translation}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nüí° T5 learned translation during pre-training on multilingual data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3.4: Question Answering\n",
    "\n",
    "T5 can answer questions based on provided context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context):\n",
    "    \"\"\"Answer a question given context using T5\"\"\"\n",
    "    # T5 format for QA\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    inputs = t5_tokenizer(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = t5_model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_length=50,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Context paragraph\n",
    "context = \"\"\"\n",
    "Python is a high-level, interpreted programming language created by Guido van Rossum \n",
    "and first released in 1991. It emphasizes code readability and simplicity, making it \n",
    "an excellent choice for beginners. Python supports multiple programming paradigms, \n",
    "including procedural, object-oriented, and functional programming. It has a large \n",
    "standard library and is widely used in web development, data science, artificial \n",
    "intelligence, scientific computing, and automation.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Context:\")\n",
    "print(\"=\" * 80)\n",
    "print(context.strip())\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Questions to ask\n",
    "questions = [\n",
    "    \"Who created Python?\",\n",
    "    \"When was Python first released?\",\n",
    "    \"What programming paradigms does Python support?\",\n",
    "    \"What is Python used for?\"\n",
    "]\n",
    "\n",
    "print(\"Question Answering:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for question in questions:\n",
    "    answer = answer_question(question, context)\n",
    "    print(f\"\\n‚ùì Q: {question}\")\n",
    "    print(f\"‚úÖ A: {answer}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nüí° T5 extracts and generates answers from the context!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3.5: Multiple Tasks Showcase\n",
    "\n",
    "Let's demonstrate T5's versatility by running multiple different tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_task(task_prefix, text, max_length=100):\n",
    "    \"\"\"Generic function to run any T5 task\"\"\"\n",
    "    input_text = f\"{task_prefix} {text}\"\n",
    "    inputs = t5_tokenizer(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = t5_model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"T5: One Model, Multiple Tasks! üöÄ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Task 1: Grammar correction\n",
    "print(\"\\n1Ô∏è‚É£ GRAMMAR CORRECTION\")\n",
    "print(\"-\" * 80)\n",
    "bad_grammar = \"She don't likes apples and oranges very much.\"\n",
    "corrected = t5_task(\"grammar:\", bad_grammar)\n",
    "print(f\"Original:  {bad_grammar}\")\n",
    "print(f\"Corrected: {corrected}\")\n",
    "\n",
    "# Task 2: Sentiment analysis\n",
    "print(\"\\n2Ô∏è‚É£ SENTIMENT ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "text = \"This product exceeded my expectations! Amazing quality.\"\n",
    "sentiment = t5_task(\"sentiment:\", text, max_length=10)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "\n",
    "# Task 3: Paraphrasing\n",
    "print(\"\\n3Ô∏è‚É£ PARAPHRASING\")\n",
    "print(\"-\" * 80)\n",
    "original = \"The quick brown fox jumps over the lazy dog.\"\n",
    "paraphrase = t5_task(\"paraphrase:\", original)\n",
    "print(f\"Original:   {original}\")\n",
    "print(f\"Paraphrase: {paraphrase}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüí° T5 treats everything as a text-to-text transformation!\")\n",
    "print(\"   Just change the task prefix to change the task!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways: Encoder-Decoder Models\n",
    "\n",
    "‚úÖ **Best for:** Transformation tasks (input ‚Üí output)\n",
    "\n",
    "‚úÖ **Architecture:**\n",
    "- **Encoder** understands the input (bidirectional attention)\n",
    "- **Decoder** generates the output (causal attention + cross-attention to encoder)\n",
    "\n",
    "‚úÖ **Common tasks:**\n",
    "- Translation\n",
    "- Summarization\n",
    "- Question answering\n",
    "- Paraphrasing\n",
    "- Grammar correction\n",
    "- Any text-to-text transformation!\n",
    "\n",
    "‚úÖ **Advantages:**\n",
    "- Combines understanding and generation\n",
    "- Great for complex transformations\n",
    "- Versatile (one model, many tasks)\n",
    "\n",
    "**T5's Special Feature:** Everything is framed as text-to-text, making it extremely flexible!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Final Comparison: Which Model When?\n",
    "\n",
    "## Quick Reference Guide\n",
    "\n",
    "| Model Type | Architecture | Best For | Example Models | Can Generate? | Can Understand? |\n",
    "|------------|--------------|----------|----------------|---------------|------------------|\n",
    "| **Encoder-Only** | Bidirectional | Understanding & Analysis | BERT, RoBERTa | ‚ùå No | ‚úÖ Yes |\n",
    "| **Decoder-Only** | Causal (Left-to-right) | Text Generation | GPT, GPT-2, GPT-3 | ‚úÖ Yes | ‚ö†Ô∏è Limited |\n",
    "| **Encoder-Decoder** | Both | Transformation Tasks | T5, BART | ‚úÖ Yes | ‚úÖ Yes |\n",
    "\n",
    "## Decision Tree üå≥\n",
    "\n",
    "```\n",
    "What's your task?\n",
    "‚îÇ\n",
    "‚îú‚îÄ Need to UNDERSTAND text?\n",
    "‚îÇ  ‚îú‚îÄ Classification ‚Üí Encoder-Only (BERT)\n",
    "‚îÇ  ‚îú‚îÄ Similarity ‚Üí Encoder-Only (BERT)\n",
    "‚îÇ  ‚îî‚îÄ Analysis ‚Üí Encoder-Only (BERT)\n",
    "‚îÇ\n",
    "‚îú‚îÄ Need to GENERATE text?\n",
    "‚îÇ  ‚îú‚îÄ Creative writing ‚Üí Decoder-Only (GPT)\n",
    "‚îÇ  ‚îú‚îÄ Continuation ‚Üí Decoder-Only (GPT)\n",
    "‚îÇ  ‚îî‚îÄ Chatbot ‚Üí Decoder-Only (GPT)\n",
    "‚îÇ\n",
    "‚îî‚îÄ Need to TRANSFORM text?\n",
    "   ‚îú‚îÄ Translation ‚Üí Encoder-Decoder (T5)\n",
    "   ‚îú‚îÄ Summarization ‚Üí Encoder-Decoder (T5)\n",
    "   ‚îî‚îÄ Question Answering ‚Üí Encoder-Decoder (T5)\n",
    "```\n",
    "\n",
    "## Real-World Examples üåç\n",
    "\n",
    "### Encoder-Only (BERT)\n",
    "- üìß Email spam detection\n",
    "- üòä Sentiment analysis in product reviews\n",
    "- üè∑Ô∏è Named entity recognition\n",
    "- üîç Semantic search engines\n",
    "\n",
    "### Decoder-Only (GPT)\n",
    "- ‚úçÔ∏è Content creation (blogs, articles)\n",
    "- üí¨ Conversational AI assistants\n",
    "- üíª Code generation and completion\n",
    "- üìñ Story and creative writing\n",
    "\n",
    "### Encoder-Decoder (T5)\n",
    "- üåê Language translation\n",
    "- üìÑ Document summarization\n",
    "- ‚ùì Question answering systems\n",
    "- ‚úèÔ∏è Grammar and style correction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéâ Congratulations!\n",
    "\n",
    "You've completed the transformer models tutorial! Here's what you learned:\n",
    "\n",
    "## ‚úÖ Key Concepts Mastered:\n",
    "\n",
    "1. **Encoder-Only Models (BERT)**\n",
    "   - Bidirectional understanding\n",
    "   - Text embeddings\n",
    "   - Classification and analysis tasks\n",
    "\n",
    "2. **Decoder-Only Models (GPT)**\n",
    "   - Autoregressive generation\n",
    "   - Temperature and sampling strategies\n",
    "   - Creative text generation\n",
    "\n",
    "3. **Encoder-Decoder Models (T5)**\n",
    "   - Text-to-text framework\n",
    "   - Translation and summarization\n",
    "   - Multi-task learning\n",
    "\n",
    "## üöÄ Next Steps:\n",
    "\n",
    "1. **Experiment:** Modify the code examples with your own text!\n",
    "2. **Explore:** Try different model sizes (e.g., `bert-large`, `gpt2-medium`, `t5-base`)\n",
    "3. **Build:** Create your own application using these models\n",
    "4. **Learn More:** Check out Hugging Face documentation at https://huggingface.co/docs\n",
    "\n",
    "## üìö Additional Resources:\n",
    "\n",
    "- Hugging Face Transformers: https://huggingface.co/transformers/\n",
    "- \"Attention Is All You Need\" paper (original Transformer)\n",
    "- BERT paper: \"BERT: Pre-training of Deep Bidirectional Transformers\"\n",
    "- GPT papers: GPT, GPT-2, GPT-3\n",
    "- T5 paper: \"Exploring the Limits of Transfer Learning\"\n",
    "\n",
    "## üí° Pro Tips:\n",
    "\n",
    "- Start with small models for experimentation (faster and less memory)\n",
    "- Use GPU acceleration for faster inference (if available)\n",
    "- Read the model cards on Hugging Face for capabilities and limitations\n",
    "- Fine-tune models on your specific data for better performance\n",
    "\n",
    "---\n",
    "\n",
    "### Happy coding! üéà\n",
    "\n",
    "*Remember: These models are tools. Understanding when and how to use each one is the real skill!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Practice Exercises\n",
    "\n",
    "Try these challenges to test your understanding:\n",
    "\n",
    "### Beginner:\n",
    "1. Modify the BERT sentiment analyzer to analyze your own sentences\n",
    "2. Change the GPT-2 temperature and observe the differences in generation\n",
    "3. Use T5 to summarize your favorite news article\n",
    "\n",
    "### Intermediate:\n",
    "4. Create a function that classifies movie reviews as positive/negative using BERT\n",
    "5. Build a story generator that takes a genre as input and generates appropriate stories\n",
    "6. Translate sentences from English to multiple languages using T5\n",
    "\n",
    "### Advanced:\n",
    "7. Compare embeddings from BERT for synonyms vs. unrelated words\n",
    "8. Implement beam search manually for GPT-2 generation\n",
    "9. Fine-tune T5 on a custom dataset (requires additional data)\n",
    "\n",
    "Use the cells below to work on these exercises!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your practice code here!\n",
    "# Try out the exercises above or experiment with your own ideas\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
