{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Transformers: A Deep Dive into Attention Mechanisms\n",
    "\n",
    "Welcome! This tutorial will help you understand how transformers work, with a special focus on the **attention mechanism** - the core innovation that makes transformers so powerful.\n",
    "\n",
    "## What You'll Learn\n",
    "1. Why transformers were invented\n",
    "2. The attention mechanism (in detail!)\n",
    "3. Self-attention step by step\n",
    "4. Multi-head attention\n",
    "5. Building a simple transformer from scratch\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install numpy matplotlib torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Why Do We Need Attention?\n",
    "\n",
    "Before transformers, we used **Recurrent Neural Networks (RNNs)** for sequence tasks. But RNNs had problems:\n",
    "\n",
    "1. **Sequential Processing**: RNNs process words one at a time, making them slow\n",
    "2. **Memory Issues**: They forget information from earlier in long sequences\n",
    "3. **No Parallelization**: Can't process multiple words simultaneously\n",
    "\n",
    "**Attention** solves these problems by letting the model look at ALL words at once and decide which ones are important!\n",
    "\n",
    "### Intuition: How Humans Read\n",
    "\n",
    "Consider this sentence: \"The animal didn't cross the street because **it** was too tired.\"\n",
    "\n",
    "When you read \"it\", your brain automatically knows it refers to \"animal\" (not \"street\"). You **attend** to the relevant word. That's what attention mechanisms do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding Attention Step by Step\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "Attention asks three questions for each word:\n",
    "1. **Query (Q)**: What am I looking for?\n",
    "2. **Key (K)**: What do I contain?\n",
    "3. **Value (V)**: What do I actually represent?\n",
    "\n",
    "Think of it like a database:\n",
    "- **Query**: Your search term\n",
    "- **Key**: The index that helps you find relevant items\n",
    "- **Value**: The actual data you retrieve\n",
    "\n",
    "### The Attention Formula\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Let's break this down with a simple example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Let's start with a simple example: \"The cat sat\"\n",
    "# We'll represent each word with a vector (in practice, these come from embeddings)\n",
    "\n",
    "sentence = [\"The\", \"cat\", \"sat\"]\n",
    "vocab_size = len(sentence)\n",
    "d_model = 4  # dimension of our word vectors (kept small for clarity)\n",
    "\n",
    "# Create simple word embeddings (random for demonstration)\n",
    "embeddings = np.random.randn(vocab_size, d_model)\n",
    "\n",
    "print(\"Our sentence:\", sentence)\n",
    "print(\"\\nWord embeddings (each word is a\", d_model, \"dimensional vector):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"{word}: {embeddings[i]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Query, Key, and Value matrices\n",
    "\n",
    "We create Q, K, and V by multiplying our embeddings with learned weight matrices.\n",
    "\n",
    "Think of these weight matrices as \"projections\" that transform our words into different representations for different purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize weight matrices (in practice, these are learned during training)\n",
    "d_k = d_model  # dimension of keys and queries\n",
    "d_v = d_model  # dimension of values\n",
    "\n",
    "W_q = np.random.randn(d_model, d_k)  # Query weight matrix\n",
    "W_k = np.random.randn(d_model, d_k)  # Key weight matrix\n",
    "W_v = np.random.randn(d_model, d_v)  # Value weight matrix\n",
    "\n",
    "# Compute Q, K, V for all words\n",
    "Q = embeddings @ W_q  # (3, 4) @ (4, 4) = (3, 4)\n",
    "K = embeddings @ W_k\n",
    "V = embeddings @ W_v\n",
    "\n",
    "print(\"Query matrix Q (shape:\", Q.shape, \")\")\n",
    "print(Q)\n",
    "print(\"\\nKey matrix K (shape:\", K.shape, \")\")\n",
    "print(K)\n",
    "print(\"\\nValue matrix V (shape:\", V.shape, \")\")\n",
    "print(V)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate Attention Scores\n",
    "\n",
    "Now we compute how much each word should \"attend\" to every other word.\n",
    "\n",
    "We do this by taking the dot product of Q and K^T. This measures similarity:\n",
    "- High score = words are related\n",
    "- Low score = words are not related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate attention scores: Q @ K^T\n",
    "attention_scores = Q @ K.T  # (3, 4) @ (4, 3) = (3, 3)\n",
    "\n",
    "print(\"Raw attention scores:\")\n",
    "print(attention_scores)\n",
    "print(\"\\nShape:\", attention_scores.shape)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Row i, Column j: How much word i attends to word j\")\n",
    "print(f\"- For example, '{sentence[1]}' attending to '{sentence[0]}': {attention_scores[1, 0]:.2f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Scale the Scores\n",
    "\n",
    "We divide by âˆšd_k to prevent the dot products from getting too large.\n",
    "\n",
    "**Why?** Large values push the softmax function into regions with tiny gradients, making training difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Scale by square root of dimension\n",
    "scaled_scores = attention_scores / np.sqrt(d_k)\n",
    "\n",
    "print(\"Scaled attention scores:\")\n",
    "print(scaled_scores)\n",
    "print(\"\\nScaling factor (âˆšd_k):\", np.sqrt(d_k))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Apply Softmax\n",
    "\n",
    "Softmax converts scores into probabilities (they sum to 1).\n",
    "\n",
    "This gives us **attention weights** - how much focus each word gets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Apply softmax to each row\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # subtract max for numerical stability\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "attention_weights = softmax(scaled_scores)\n",
    "\n",
    "print(\"Attention weights:\")\n",
    "print(attention_weights)\n",
    "print(\"\\nEach row sums to 1:\", attention_weights.sum(axis=1))\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attention_weights, cmap='Blues', aspect='auto')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xticks(range(len(sentence)), sentence)\n",
    "plt.yticks(range(len(sentence)), sentence)\n",
    "plt.xlabel('Key (attending TO)')\n",
    "plt.ylabel('Query (attending FROM)')\n",
    "plt.title('Attention Weight Visualization')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(sentence)):\n",
    "    for j in range(len(sentence)):\n",
    "        plt.text(j, i, f'{attention_weights[i, j]:.2f}', \n",
    "                ha='center', va='center', color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Compute Weighted Sum\n",
    "\n",
    "Finally, we multiply attention weights by the Value matrix.\n",
    "\n",
    "This creates a new representation for each word that incorporates information from all other words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Multiply attention weights by values\n",
    "output = attention_weights @ V  # (3, 3) @ (3, 4) = (3, 4)\n",
    "\n",
    "print(\"Output after attention:\")\n",
    "print(output)\n",
    "print(\"\\nShape:\", output.shape)\n",
    "print(\"\\nNotice: Each word now has a new representation that combines information from all words!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Self-Attention Implementation\n",
    "\n",
    "Now let's put it all together in a clean function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix (batch_size, seq_len, d_k)\n",
    "        K: Key matrix (batch_size, seq_len, d_k)\n",
    "        V: Value matrix (batch_size, seq_len, d_v)\n",
    "        mask: Optional mask (batch_size, seq_len, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output (batch_size, seq_len, d_v)\n",
    "        attention_weights: Attention weights (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Step 1: Compute attention scores\n",
    "    scores = Q @ K.transpose(-2, -1)  # (batch, seq_len, seq_len)\n",
    "    \n",
    "    # Step 2: Scale\n",
    "    scores = scores / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        scores = np.where(mask == 0, -1e9, scores)\n",
    "    \n",
    "    # Step 4: Softmax\n",
    "    attention_weights = softmax(scores)\n",
    "    \n",
    "    # Step 5: Weighted sum\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test our function\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "print(\"Output from our attention function:\")\n",
    "print(output)\n",
    "print(\"\\nAttention weights:\")\n",
    "print(weights)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multi-Head Attention\n",
    "\n",
    "**Key Insight**: Different words might be related in different ways!\n",
    "\n",
    "Examples:\n",
    "- Grammatical relationships (subject-verb)\n",
    "- Semantic relationships (synonyms, antonyms)\n",
    "- Positional relationships (nearby words)\n",
    "\n",
    "**Multi-head attention** runs attention multiple times in parallel, each learning different types of relationships.\n",
    "\n",
    "### How it works:\n",
    "1. Split Q, K, V into multiple \"heads\"\n",
    "2. Run attention independently on each head\n",
    "3. Concatenate the results\n",
    "4. Apply a final linear transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Multi-head attention layer.\n",
    "        \n",
    "        Args:\n",
    "            d_model: Dimension of the model\n",
    "            num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # dimension per head\n",
    "        \n",
    "        # Weight matrices for all heads (combined)\n",
    "        self.W_q = np.random.randn(d_model, d_model) * 0.01\n",
    "        self.W_k = np.random.randn(d_model, d_model) * 0.01\n",
    "        self.W_v = np.random.randn(d_model, d_model) * 0.01\n",
    "        self.W_o = np.random.randn(d_model, d_model) * 0.01  # output projection\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, d_k).\n",
    "        \n",
    "        Input shape: (batch_size, seq_len, d_model)\n",
    "        Output shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        x = x.reshape(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, d_k)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        Combine heads back into single dimension.\n",
    "        \n",
    "        Input shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        Output shape: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, num_heads, seq_len, d_k = x.shape\n",
    "        x = x.transpose(0, 2, 1, 3)  # (batch_size, seq_len, num_heads, d_k)\n",
    "        return x.reshape(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of multi-head attention.\n",
    "        \n",
    "        Args:\n",
    "            x: Input (batch_size, seq_len, d_model)\n",
    "            mask: Optional mask\n",
    "        \n",
    "        Returns:\n",
    "            output: Attention output (batch_size, seq_len, d_model)\n",
    "            attention_weights: Weights from all heads\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = x @ self.W_q  # (batch_size, seq_len, d_model)\n",
    "        K = x @ self.W_k\n",
    "        V = x @ self.W_v\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = self.split_heads(Q)  # (batch_size, num_heads, seq_len, d_k)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # Apply attention to each head\n",
    "        attention_outputs = []\n",
    "        attention_weights_list = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            output, weights = scaled_dot_product_attention(\n",
    "                Q[:, i, :, :], K[:, i, :, :], V[:, i, :, :], mask\n",
    "            )\n",
    "            attention_outputs.append(output)\n",
    "            attention_weights_list.append(weights)\n",
    "        \n",
    "        # Stack outputs from all heads\n",
    "        attention_output = np.stack(attention_outputs, axis=1)  # (batch, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # Combine heads\n",
    "        output = self.combine_heads(attention_output)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = output @ self.W_o\n",
    "        \n",
    "        return output, attention_weights_list\n",
    "\n",
    "# Test multi-head attention\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "seq_len = 3\n",
    "batch_size = 1\n",
    "\n",
    "# Create input (batch_size, seq_len, d_model)\n",
    "x = np.random.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "output, attention_weights = mha.forward(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nNumber of attention heads: {num_heads}\")\n",
    "print(f\"Dimension per head: {mha.d_k}\")\n",
    "\n",
    "# Visualize attention from different heads\n",
    "fig, axes = plt.subplots(1, num_heads, figsize=(12, 4))\n",
    "for i in range(num_heads):\n",
    "    ax = axes[i] if num_heads > 1 else axes\n",
    "    im = ax.imshow(attention_weights[i][0], cmap='Blues', aspect='auto')\n",
    "    ax.set_title(f'Head {i+1}')\n",
    "    ax.set_xlabel('Key')\n",
    "    ax.set_ylabel('Query')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Attention Patterns from Different Heads', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: Different heads learn different attention patterns!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Building a Complete Transformer Block\n",
    "\n",
    "A transformer block consists of:\n",
    "1. **Multi-head attention layer**\n",
    "2. **Add & Normalize** (residual connection + layer normalization)\n",
    "3. **Feed-forward network** (2 linear layers with activation)\n",
    "4. **Add & Normalize** (another residual connection)\n",
    "\n",
    "Let's implement this using PyTorch for cleaner code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        A single transformer block.\n",
    "        \n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            d_ff: Dimension of feed-forward network\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ff_network = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Multi-head attention with residual connection\n",
    "        attn_output, _ = self.attention(x, x, x, attn_mask=mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward network with residual connection\n",
    "        ff_output = self.ff_network(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test the transformer block\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "d_ff = 256\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "# Create random input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Create transformer block\n",
    "transformer_block = TransformerBlock(d_model, num_heads, d_ff)\n",
    "\n",
    "# Forward pass\n",
    "output = transformer_block(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nTransformer block parameters:\")\n",
    "print(f\"  - Model dimension (d_model): {d_model}\")\n",
    "print(f\"  - Number of heads: {num_heads}\")\n",
    "print(f\"  - Feed-forward dimension: {d_ff}\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in transformer_block.parameters()):,}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Positional Encoding\n",
    "\n",
    "**Problem**: Attention has no sense of word order!\n",
    "\n",
    "\"The cat sat\" and \"Sat cat the\" would look identical to pure attention.\n",
    "\n",
    "**Solution**: Add positional information to word embeddings.\n",
    "\n",
    "We use sine and cosine functions at different frequencies:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def get_positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Generate positional encodings.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Sequence length\n",
    "        d_model: Model dimension\n",
    "    \n",
    "    Returns:\n",
    "        Positional encodings (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    position = np.arange(seq_len)[:, np.newaxis]  # (seq_len, 1)\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    \n",
    "    pos_encoding = np.zeros((seq_len, d_model))\n",
    "    pos_encoding[:, 0::2] = np.sin(position * div_term)  # even indices\n",
    "    pos_encoding[:, 1::2] = np.cos(position * div_term)  # odd indices\n",
    "    \n",
    "    return pos_encoding\n",
    "\n",
    "# Visualize positional encodings\n",
    "seq_len = 50\n",
    "d_model = 128\n",
    "\n",
    "pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(pos_encoding, cmap='RdBu', aspect='auto')\n",
    "plt.colorbar(label='Encoding Value')\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Positional Encoding Visualization')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Positional encoding shape: {pos_encoding.shape}\")\n",
    "print(\"\\nNotice the wavelike patterns! Each position gets a unique encoding.\")\n",
    "\n",
    "# Show how positions differ\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in [0, 10, 20, 30, 40]:\n",
    "    plt.plot(pos_encoding[i, :50], label=f'Position {i}')\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Positional Encodings for Different Positions (first 50 dims)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Putting It All Together - Complete Transformer\n",
    "\n",
    "Now let's build a complete transformer model with:\n",
    "1. Input embedding\n",
    "2. Positional encoding\n",
    "3. Multiple transformer blocks\n",
    "4. Output projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout=0.1):\n",
    "        \"\"\"\n",
    "        A simple transformer model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            num_layers: Number of transformer blocks\n",
    "            d_ff: Feed-forward dimension\n",
    "            max_seq_len: Maximum sequence length\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Token embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding (fixed, not learned)\n",
    "        self.register_buffer('pos_encoding', \n",
    "                           torch.FloatTensor(get_positional_encoding(max_seq_len, d_model)))\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input token indices (batch_size, seq_len)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            Output logits (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Embed tokens and scale\n",
    "        x = self.embedding(x) * np.sqrt(self.d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x + self.pos_encoding[:seq_len, :]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.output_layer(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create a small transformer\n",
    "vocab_size = 1000\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "d_ff = 256\n",
    "max_seq_len = 100\n",
    "\n",
    "model = SimpleTransformer(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len)\n",
    "\n",
    "# Test with random input\n",
    "batch_size = 2\n",
    "seq_len = 20\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "output = model(x)\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Count parameters by component\n",
    "print(\"\\nParameters by component:\")\n",
    "print(f\"  Embedding: {model.embedding.weight.numel():,}\")\n",
    "print(f\"  Transformer blocks: {sum(p.numel() for block in model.transformer_blocks for p in block.parameters()):,}\")\n",
    "print(f\"  Output layer: {model.output_layer.weight.numel() + model.output_layer.bias.numel():,}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Real-World Example - Attention Visualization\n",
    "\n",
    "Let's see how attention works on a real sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def visualize_attention_pattern(sentence, attention_weights, head_idx=0):\n",
    "    \"\"\"\n",
    "    Visualize attention pattern for a sentence.\n",
    "    \n",
    "    Args:\n",
    "        sentence: List of words\n",
    "        attention_weights: Attention weights tensor\n",
    "        head_idx: Which attention head to visualize\n",
    "    \"\"\"\n",
    "    # Get attention for specified head\n",
    "    attn = attention_weights[head_idx].detach().numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    im = ax.imshow(attn, cmap='Blues', aspect='auto')\n",
    "    \n",
    "    # Set ticks and labels\n",
    "    ax.set_xticks(range(len(sentence)))\n",
    "    ax.set_yticks(range(len(sentence)))\n",
    "    ax.set_xticklabels(sentence, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(sentence)\n",
    "    \n",
    "    ax.set_xlabel('Attending TO (Key)')\n",
    "    ax.set_ylabel('Attending FROM (Query)')\n",
    "    ax.set_title(f'Attention Pattern (Head {head_idx})')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Attention Weight')\n",
    "    \n",
    "    # Add value annotations\n",
    "    for i in range(len(sentence)):\n",
    "        for j in range(len(sentence)):\n",
    "            text = ax.text(j, i, f'{attn[i, j]:.2f}',\n",
    "                         ha='center', va='center', \n",
    "                         color='red' if attn[i, j] > 0.3 else 'black',\n",
    "                         fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create a simple example\n",
    "example_sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "seq_len = len(example_sentence)\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "\n",
    "# Create simple embeddings\n",
    "embeddings = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "# Create attention layer\n",
    "attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "\n",
    "# Get attention weights\n",
    "with torch.no_grad():\n",
    "    _, attention_weights = attention(embeddings, embeddings, embeddings, average_attn_weights=False)\n",
    "\n",
    "# Visualize different heads\n",
    "print(\"Attention Patterns from Different Heads\")\n",
    "print(\"Notice how different heads focus on different relationships!\\n\")\n",
    "\n",
    "for head_idx in range(min(2, num_heads)):\n",
    "    visualize_attention_pattern(example_sentence, attention_weights[0], head_idx)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Key Takeaways & Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Attention Mechanism**\n",
    "   - Allows model to focus on relevant parts of input\n",
    "   - Uses Query, Key, Value paradigm\n",
    "   - Formula: Attention(Q,K,V) = softmax(QK^T / âˆšd_k)V\n",
    "\n",
    "2. **Self-Attention**\n",
    "   - Each word attends to all words (including itself)\n",
    "   - Captures relationships between words\n",
    "   - Parallel processing (unlike RNNs)\n",
    "\n",
    "3. **Multi-Head Attention**\n",
    "   - Multiple attention mechanisms in parallel\n",
    "   - Each head learns different relationships\n",
    "   - More expressive than single attention\n",
    "\n",
    "4. **Transformer Block**\n",
    "   - Multi-head attention + Feed-forward network\n",
    "   - Residual connections + Layer normalization\n",
    "   - Stack multiple blocks for deeper models\n",
    "\n",
    "5. **Positional Encoding**\n",
    "   - Adds word order information\n",
    "   - Sine/cosine functions at different frequencies\n",
    "   - Added to input embeddings\n",
    "\n",
    "### Why Transformers Are Powerful:\n",
    "\n",
    "âœ… **Parallelization**: Process all words simultaneously\n",
    "âœ… **Long-range dependencies**: Can attend to any word regardless of distance\n",
    "âœ… **Flexibility**: Same architecture for many tasks (translation, generation, etc.)\n",
    "âœ… **Scalability**: Can be trained on massive datasets\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Study encoder-decoder architecture (for translation)\n",
    "2. Learn about different transformer variants (BERT, GPT, T5)\n",
    "3. Understand training techniques (learning rate schedules, warmup)\n",
    "4. Explore applications (NLP, vision transformers, protein folding)\n",
    "\n",
    "### Recommended Resources:\n",
    "\n",
    "- \"Attention Is All You Need\" paper (Vaswani et al., 2017)\n",
    "- The Illustrated Transformer (Jay Alammar)\n",
    "- Stanford CS224N: Natural Language Processing\n",
    "- Hugging Face Transformers library documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Interactive Exercise\n",
    "\n",
    "Try modifying the code to experiment with different configurations!\n",
    "\n",
    "Ideas to explore:\n",
    "1. Change the number of attention heads - what happens?\n",
    "2. Modify the dimension of the model (d_model)\n",
    "3. Add more transformer blocks\n",
    "4. Try different sentences and observe attention patterns\n",
    "5. Implement masking for decoder (prevent attending to future tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR EXPERIMENTS HERE!\n",
    "# Try changing these parameters:\n",
    "\n",
    "d_model = 64          # Try: 32, 64, 128, 256\n",
    "num_heads = 4         # Try: 1, 2, 4, 8 (must divide d_model)\n",
    "num_layers = 2        # Try: 1, 2, 4, 6\n",
    "d_ff = 256           # Try: 128, 256, 512, 1024\n",
    "\n",
    "# Build and test your model!\n",
    "vocab_size = 1000\n",
    "max_seq_len = 100\n",
    "\n",
    "custom_model = SimpleTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_seq_len=max_seq_len\n",
    ")\n",
    "\n",
    "print(f\"Custom model parameters: {sum(p.numel() for p in custom_model.parameters()):,}\")\n",
    "\n",
    "# Test it!\n",
    "test_input = torch.randint(0, vocab_size, (1, 10))\n",
    "test_output = custom_model(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! ðŸŽ‰\n",
    "\n",
    "You now understand how transformers work, especially the attention mechanism!\n",
    "\n",
    "Remember:\n",
    "- **Attention** is about focusing on relevant information\n",
    "- **Self-attention** lets words interact with each other\n",
    "- **Multi-head attention** learns multiple types of relationships\n",
    "- **Transformers** stack these mechanisms to build powerful models\n",
    "\n",
    "Keep learning and experimenting! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
