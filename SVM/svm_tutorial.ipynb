{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complete Guide to Support Vector Machines (SVM)\n",
        "\n",
        "This notebook provides an intuitive understanding of SVMs, covering:\n",
        "- Hard Margin SVM\n",
        "- Soft Margin SVM and Loss Functions\n",
        "- Kernel Trick\n",
        "- Practical Examples\n",
        "\n",
        "Let's start by importing necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction: What is SVM?\n",
        "\n",
        "**Support Vector Machine (SVM)** is a powerful supervised learning algorithm used for classification and regression. The main idea:\n",
        "\n",
        "- Find the **hyperplane** that best separates different classes\n",
        "- Maximize the **margin** (distance) between the hyperplane and the closest data points from each class\n",
        "- These closest points are called **support vectors**\n",
        "\n",
        "### Visual Intuition\n",
        "Imagine you have red and blue balls on a table. SVM tries to draw a line (in 2D) or plane (in higher dimensions) that:\n",
        "1. Separates the colors\n",
        "2. Is as far as possible from both groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple linearly separable dataset\n",
        "np.random.seed(42)\n",
        "\n",
        "# Class 1 (red)\n",
        "X1 = np.random.randn(20, 2) + np.array([2, 2])\n",
        "y1 = np.zeros(20)\n",
        "\n",
        "# Class 2 (blue)\n",
        "X2 = np.random.randn(20, 2) + np.array([-2, -2])\n",
        "y2 = np.ones(20)\n",
        "\n",
        "X_simple = np.vstack([X1, X2])\n",
        "y_simple = np.hstack([y1, y2])\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_simple[y_simple==0][:, 0], X_simple[y_simple==0][:, 1], \n",
        "            c='red', label='Class 0', s=100, alpha=0.7, edgecolors='k')\n",
        "plt.scatter(X_simple[y_simple==1][:, 0], X_simple[y_simple==1][:, 1], \n",
        "            c='blue', label='Class 1', s=100, alpha=0.7, edgecolors='k')\n",
        "plt.xlabel('Feature 1', fontsize=12)\n",
        "plt.ylabel('Feature 2', fontsize=12)\n",
        "plt.title('Simple Linearly Separable Data', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Hard Margin SVM\n",
        "\n",
        "### What is Hard Margin?\n",
        "Hard margin SVM assumes that the data is **perfectly linearly separable** - there exists a hyperplane that completely separates the two classes with no errors.\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "For a hyperplane defined by $\\mathbf{w}^T\\mathbf{x} + b = 0$:\n",
        "\n",
        "**Optimization Problem:**\n",
        "$$\\min_{\\mathbf{w}, b} \\frac{1}{2}||\\mathbf{w}||^2$$\n",
        "\n",
        "**Subject to:**\n",
        "$$y_i(\\mathbf{w}^T\\mathbf{x}_i + b) \\geq 1, \\quad \\forall i$$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{w}$: weight vector (defines hyperplane orientation)\n",
        "- $b$: bias term (defines hyperplane position)\n",
        "- $y_i \\in \\{-1, +1\\}$: class labels\n",
        "- The margin width is $\\frac{2}{||\\mathbf{w}||}$\n",
        "\n",
        "**Intuition:** \n",
        "- We minimize $||\\mathbf{w}||^2$ to **maximize the margin** $\\frac{2}{||\\mathbf{w}||}$\n",
        "- The constraint ensures all points are correctly classified with margin ‚â• 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Hard Margin SVM (using very large C for hard margin approximation)\n",
        "svm_hard = SVC(kernel='linear', C=1e10)\n",
        "svm_hard.fit(X_simple, y_simple)\n",
        "\n",
        "# Function to plot decision boundary\n",
        "def plot_svm_decision_boundary(X, y, model, title):\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    \n",
        "    # Create mesh\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                         np.linspace(y_min, y_max, 200))\n",
        "    \n",
        "    # Predict on mesh\n",
        "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    # Plot decision boundary and margins\n",
        "    plt.contourf(xx, yy, Z, levels=[-100, 0, 100], colors=['lightcoral', 'lightblue'], alpha=0.3)\n",
        "    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'], \n",
        "                colors=['red', 'black', 'blue'], linewidths=[2, 3, 2])\n",
        "    \n",
        "    # Plot data points\n",
        "    plt.scatter(X[y==0][:, 0], X[y==0][:, 1], c='red', label='Class 0', \n",
        "                s=100, alpha=0.7, edgecolors='k')\n",
        "    plt.scatter(X[y==1][:, 0], X[y==1][:, 1], c='blue', label='Class 1', \n",
        "                s=100, alpha=0.7, edgecolors='k')\n",
        "    \n",
        "    # Highlight support vectors\n",
        "    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], \n",
        "                s=200, linewidth=2, facecolors='none', edgecolors='green', \n",
        "                label='Support Vectors')\n",
        "    \n",
        "    plt.xlabel('Feature 1', fontsize=12)\n",
        "    plt.ylabel('Feature 2', fontsize=12)\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add text annotations\n",
        "    plt.text(0.02, 0.98, f'Support Vectors: {len(model.support_vectors_)}', \n",
        "             transform=plt.gca().transAxes, fontsize=10, verticalalignment='top',\n",
        "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "plot_svm_decision_boundary(X_simple, y_simple, svm_hard, \n",
        "                          'Hard Margin SVM\\n(Perfectly Separable Data)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Observations:\n",
        "- The **solid black line** is the decision boundary\n",
        "- The **dashed lines** represent the margins\n",
        "- The **green circles** are support vectors (points on the margin)\n",
        "- The distance between the dashed lines is the margin\n",
        "\n",
        "### Problem with Hard Margin\n",
        "‚ùå **Limitations:**\n",
        "1. Only works when data is perfectly linearly separable\n",
        "2. Very sensitive to outliers\n",
        "3. Not practical for real-world noisy data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Soft Margin SVM (The Practical Solution)\n",
        "\n",
        "### Why Soft Margin?\n",
        "Real-world data is rarely perfectly separable. We need to:\n",
        "1. Allow some misclassifications\n",
        "2. Be robust to outliers\n",
        "3. Balance between margin maximization and classification errors\n",
        "\n",
        "### The Hinge Loss Function\n",
        "\n",
        "Soft margin SVM introduces **slack variables** $\\xi_i$ to allow violations:\n",
        "\n",
        "**Hinge Loss:**\n",
        "$$L_{hinge}(y, f(x)) = \\max(0, 1 - y \\cdot f(x))$$\n",
        "\n",
        "Where $f(x) = \\mathbf{w}^T\\mathbf{x} + b$\n",
        "\n",
        "**Intuition:**\n",
        "- If point is correctly classified and beyond margin: loss = 0\n",
        "- If point is within margin or misclassified: loss increases linearly\n",
        "- This is why it's called \"hinge\" - looks like a door hinge!\n",
        "\n",
        "### Soft Margin Optimization Problem\n",
        "\n",
        "$$\\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2}||\\mathbf{w}||^2 + C\\sum_{i=1}^{n}\\xi_i$$\n",
        "\n",
        "**Subject to:**\n",
        "$$y_i(\\mathbf{w}^T\\mathbf{x}_i + b) \\geq 1 - \\xi_i$$\n",
        "$$\\xi_i \\geq 0, \\quad \\forall i$$\n",
        "\n",
        "Where:\n",
        "- $C$: regularization parameter (controls trade-off)\n",
        "- $\\xi_i$: slack variable (amount of violation for point $i$)\n",
        "\n",
        "**The C Parameter:**\n",
        "- **Large C**: Fewer violations allowed ‚Üí smaller margin, less regularization (risk overfitting)\n",
        "- **Small C**: More violations allowed ‚Üí larger margin, more regularization (risk underfitting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Hinge Loss\n",
        "def plot_hinge_loss():\n",
        "    z = np.linspace(-3, 3, 300)\n",
        "    hinge_loss = np.maximum(0, 1 - z)\n",
        "    zero_one_loss = (z < 0).astype(float)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(z, hinge_loss, 'b-', linewidth=3, label='Hinge Loss: max(0, 1-z)')\n",
        "    plt.plot(z, zero_one_loss, 'r--', linewidth=2, label='0-1 Loss (actual misclassification)')\n",
        "    \n",
        "    # Annotations\n",
        "    plt.axvline(x=0, color='gray', linestyle=':', alpha=0.5)\n",
        "    plt.axvline(x=1, color='gray', linestyle=':', alpha=0.5)\n",
        "    plt.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
        "    \n",
        "    # Add regions\n",
        "    plt.fill_between(z, 0, 3, where=(z < 0), alpha=0.1, color='red', \n",
        "                     label='Misclassified (z<0)')\n",
        "    plt.fill_between(z, 0, 3, where=((z >= 0) & (z < 1)), alpha=0.1, color='orange',\n",
        "                     label='Correct but within margin (0‚â§z<1)')\n",
        "    plt.fill_between(z, 0, 3, where=(z >= 1), alpha=0.1, color='green',\n",
        "                     label='Correct and beyond margin (z‚â•1)')\n",
        "    \n",
        "    plt.xlabel('z = y¬∑f(x) (margin)', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.title('Hinge Loss Function\\nz = y¬∑f(x) represents how confident/correct the prediction is', \n",
        "              fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.ylim(-0.1, 3)\n",
        "    plt.show()\n",
        "\n",
        "plot_hinge_loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Loss Regions:\n",
        "\n",
        "1. **z ‚â• 1 (Green)**: Point is correctly classified and beyond the margin ‚Üí Loss = 0\n",
        "2. **0 ‚â§ z < 1 (Orange)**: Point is correctly classified but within the margin ‚Üí Loss > 0 (small penalty)\n",
        "3. **z < 0 (Red)**: Point is misclassified ‚Üí Loss > 1 (large penalty)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dataset with noise/outliers\n",
        "np.random.seed(42)\n",
        "\n",
        "# Main clusters\n",
        "X1_noisy = np.random.randn(30, 2) + np.array([2, 2])\n",
        "X2_noisy = np.random.randn(30, 2) + np.array([-2, -2])\n",
        "\n",
        "# Add outliers\n",
        "outliers1 = np.array([[-1, -1], [-2, -1], [-1.5, -2]])\n",
        "outliers2 = np.array([[1, 1], [2, 1], [1.5, 2]])\n",
        "\n",
        "X_noisy = np.vstack([X1_noisy, outliers2, X2_noisy, outliers1])\n",
        "y_noisy = np.hstack([np.zeros(33), np.ones(33)])\n",
        "\n",
        "# Visualize the noisy dataset\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_noisy[y_noisy==0][:, 0], X_noisy[y_noisy==0][:, 1], \n",
        "            c='red', label='Class 0', s=100, alpha=0.7, edgecolors='k')\n",
        "plt.scatter(X_noisy[y_noisy==1][:, 0], X_noisy[y_noisy==1][:, 1], \n",
        "            c='blue', label='Class 1', s=100, alpha=0.7, edgecolors='k')\n",
        "plt.xlabel('Feature 1', fontsize=12)\n",
        "plt.ylabel('Feature 2', fontsize=12)\n",
        "plt.title('Data with Outliers and Noise', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different C values\n",
        "C_values = [0.01, 1, 100]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, C in enumerate(C_values):\n",
        "    svm = SVC(kernel='linear', C=C)\n",
        "    svm.fit(X_noisy, y_noisy)\n",
        "    \n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Create mesh\n",
        "    x_min, x_max = X_noisy[:, 0].min() - 1, X_noisy[:, 0].max() + 1\n",
        "    y_min, y_max = X_noisy[:, 1].min() - 1, X_noisy[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                         np.linspace(y_min, y_max, 200))\n",
        "    \n",
        "    Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    # Plot\n",
        "    ax.contourf(xx, yy, Z, levels=[-100, 0, 100], colors=['lightcoral', 'lightblue'], alpha=0.3)\n",
        "    ax.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'], \n",
        "               colors=['red', 'black', 'blue'], linewidths=[2, 3, 2])\n",
        "    \n",
        "    ax.scatter(X_noisy[y_noisy==0][:, 0], X_noisy[y_noisy==0][:, 1], \n",
        "               c='red', s=50, alpha=0.7, edgecolors='k')\n",
        "    ax.scatter(X_noisy[y_noisy==1][:, 0], X_noisy[y_noisy==1][:, 1], \n",
        "               c='blue', s=50, alpha=0.7, edgecolors='k')\n",
        "    ax.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], \n",
        "               s=150, linewidth=2, facecolors='none', edgecolors='green')\n",
        "    \n",
        "    ax.set_xlabel('Feature 1', fontsize=11)\n",
        "    ax.set_ylabel('Feature 2', fontsize=11)\n",
        "    ax.set_title(f'C = {C}\\nSupport Vectors: {len(svm.support_vectors_)}', \n",
        "                 fontsize=12, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Effect of C Parameter on Soft Margin SVM', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä INTERPRETATION:\")\n",
        "print(\"\\nüîπ C = 0.01 (Small C):\")\n",
        "print(\"   - WIDE margin, more regularization\")\n",
        "print(\"   - Many violations allowed (more support vectors)\")\n",
        "print(\"   - More generalization, less sensitive to outliers\")\n",
        "print(\"   - Risk: Underfitting\")\n",
        "\n",
        "print(\"\\nüîπ C = 1 (Moderate C):\")\n",
        "print(\"   - BALANCED trade-off\")\n",
        "print(\"   - Moderate margin width\")\n",
        "print(\"   - Good generalization with reasonable accuracy\")\n",
        "\n",
        "print(\"\\nüîπ C = 100 (Large C):\")\n",
        "print(\"   - NARROW margin, less regularization\")\n",
        "print(\"   - Fewer violations (fewer support vectors)\")\n",
        "print(\"   - Tries to classify all points correctly\")\n",
        "print(\"   - Risk: Overfitting, sensitive to outliers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hard Margin vs Soft Margin: Complete Comparison\n",
        "\n",
        "| Aspect | Hard Margin | Soft Margin |\n",
        "|--------|-------------|-------------|\n",
        "| **Data Requirement** | Must be perfectly separable | Can handle non-separable data |\n",
        "| **Optimization** | $\\min \\frac{1}{2}\\|\\mathbf{w}\\|^2$ | $\\min \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C\\sum \\xi_i$ |\n",
        "| **Constraints** | $y_i(\\mathbf{w}^T\\mathbf{x}_i + b) \\geq 1$ (strict) | $y_i(\\mathbf{w}^T\\mathbf{x}_i + b) \\geq 1 - \\xi_i$ (relaxed) |\n",
        "| **Loss Function** | No explicit loss (hard constraints) | Hinge loss: $\\max(0, 1-y \\cdot f(x))$ |\n",
        "| **Violations** | ‚ùå None allowed | ‚úÖ Controlled by C parameter |\n",
        "| **Outlier Sensitivity** | Very sensitive | Robust |\n",
        "| **Flexibility** | Rigid | Flexible (tune with C) |\n",
        "| **Real-world Use** | Rare | Standard approach |\n",
        "| **Parameters** | None | C (regularization parameter) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. The Kernel Trick: Handling Non-Linear Data\n",
        "\n",
        "### The Problem\n",
        "What if the data is not linearly separable even with soft margins?\n",
        "\n",
        "### The Solution: Kernels\n",
        "\n",
        "**Core Idea:** Map data to a higher-dimensional space where it becomes linearly separable!\n",
        "\n",
        "$$\\phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}^D \\quad (d < D)$$\n",
        "\n",
        "### The Kernel Trick\n",
        "Instead of explicitly computing $\\phi(\\mathbf{x})$, we use a kernel function:\n",
        "\n",
        "$$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j)$$\n",
        "\n",
        "This computes the dot product in high-dimensional space **without explicitly going there**!\n",
        "\n",
        "### Common Kernels\n",
        "\n",
        "1. **Linear Kernel:**\n",
        "   $$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j$$\n",
        "   - Use when: Data is linearly separable\n",
        "\n",
        "2. **Polynomial Kernel:**\n",
        "   $$K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i^T \\mathbf{x}_j + r)^d$$\n",
        "   - Use when: Decision boundary is polynomial\n",
        "   - $d$: degree of polynomial\n",
        "\n",
        "3. **RBF (Radial Basis Function) / Gaussian Kernel:**\n",
        "   $$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma ||\\mathbf{x}_i - \\mathbf{x}_j||^2)$$\n",
        "   - Use when: Complex, non-linear boundaries\n",
        "   - $\\gamma$: controls influence of single training example\n",
        "   - **Most popular choice!**\n",
        "\n",
        "4. **Sigmoid Kernel:**\n",
        "   $$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\gamma \\mathbf{x}_i^T \\mathbf{x}_j + r)$$\n",
        "   - Use when: Mimicking neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create non-linear dataset (XOR-like problem)\n",
        "np.random.seed(42)\n",
        "\n",
        "def create_nonlinear_data():\n",
        "    # Create concentric circles\n",
        "    n_samples = 200\n",
        "    X, y = datasets.make_circles(n_samples=n_samples, noise=0.1, factor=0.5, random_state=42)\n",
        "    return X, y\n",
        "\n",
        "X_nonlinear, y_nonlinear = create_nonlinear_data()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_nonlinear[y_nonlinear==0][:, 0], X_nonlinear[y_nonlinear==0][:, 1], \n",
        "            c='red', label='Class 0', s=50, alpha=0.7, edgecolors='k')\n",
        "plt.scatter(X_nonlinear[y_nonlinear==1][:, 0], X_nonlinear[y_nonlinear==1][:, 1], \n",
        "            c='blue', label='Class 1', s=50, alpha=0.7, edgecolors='k')\n",
        "plt.xlabel('Feature 1', fontsize=12)\n",
        "plt.ylabel('Feature 2', fontsize=12)\n",
        "plt.title('Non-Linear Data (Concentric Circles)\\nCannot be separated by a straight line!', \n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different kernels\n",
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "kernel_params = [\n",
        "    {'kernel': 'linear', 'C': 1},\n",
        "    {'kernel': 'poly', 'C': 1, 'degree': 3, 'gamma': 'auto'},\n",
        "    {'kernel': 'rbf', 'C': 1, 'gamma': 'auto'},\n",
        "    {'kernel': 'sigmoid', 'C': 1, 'gamma': 'auto'}\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, (kernel_name, params) in enumerate(zip(kernels, kernel_params)):\n",
        "    svm = SVC(**params)\n",
        "    svm.fit(X_nonlinear, y_nonlinear)\n",
        "    \n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Create mesh\n",
        "    x_min, x_max = X_nonlinear[:, 0].min() - 0.5, X_nonlinear[:, 0].max() + 0.5\n",
        "    y_min, y_max = X_nonlinear[:, 1].min() - 0.5, X_nonlinear[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                         np.linspace(y_min, y_max, 200))\n",
        "    \n",
        "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    # Plot\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
        "    ax.scatter(X_nonlinear[y_nonlinear==0][:, 0], X_nonlinear[y_nonlinear==0][:, 1], \n",
        "               c='red', s=30, alpha=0.7, edgecolors='k', label='Class 0')\n",
        "    ax.scatter(X_nonlinear[y_nonlinear==1][:, 0], X_nonlinear[y_nonlinear==1][:, 1], \n",
        "               c='blue', s=30, alpha=0.7, edgecolors='k', label='Class 1')\n",
        "    ax.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], \n",
        "               s=100, linewidth=2, facecolors='none', edgecolors='green', \n",
        "               label='Support Vectors')\n",
        "    \n",
        "    accuracy = svm.score(X_nonlinear, y_nonlinear)\n",
        "    \n",
        "    ax.set_xlabel('Feature 1', fontsize=11)\n",
        "    ax.set_ylabel('Feature 2', fontsize=11)\n",
        "    ax.set_title(f'{kernel_name.upper()} Kernel\\nAccuracy: {accuracy:.2%} | '\n",
        "                 f'Support Vectors: {len(svm.support_vectors_)}', \n",
        "                 fontsize=12, fontweight='bold')\n",
        "    ax.legend(loc='upper right', fontsize=9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "plt.suptitle('Kernel Comparison on Non-Linear Data', fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä KERNEL PERFORMANCE ANALYSIS:\")\n",
        "print(\"\\n‚ùå Linear Kernel: FAILS - cannot capture circular pattern\")\n",
        "print(\"‚úÖ Polynomial Kernel: GOOD - can model curved boundaries\")\n",
        "print(\"‚úÖ‚úÖ RBF Kernel: EXCELLENT - best for complex non-linear patterns\")\n",
        "print(\"‚ö†Ô∏è  Sigmoid Kernel: MODERATE - limited flexibility\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing the Kernel Transformation\n",
        "\n",
        "Let's understand what happens when we apply a kernel - we're essentially transforming the data to a higher dimension where it becomes separable!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple example: Transform 1D data to 2D using polynomial kernel\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create 1D data that's not linearly separable\n",
        "X_1d = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "y_1d = (np.abs(X_1d.ravel()) < 1.5).astype(int)\n",
        "\n",
        "# Transform to 2D: [x, x^2]\n",
        "X_2d = np.c_[X_1d, X_1d**2]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Original 1D space\n",
        "axes[0].scatter(X_1d[y_1d==0], np.zeros(sum(y_1d==0)), c='red', s=100, \n",
        "                alpha=0.7, edgecolors='k', label='Class 0')\n",
        "axes[0].scatter(X_1d[y_1d==1], np.zeros(sum(y_1d==1)), c='blue', s=100, \n",
        "                alpha=0.7, edgecolors='k', label='Class 1')\n",
        "axes[0].set_xlabel('x', fontsize=12)\n",
        "axes[0].set_title('Original 1D Space\\n(NOT linearly separable)', fontsize=13, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].set_ylim(-0.5, 0.5)\n",
        "\n",
        "# Transformed 2D space\n",
        "axes[1].scatter(X_2d[y_1d==0][:, 0], X_2d[y_1d==0][:, 1], c='red', s=100, \n",
        "                alpha=0.7, edgecolors='k', label='Class 0')\n",
        "axes[1].scatter(X_2d[y_1d==1][:, 0], X_2d[y_1d==1][:, 1], c='blue', s=100, \n",
        "                alpha=0.7, edgecolors='k', label='Class 1')\n",
        "\n",
        "# Draw separating line in 2D space\n",
        "x_line = np.linspace(-3, 3, 100)\n",
        "y_line = np.ones_like(x_line) * 2.25\n",
        "axes[1].plot(x_line, y_line, 'k-', linewidth=3, label='Linear separator')\n",
        "\n",
        "axes[1].set_xlabel('x', fontsize=12)\n",
        "axes[1].set_ylabel('x¬≤', fontsize=12)\n",
        "axes[1].set_title('Transformed 2D Space: œÜ(x) = [x, x¬≤]\\n(NOW linearly separable!)', \n",
        "                  fontsize=13, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Kernel Transformation: Making Non-Linear Data Linearly Separable', \n",
        "             fontsize=15, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚ú® THE MAGIC OF KERNELS:\")\n",
        "print(\"\\n1. Original space: Classes overlap - NO straight line can separate them\")\n",
        "print(\"2. Transform to higher dimension: œÜ(x) = [x, x¬≤]\")\n",
        "print(\"3. New space: Classes become linearly separable!\")\n",
        "print(\"4. Kernel trick: We don't need to explicitly compute œÜ(x)\")\n",
        "print(\"   We just compute K(x_i, x_j) = œÜ(x_i)¬∑œÜ(x_j) directly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Real-World Example: Iris Dataset Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # Use only first 2 features for visualization\n",
        "y = iris.target\n",
        "\n",
        "# For simplicity, convert to binary classification (class 0 vs rest)\n",
        "y_binary = (y != 0).astype(int)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_iris = SVC(kernel='rbf', C=1, gamma='auto')\n",
        "svm_iris.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = svm_iris.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nüéØ Test Accuracy: {accuracy:.2%}\")\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Setosa', 'Non-Setosa']))\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Training data\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_train_scaled[y_train==0][:, 0], X_train_scaled[y_train==0][:, 1], \n",
        "            c='red', label='Setosa', s=100, alpha=0.7, edgecolors='k')\n",
        "plt.scatter(X_train_scaled[y_train==1][:, 0], X_train_scaled[y_train==1][:, 1], \n",
        "            c='blue', label='Non-Setosa', s=100, alpha=0.7, edgecolors='k')\n",
        "plt.xlabel('Sepal Length (scaled)', fontsize=11)\n",
        "plt.ylabel('Sepal Width (scaled)', fontsize=11)\n",
        "plt.title('Training Data', fontsize=12, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Test data with decision boundary\n",
        "plt.subplot(1, 2, 2)\n",
        "x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
        "y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                     np.linspace(y_min, y_max, 200))\n",
        "Z = svm_iris.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
        "plt.scatter(X_test_scaled[y_test==0][:, 0], X_test_scaled[y_test==0][:, 1], \n",
        "            c='red', label='Setosa', s=100, alpha=0.7, edgecolors='k')\n",
        "plt.scatter(X_test_scaled[y_test==1][:, 0], X_test_scaled[y_test==1][:, 1], \n",
        "            c='blue', label='Non-Setosa', s=100, alpha=0.7, edgecolors='k')\n",
        "plt.scatter(svm_iris.support_vectors_[:, 0], svm_iris.support_vectors_[:, 1], \n",
        "            s=150, linewidth=2, facecolors='none', edgecolors='green', label='Support Vectors')\n",
        "plt.xlabel('Sepal Length (scaled)', fontsize=11)\n",
        "plt.ylabel('Sepal Width (scaled)', fontsize=11)\n",
        "plt.title(f'Test Data with Decision Boundary\\nAccuracy: {accuracy:.2%}', \n",
        "          fontsize=12, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('SVM on Iris Dataset (Binary Classification)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Hyperparameter Tuning: Finding Optimal C and Gamma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grid search for best parameters\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1, 'auto']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nüîç HYPERPARAMETER TUNING RESULTS:\")\n",
        "print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Cross-Validation Score: {grid_search.best_score_:.2%}\")\n",
        "print(f\"Test Set Score: {grid_search.score(X_test_scaled, y_test):.2%}\")\n",
        "\n",
        "# Visualize grid search results\n",
        "results = grid_search.cv_results_\n",
        "scores = results['mean_test_score'].reshape(len(param_grid['C']), len(param_grid['gamma']))\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "im = plt.imshow(scores, interpolation='nearest', cmap='viridis')\n",
        "plt.colorbar(im, label='Mean CV Accuracy')\n",
        "plt.xlabel('Gamma', fontsize=12)\n",
        "plt.ylabel('C', fontsize=12)\n",
        "plt.title('Hyperparameter Grid Search Results\\n(RBF Kernel)', fontsize=14, fontweight='bold')\n",
        "plt.xticks(range(len(param_grid['gamma'])), [str(g) for g in param_grid['gamma']])\n",
        "plt.yticks(range(len(param_grid['C'])), param_grid['C'])\n",
        "\n",
        "# Annotate cells with values\n",
        "for i in range(len(param_grid['C'])):\n",
        "    for j in range(len(param_grid['gamma'])):\n",
        "        text = plt.text(j, i, f'{scores[i, j]:.3f}',\n",
        "                       ha=\"center\", va=\"center\", color=\"white\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° PARAMETER INTERPRETATION:\")\n",
        "print(\"\\nüìå C (Regularization):\")\n",
        "print(\"   - Low C: Simpler model, wider margin, more regularization\")\n",
        "print(\"   - High C: Complex model, narrower margin, less regularization\")\n",
        "print(\"\\nüìå Gamma (RBF Kernel Parameter):\")\n",
        "print(\"   - Low gamma: Far reach, smoother decision boundary\")\n",
        "print(\"   - High gamma: Close reach, more complex decision boundary\")\n",
        "print(\"   - Too high gamma ‚Üí Overfitting (each point becomes its own island)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Key Takeaways\n",
        "\n",
        "### ‚úÖ When to Use SVM:\n",
        "1. **High-dimensional data** (e.g., text classification, genomics)\n",
        "2. **Clear margin of separation** between classes\n",
        "3. **Medium-sized datasets** (computationally expensive for large datasets)\n",
        "4. **Binary or multi-class classification**\n",
        "5. **When interpretability matters** (linear SVM shows feature importance)\n",
        "\n",
        "### ‚ùå When NOT to Use SVM:\n",
        "1. **Very large datasets** (millions of samples) ‚Üí Use logistic regression or neural networks\n",
        "2. **When probability estimates are critical** ‚Üí SVM gives scores, not probabilities\n",
        "3. **Highly noisy data** with overlapping classes\n",
        "4. **When training time is critical**\n",
        "\n",
        "### üéØ Best Practices:\n",
        "1. **Always scale your features** (SVM is sensitive to feature scales)\n",
        "2. **Start with RBF kernel** for non-linear problems\n",
        "3. **Use cross-validation** to tune C and gamma\n",
        "4. **For linear problems**, try linear kernel first (faster)\n",
        "5. **Check class balance** - use class_weight='balanced' for imbalanced data\n",
        "\n",
        "### üìö Summary Formula Reference:\n",
        "\n",
        "**Hard Margin:**\n",
        "$$\\min_{\\mathbf{w}, b} \\frac{1}{2}||\\mathbf{w}||^2 \\quad \\text{s.t.} \\quad y_i(\\mathbf{w}^T\\mathbf{x}_i + b) \\geq 1$$\n",
        "\n",
        "**Soft Margin:**\n",
        "$$\\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2}||\\mathbf{w}||^2 + C\\sum_{i=1}^{n}\\xi_i \\quad \\text{s.t.} \\quad y_i(\\mathbf{w}^T\\mathbf{x}_i + b) \\geq 1 - \\xi_i$$\n",
        "\n",
        "**Hinge Loss:**\n",
        "$$L_{hinge}(y, f(x)) = \\max(0, 1 - y \\cdot f(x))$$\n",
        "\n",
        "**RBF Kernel:**\n",
        "$$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma ||\\mathbf{x}_i - \\mathbf{x}_j||^2)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Practice Exercise\n",
        "\n",
        "Try modifying the parameters below and observe how the decision boundary changes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive exercise - Try different parameters!\n",
        "# Experiment with these values:\n",
        "KERNEL = 'rbf'  # Options: 'linear', 'poly', 'rbf', 'sigmoid'\n",
        "C_VALUE = 1.0   # Try: 0.01, 0.1, 1, 10, 100\n",
        "GAMMA_VALUE = 'auto'  # Try: 0.001, 0.01, 0.1, 1, 'auto'\n",
        "\n",
        "# Create and train model\n",
        "svm_practice = SVC(kernel=KERNEL, C=C_VALUE, gamma=GAMMA_VALUE)\n",
        "svm_practice.fit(X_nonlinear, y_nonlinear)\n",
        "\n",
        "# Plot\n",
        "plot_svm_decision_boundary(X_nonlinear, y_nonlinear, svm_practice,\n",
        "                          f'Your SVM: kernel={KERNEL}, C={C_VALUE}, gamma={GAMMA_VALUE}\\n'\n",
        "                          f'Accuracy: {svm_practice.score(X_nonlinear, y_nonlinear):.2%}')\n",
        "\n",
        "print(\"\\nüéÆ TRY THIS:\")\n",
        "print(\"1. Change KERNEL to 'linear' - what happens?\")\n",
        "print(\"2. Set C_VALUE to 0.01 and then 100 - compare the margins\")\n",
        "print(\"3. Set GAMMA_VALUE to 0.01 and then 10 with 'rbf' kernel - see the difference\")\n",
        "print(\"4. Which combination gives the best result for this dataset?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Conclusion\n",
        "\n",
        "Congratulations! You now understand:\n",
        "- ‚úÖ How SVM finds the optimal separating hyperplane\n",
        "- ‚úÖ The difference between hard and soft margins\n",
        "- ‚úÖ Hinge loss and how it allows violations\n",
        "- ‚úÖ The kernel trick for handling non-linear data\n",
        "- ‚úÖ How to tune hyperparameters (C and gamma)\n",
        "- ‚úÖ When to use SVM in practice\n",
        "\n",
        "**Next Steps:**\n",
        "1. Try SVM on your own datasets\n",
        "2. Experiment with multi-class classification (one-vs-one, one-vs-rest)\n",
        "3. Explore SVM for regression (SVR)\n",
        "4. Learn about advanced kernels and custom kernel functions\n",
        "\n",
        "Happy Learning! üöÄ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
