{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN) - Complete Tutorial\n",
    "## From Concepts to Implementation\n",
    "\n",
    "Welcome to this comprehensive tutorial on Recurrent Neural Networks! This notebook covers:\n",
    "1. **Core Concepts** - Understanding RNNs from the ground up\n",
    "2. **Real-World Example** - Stock price prediction in StatLand\n",
    "3. **Implementation from Scratch** - Building an RNN with NumPy\n",
    "4. **Implementation with PyTorch** - Using modern libraries\n",
    "5. **Visualizations** - Understanding the internal mechanisms\n",
    "6. **The Vanishing/Exploding Gradient Problem**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: What Makes RNNs Special?\n",
    "\n",
    "### The Problem RNNs Solve\n",
    "\n",
    "Traditional neural networks have a **fixed number of inputs**:\n",
    "- A network trained on images of 6Ã—6 pixels can't handle 7Ã—7 images\n",
    "- A network with 3 inputs can't process 5 inputs\n",
    "\n",
    "But many real-world problems involve **sequential data of varying lengths**:\n",
    "- Stock prices over different time periods\n",
    "- Sentences with different numbers of words\n",
    "- Music sequences of varying durations\n",
    "\n",
    "**Recurrent Neural Networks (RNNs)** solve this by using **feedback loops** that allow them to process sequences of any length!\n",
    "\n",
    "### Key Features of RNNs\n",
    "\n",
    "1. **Feedback Loops**: Output from one step becomes input to the next\n",
    "2. **Shared Weights**: Same parameters used across all time steps\n",
    "3. **Sequential Processing**: Process data one element at a time\n",
    "4. **Memory**: Maintain information about previous inputs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The StatLand Stock Market Example\n",
    "\n",
    "Let's use the example from the transcript: predicting stock prices in StatLand.\n",
    "\n",
    "### StatLand Stock Market Rules:\n",
    "\n",
    "1. **Low â†’ Low**: If price is low for 2 days â†’ stays low tomorrow\n",
    "2. **Low â†’ Medium**: If price goes from low to medium â†’ goes high tomorrow\n",
    "3. **High â†’ Medium**: If price decreases from high to medium â†’ goes lower tomorrow\n",
    "4. **High â†’ High**: If price stays high for 2 days â†’ stays high tomorrow\n",
    "\n",
    "### Data Encoding:\n",
    "- **Low** = 0\n",
    "- **Medium** = 0.5\n",
    "- **High** = 1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "from matplotlib.patches import Rectangle, Circle, Arrow\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing Sequential Data\n",
    "\n",
    "Let's visualize why we need RNNs - different sequences have different lengths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample stock price data with different lengths\n",
    "np.random.seed(42)\n",
    "\n",
    "# Blue company - 9 days of data\n",
    "blue_company_days = np.arange(1, 10)\n",
    "blue_company_prices = np.array([20, 22, 25, 28, 30, 32, 29, 27, 26])\n",
    "\n",
    "# Red company - 5 days of data\n",
    "red_company_days = np.arange(1, 6)\n",
    "red_company_prices = np.array([15, 17, 19, 18, 20])\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(blue_company_days, blue_company_prices, 'o-', linewidth=3, \n",
    "        markersize=10, label='Blue Company (9 days)', color='#3498db')\n",
    "ax.plot(red_company_days, red_company_prices, 's-', linewidth=3, \n",
    "        markersize=10, label='Red Company (5 days)', color='#e74c3c')\n",
    "\n",
    "ax.set_xlabel('Day', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Stock Price ($)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Stock Prices Over Time - Different Sequence Lengths', \n",
    "             fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotation\n",
    "ax.annotate('Different lengths!\\nWe need flexible networks', \n",
    "            xy=(7, 27), xytext=(5, 22),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='black'),\n",
    "            fontsize=12, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Blue Company has {len(blue_company_days)} data points\")\n",
    "print(f\"Red Company has {len(red_company_days)} data points\")\n",
    "print(\"\\nâ†’ RNNs can handle BOTH sequences!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RNN Architecture Visualization\n",
    "\n",
    "### 4.1 Basic RNN with Feedback Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rnn_with_feedback():\n",
    "    \"\"\"Visualize RNN with feedback loop (folded representation)\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Input node\n",
    "    input_circle = Circle((2, 3), 0.5, color='lightblue', ec='black', lw=2)\n",
    "    ax.add_patch(input_circle)\n",
    "    ax.text(2, 3, 'Input\\n(x)', ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    # Hidden node with feedback\n",
    "    hidden_circle = Circle((5, 6), 0.7, color='lightcoral', ec='black', lw=2)\n",
    "    ax.add_patch(hidden_circle)\n",
    "    ax.text(5, 6, 'Hidden\\nLayer', ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    # Output node\n",
    "    output_circle = Circle((8, 3), 0.5, color='lightgreen', ec='black', lw=2)\n",
    "    ax.add_patch(output_circle)\n",
    "    ax.text(8, 3, 'Output\\n(Å·)', ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    # Arrows\n",
    "    # Input to hidden\n",
    "    ax.annotate('', xy=(4.5, 5.6), xytext=(2.4, 3.4),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2.5, color='blue'))\n",
    "    ax.text(3, 4.8, 'Wâ‚', fontsize=12, fontweight='bold', color='blue')\n",
    "    \n",
    "    # Hidden to output\n",
    "    ax.annotate('', xy=(7.6, 3.4), xytext=(5.5, 5.6),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2.5, color='green'))\n",
    "    ax.text(7, 4.8, 'Wâ‚ƒ', fontsize=12, fontweight='bold', color='green')\n",
    "    \n",
    "    # Feedback loop - the key feature!\n",
    "    from matplotlib.patches import FancyBboxPatch\n",
    "    feedback_arrow = FancyArrowPatch((5.7, 6), (6.5, 7.5),\n",
    "                                     connectionstyle=\"arc3,rad=.5\",\n",
    "                                     arrowstyle='->', lw=3, color='red')\n",
    "    ax.add_patch(feedback_arrow)\n",
    "    \n",
    "    feedback_arrow2 = FancyArrowPatch((6.5, 7.5), (4.3, 6),\n",
    "                                      connectionstyle=\"arc3,rad=-.5\",\n",
    "                                      arrowstyle='->', lw=3, color='red')\n",
    "    ax.add_patch(feedback_arrow2)\n",
    "    \n",
    "    ax.text(6.5, 8, 'Feedback Loop\\n(Memory!)', ha='center', \n",
    "            fontsize=12, fontweight='bold', color='red',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))\n",
    "    ax.text(6.2, 7, 'Wâ‚‚', fontsize=12, fontweight='bold', color='red')\n",
    "    \n",
    "    ax.set_title('RNN with Feedback Loop (Folded Representation)', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_rnn_with_feedback()\n",
    "print(\"\\nðŸ”„ The feedback loop allows the network to remember previous inputs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Unrolled RNN - The Secret to Understanding!\n",
    "\n",
    "**Unrolling** is the key to understanding RNNs. We create a copy of the network for each time step, which makes it easier to see how information flows through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_unrolled_rnn(num_timesteps=3):\n",
    "    \"\"\"Visualize unrolled RNN across multiple timesteps\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    ax.set_xlim(0, num_timesteps * 4 + 1)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    colors = ['#ff9999', '#ffcc99', '#99ccff']\n",
    "    time_labels = ['Yesterday\\n(t-2)', 'Yesterday\\n(t-1)', 'Today\\n(t)']\n",
    "    output_labels = ['Ignore', 'Ignore', 'Tomorrow\\nPrediction!']\n",
    "    \n",
    "    for t in range(num_timesteps):\n",
    "        x_offset = t * 4 + 2\n",
    "        \n",
    "        # Input\n",
    "        input_circle = Circle((x_offset, 2), 0.4, color='lightblue', ec='black', lw=2)\n",
    "        ax.add_patch(input_circle)\n",
    "        ax.text(x_offset, 2, f'x_{t+1}', ha='center', va='center', fontweight='bold')\n",
    "        ax.text(x_offset, 0.8, time_labels[t], ha='center', fontsize=9, style='italic')\n",
    "        \n",
    "        # Hidden layer\n",
    "        hidden_circle = Circle((x_offset, 5), 0.5, color=colors[t], ec='black', lw=2)\n",
    "        ax.add_patch(hidden_circle)\n",
    "        ax.text(x_offset, 5, f'h_{t+1}', ha='center', va='center', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        # Output\n",
    "        output_circle = Circle((x_offset, 8), 0.4, color='lightgreen', ec='black', lw=2)\n",
    "        ax.add_patch(output_circle)\n",
    "        ax.text(x_offset, 8, f'Å·_{t+1}', ha='center', va='center', fontweight='bold')\n",
    "        ax.text(x_offset, 9.2, output_labels[t], ha='center', fontsize=9, \n",
    "                style='italic', fontweight='bold',\n",
    "                color='red' if t < 2 else 'green')\n",
    "        \n",
    "        # Connections\n",
    "        # Input to hidden\n",
    "        ax.annotate('', xy=(x_offset, 4.5), xytext=(x_offset, 2.4),\n",
    "                   arrowprops=dict(arrowstyle='->', lw=2, color='blue'))\n",
    "        ax.text(x_offset + 0.3, 3.3, 'Wâ‚', fontsize=10, color='blue', fontweight='bold')\n",
    "        \n",
    "        # Hidden to output\n",
    "        ax.annotate('', xy=(x_offset, 7.6), xytext=(x_offset, 5.5),\n",
    "                   arrowprops=dict(arrowstyle='->', lw=2, color='green'))\n",
    "        ax.text(x_offset + 0.3, 6.5, 'Wâ‚ƒ', fontsize=10, color='green', fontweight='bold')\n",
    "        \n",
    "        # Recurrent connection (except for last timestep)\n",
    "        if t < num_timesteps - 1:\n",
    "            ax.annotate('', xy=(x_offset + 4, 5), xytext=(x_offset + 0.5, 5),\n",
    "                       arrowprops=dict(arrowstyle='->', lw=3, color='red'))\n",
    "            ax.text(x_offset + 2, 5.5, 'Wâ‚‚', fontsize=11, color='red', fontweight='bold',\n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "    \n",
    "    # Title and annotations\n",
    "    ax.set_title('Unrolled RNN - Processing Sequential Data Through Time', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add note about shared weights\n",
    "    ax.text(num_timesteps * 2, 0.2, \n",
    "            'âš ï¸ Important: Wâ‚, Wâ‚‚, Wâ‚ƒ, and biases are SHARED across all timesteps!',\n",
    "            ha='center', fontsize=11, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.7', facecolor='lightyellow', \n",
    "                     edgecolor='orange', lw=2))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_unrolled_rnn(3)\n",
    "print(\"\\nðŸ“Š Unrolling makes it easy to see how data flows through time!\")\n",
    "print(\"ðŸ“Œ Key insight: Same weights (Wâ‚, Wâ‚‚, Wâ‚ƒ) used at every time step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mathematical Foundation\n",
    "\n",
    "Let's break down the math behind RNNs step by step.\n",
    "\n",
    "### RNN Equations\n",
    "\n",
    "At each time step $t$:\n",
    "\n",
    "1. **Hidden state update:**\n",
    "   $$h_t = \\tanh(W_1 \\cdot x_t + W_2 \\cdot h_{t-1} + b_1)$$\n",
    "\n",
    "2. **Output:**\n",
    "   $$\\hat{y}_t = W_3 \\cdot h_t + b_2$$\n",
    "\n",
    "Where:\n",
    "- $x_t$ = input at time $t$\n",
    "- $h_t$ = hidden state at time $t$\n",
    "- $h_{t-1}$ = hidden state from previous time step\n",
    "- $W_1$ = input weight\n",
    "- $W_2$ = recurrent weight (the feedback loop!)\n",
    "- $W_3$ = output weight\n",
    "- $b_1, b_2$ = biases\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementation from Scratch\n",
    "\n",
    "Let's build an RNN from scratch using only NumPy to truly understand how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    Simple RNN implementation from scratch for stock price prediction.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input layer: 1 feature (stock price)\n",
    "    - Hidden layer: configurable size with tanh activation\n",
    "    - Output layer: 1 value (predicted next price)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=1, hidden_size=5, output_size=1, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize RNN with random weights.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size: Number of input features\n",
    "        - hidden_size: Number of hidden units\n",
    "        - output_size: Number of output values\n",
    "        - learning_rate: Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights with small random values\n",
    "        # W1: input to hidden\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        \n",
    "        # W2: hidden to hidden (recurrent connection)\n",
    "        self.W2 = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        \n",
    "        # W3: hidden to output\n",
    "        self.W3 = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        \n",
    "        # Biases\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "        \n",
    "        print(\"ðŸ§  RNN initialized!\")\n",
    "        print(f\"   Hidden size: {hidden_size}\")\n",
    "        print(f\"   Total parameters: {self.count_parameters()}\")\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count total number of parameters.\"\"\"\n",
    "        return (self.W1.size + self.W2.size + self.W3.size + \n",
    "                self.b1.size + self.b2.size)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        \"\"\"Tanh activation function.\"\"\"\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def tanh_derivative(self, x):\n",
    "        \"\"\"Derivative of tanh.\"\"\"\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the RNN.\n",
    "        \n",
    "        Parameters:\n",
    "        - inputs: Sequence of inputs (list or array)\n",
    "        \n",
    "        Returns:\n",
    "        - outputs: Predictions at each time step\n",
    "        - hidden_states: Hidden states at each time step\n",
    "        \"\"\"\n",
    "        T = len(inputs)  # Number of time steps\n",
    "        \n",
    "        # Store hidden states and outputs\n",
    "        hidden_states = []\n",
    "        outputs = []\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h_prev = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        # Process each time step\n",
    "        for t in range(T):\n",
    "            # Get input at time t\n",
    "            x_t = np.array([[inputs[t]]])\n",
    "            \n",
    "            # Hidden state computation: h_t = tanh(W1*x_t + W2*h_{t-1} + b1)\n",
    "            h_t = self.tanh(np.dot(self.W1, x_t) + \n",
    "                           np.dot(self.W2, h_prev) + \n",
    "                           self.b1)\n",
    "            \n",
    "            # Output computation: y_t = W3*h_t + b2\n",
    "            y_t = np.dot(self.W3, h_t) + self.b2\n",
    "            \n",
    "            # Store for backpropagation\n",
    "            hidden_states.append(h_t)\n",
    "            outputs.append(y_t[0, 0])\n",
    "            \n",
    "            # Update h_prev for next iteration\n",
    "            h_prev = h_t\n",
    "        \n",
    "        return outputs, hidden_states\n",
    "    \n",
    "    def backward(self, inputs, targets, outputs, hidden_states):\n",
    "        \"\"\"\n",
    "        Backward pass - Backpropagation Through Time (BPTT).\n",
    "        \n",
    "        Parameters:\n",
    "        - inputs: Input sequence\n",
    "        - targets: Target values\n",
    "        - outputs: Predicted outputs from forward pass\n",
    "        - hidden_states: Hidden states from forward pass\n",
    "        \"\"\"\n",
    "        T = len(inputs)\n",
    "        \n",
    "        # Initialize gradients\n",
    "        dW1 = np.zeros_like(self.W1)\n",
    "        dW2 = np.zeros_like(self.W2)\n",
    "        dW3 = np.zeros_like(self.W3)\n",
    "        db1 = np.zeros_like(self.b1)\n",
    "        db2 = np.zeros_like(self.b2)\n",
    "        \n",
    "        # Initialize hidden state gradient\n",
    "        dh_next = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        # Backpropagate through time\n",
    "        for t in reversed(range(T)):\n",
    "            # Output error\n",
    "            dy = outputs[t] - targets[t]\n",
    "            \n",
    "            # Gradient for W3 and b2\n",
    "            dW3 += dy * hidden_states[t].T\n",
    "            db2 += dy\n",
    "            \n",
    "            # Gradient flowing into hidden state\n",
    "            dh = np.dot(self.W3.T, [[dy]]) + dh_next\n",
    "            \n",
    "            # Gradient through tanh\n",
    "            dh_raw = dh * self.tanh_derivative(hidden_states[t])\n",
    "            \n",
    "            # Gradients for W1 and b1\n",
    "            x_t = np.array([[inputs[t]]])\n",
    "            dW1 += np.dot(dh_raw, x_t.T)\n",
    "            db1 += dh_raw\n",
    "            \n",
    "            # Gradient for W2 (recurrent connection)\n",
    "            if t > 0:\n",
    "                dW2 += np.dot(dh_raw, hidden_states[t-1].T)\n",
    "                dh_next = np.dot(self.W2.T, dh_raw)\n",
    "            else:\n",
    "                dh_next = np.zeros_like(dh_next)\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        for grad in [dW1, dW2, dW3, db1, db2]:\n",
    "            np.clip(grad, -5, 5, out=grad)\n",
    "        \n",
    "        # Update weights\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.W3 -= self.learning_rate * dW3\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs=100, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the RNN on sequential data.\n",
    "        \n",
    "        Parameters:\n",
    "        - X_train: List of input sequences\n",
    "        - y_train: List of target sequences\n",
    "        - epochs: Number of training epochs\n",
    "        - verbose: Whether to print training progress\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            \n",
    "            for inputs, targets in zip(X_train, y_train):\n",
    "                # Forward pass\n",
    "                outputs, hidden_states = self.forward(inputs)\n",
    "                \n",
    "                # Calculate loss (MSE)\n",
    "                loss = np.mean([(o - t) ** 2 for o, t in zip(outputs, targets)])\n",
    "                total_loss += loss\n",
    "                \n",
    "                # Backward pass\n",
    "                self.backward(inputs, targets, outputs, hidden_states)\n",
    "            \n",
    "            avg_loss = total_loss / len(X_train)\n",
    "            losses.append(avg_loss)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        \"\"\"Make predictions on a sequence.\"\"\"\n",
    "        outputs, _ = self.forward(inputs)\n",
    "        return outputs[-1]  # Return only the final prediction\n",
    "\n",
    "print(\"âœ“ SimpleRNN class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Prepare Training Data - StatLand Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data based on StatLand rules\n",
    "# Format: [yesterday, today] -> tomorrow\n",
    "\n",
    "# Training sequences\n",
    "X_train = [\n",
    "    [0, 0],      # Low, Low\n",
    "    [0, 0.5],    # Low, Medium\n",
    "    [1, 0.5],    # High, Medium\n",
    "    [1, 1],      # High, High\n",
    "]\n",
    "\n",
    "# Target outputs (tomorrow's price)\n",
    "y_train = [\n",
    "    [0, 0],      # -> Low\n",
    "    [0.5, 1],    # -> High\n",
    "    [0.5, 0],    # -> Low\n",
    "    [1, 1],      # -> High\n",
    "]\n",
    "\n",
    "# Display training data\n",
    "print(\"ðŸ“š StatLand Training Data:\")\n",
    "print(\"=\"*50)\n",
    "rules = [\n",
    "    \"Low â†’ Low â‡’ Low\",\n",
    "    \"Low â†’ Medium â‡’ High\",\n",
    "    \"High â†’ Medium â‡’ Low\",\n",
    "    \"High â†’ High â‡’ High\"\n",
    "]\n",
    "for i, (x, y, rule) in enumerate(zip(X_train, y_train, rules)):\n",
    "    print(f\"{i+1}. {rule}\")\n",
    "    print(f\"   Input:  {x} â†’ Target: {y[-1]}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Train the RNN from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the RNN\n",
    "print(\"ðŸš€ Training RNN from Scratch...\\n\")\n",
    "\n",
    "rnn = SimpleRNN(input_size=1, hidden_size=8, output_size=1, learning_rate=0.05)\n",
    "losses = rnn.train(X_train, y_train, epochs=200, verbose=True)\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.plot(losses, linewidth=2, color='#e74c3c')\n",
    "plt.xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Loss (MSE)', fontsize=12, fontweight='bold')\n",
    "plt.title('RNN Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Loss: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Test the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on training data\n",
    "print(\"ðŸ§ª Testing RNN Predictions:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_scenarios = [\n",
    "    ([0, 0], 0, \"Low â†’ Low\"),\n",
    "    ([0, 0.5], 1, \"Low â†’ Medium\"),\n",
    "    ([1, 0.5], 0, \"High â†’ Medium\"),\n",
    "    ([1, 1], 1, \"High â†’ High\")\n",
    "]\n",
    "\n",
    "for inputs, expected, scenario in test_scenarios:\n",
    "    prediction = rnn.predict(inputs)\n",
    "    expected_label = \"Low\" if expected == 0 else \"High\"\n",
    "    pred_label = \"Low\" if prediction < 0.25 else (\"High\" if prediction > 0.75 else \"Medium\")\n",
    "    \n",
    "    print(f\"\\n{scenario} â†’ Expected: {expected_label} ({expected})\")\n",
    "    print(f\"   Predicted: {pred_label} ({prediction:.4f})\")\n",
    "    print(f\"   âœ“ Correct!\" if abs(prediction - expected) < 0.3 else \"   âœ— Needs more training\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Visualize Predictions vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of predictions\n",
    "scenarios = [\"Lowâ†’Low\", \"Lowâ†’Med\", \"Highâ†’Med\", \"Highâ†’High\"]\n",
    "expected_values = [0, 1, 0, 1]\n",
    "predicted_values = [rnn.predict(x) for x in X_train]\n",
    "\n",
    "x = np.arange(len(scenarios))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width/2, expected_values, width, label='Expected', \n",
    "               color='#3498db', alpha=0.8, edgecolor='black', linewidth=2)\n",
    "bars2 = ax.bar(x + width/2, predicted_values, width, label='Predicted', \n",
    "               color='#e74c3c', alpha=0.8, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Scenario', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Stock Price (Normalized)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('RNN Predictions vs Expected Values', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(scenarios)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implementation with PyTorch\n",
    "\n",
    "Now let's implement the same RNN using PyTorch, a modern deep learning library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    print(\"âœ“ PyTorch imported successfully!\")\n",
    "except ImportError:\n",
    "    print(\"Installing PyTorch...\")\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install torch --break-system-packages\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    print(\"âœ“ PyTorch installed and imported!\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN implementation using PyTorch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=1, hidden_size=8, output_size=1):\n",
    "        super(PyTorchRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, sequence_length, input_size)\n",
    "        - hidden: Initial hidden state\n",
    "        \n",
    "        Returns:\n",
    "        - output: Predictions\n",
    "        - hidden: Final hidden state\n",
    "        \"\"\"\n",
    "        # RNN forward pass\n",
    "        rnn_out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Get output from the last time step\n",
    "        last_output = rnn_out[:, -1, :]\n",
    "        \n",
    "        # Pass through output layer\n",
    "        output = self.fc(last_output)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "print(\"âœ“ PyTorchRNN class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Prepare Data for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_torch = [torch.FloatTensor([[x] for x in seq]).unsqueeze(0) for seq in X_train]\n",
    "y_train_torch = [torch.FloatTensor([[seq[-1]]]) for seq in y_train]\n",
    "\n",
    "print(\"Sample input shape:\", X_train_torch[0].shape)\n",
    "print(\"Sample target shape:\", y_train_torch[0].shape)\n",
    "print(\"\\nâœ“ Data prepared for PyTorch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Train PyTorch RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = PyTorchRNN(input_size=1, hidden_size=8, output_size=1)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training\n",
    "print(\"ðŸš€ Training PyTorch RNN...\\n\")\n",
    "epochs = 200\n",
    "losses_pytorch = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in zip(X_train_torch, y_train_torch):\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(x)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(X_train_torch)\n",
    "    losses_pytorch.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Compare Training Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# From scratch\n",
    "ax1.plot(losses, linewidth=2, color='#e74c3c')\n",
    "ax1.set_xlabel('Epoch', fontweight='bold')\n",
    "ax1.set_ylabel('Loss', fontweight='bold')\n",
    "ax1.set_title('From Scratch (NumPy)', fontweight='bold', fontsize=13)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# PyTorch\n",
    "ax2.plot(losses_pytorch, linewidth=2, color='#3498db')\n",
    "ax2.set_xlabel('Epoch', fontweight='bold')\n",
    "ax2.set_ylabel('Loss', fontweight='bold')\n",
    "ax2.set_title('PyTorch Implementation', fontweight='bold', fontsize=13)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"From Scratch - Final Loss: {losses[-1]:.6f}\")\n",
    "print(f\"PyTorch - Final Loss: {losses_pytorch[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Test PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PyTorch model\n",
    "model.eval()\n",
    "\n",
    "print(\"ðŸ§ª Testing PyTorch RNN Predictions:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (x, y, scenario) in enumerate(zip(X_train_torch, y_train_torch, \n",
    "                                              [\"Lowâ†’Low\", \"Lowâ†’Med\", \"Highâ†’Med\", \"Highâ†’High\"])):\n",
    "        prediction, _ = model(x)\n",
    "        pred_val = prediction.item()\n",
    "        expected_val = y.item()\n",
    "        \n",
    "        expected_label = \"Low\" if expected_val == 0 else \"High\"\n",
    "        pred_label = \"Low\" if pred_val < 0.25 else (\"High\" if pred_val > 0.75 else \"Medium\")\n",
    "        \n",
    "        print(f\"\\n{scenario} â†’ Expected: {expected_label} ({expected_val})\")\n",
    "        print(f\"   Predicted: {pred_label} ({pred_val:.4f})\")\n",
    "        print(f\"   âœ“ Correct!\" if abs(pred_val - expected_val) < 0.3 else \"   âœ— Needs more training\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The Vanishing/Exploding Gradient Problem\n",
    "\n",
    "This is the **biggest challenge** with vanilla RNNs and explains why they're rarely used in practice!\n",
    "\n",
    "### The Problem\n",
    "\n",
    "When we unroll an RNN many times (long sequences), the gradient gets multiplied by the recurrent weight $W_2$ repeatedly:\n",
    "\n",
    "$$\\text{Gradient} \\propto W_2^T$$\n",
    "\n",
    "Where $T$ is the number of time steps.\n",
    "\n",
    "- If $|W_2| > 1$ â†’ **Exploding Gradients** ðŸ’¥\n",
    "- If $|W_2| < 1$ â†’ **Vanishing Gradients** ðŸ‘»\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_gradient_problem():\n",
    "    \"\"\"\n",
    "    Visualize how gradients explode or vanish based on W2 value.\n",
    "    \"\"\"\n",
    "    timesteps = np.arange(1, 51)\n",
    "    \n",
    "    # Different values of W2\n",
    "    w2_values = [0.5, 0.9, 1.0, 1.1, 2.0]\n",
    "    colors = ['#3498db', '#9b59b6', '#95a5a6', '#e67e22', '#e74c3c']\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Linear scale\n",
    "    for w2, color in zip(w2_values, colors):\n",
    "        gradient_magnitude = w2 ** timesteps\n",
    "        ax1.plot(timesteps, gradient_magnitude, linewidth=2.5, \n",
    "                label=f'Wâ‚‚ = {w2}', color=color)\n",
    "    \n",
    "    ax1.set_xlabel('Number of Time Steps', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Gradient Magnitude', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Gradient Magnitude (Linear Scale)', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim([0, 20])\n",
    "    \n",
    "    # Add annotations\n",
    "    ax1.axhline(y=1, color='green', linestyle='--', linewidth=2, label='Ideal')\n",
    "    ax1.text(25, 15, 'EXPLODING\\nGRADIENTS\\nðŸ’¥', fontsize=12, \n",
    "            fontweight='bold', color='red',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))\n",
    "    \n",
    "    # Log scale\n",
    "    for w2, color in zip(w2_values, colors):\n",
    "        gradient_magnitude = w2 ** timesteps\n",
    "        ax2.semilogy(timesteps, gradient_magnitude, linewidth=2.5, \n",
    "                    label=f'Wâ‚‚ = {w2}', color=color)\n",
    "    \n",
    "    ax2.set_xlabel('Number of Time Steps', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Gradient Magnitude (log scale)', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Gradient Magnitude (Log Scale)', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations\n",
    "    ax2.axhline(y=1, color='green', linestyle='--', linewidth=2)\n",
    "    ax2.text(25, 1e-10, 'VANISHING\\nGRADIENTS\\nðŸ‘»', fontsize=12, \n",
    "            fontweight='bold', color='blue',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_gradient_problem()\n",
    "\n",
    "print(\"\\nâš ï¸ Key Insights:\")\n",
    "print(\"   â€¢ Wâ‚‚ > 1: Gradients EXPLODE exponentially (red/orange lines)\")\n",
    "print(\"   â€¢ Wâ‚‚ < 1: Gradients VANISH exponentially (blue/purple lines)\")\n",
    "print(\"   â€¢ Wâ‚‚ = 1: Stable, but loses expressiveness\")\n",
    "print(\"\\nðŸ’¡ Solution: Long Short-Term Memory (LSTM) networks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Numerical Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show exact numbers\n",
    "print(\"ðŸ”¢ Numerical Examples:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "timesteps_examples = [10, 20, 50]\n",
    "w2_examples = [0.5, 2.0]\n",
    "\n",
    "for w2 in w2_examples:\n",
    "    problem_type = \"VANISHING\" if w2 < 1 else \"EXPLODING\"\n",
    "    print(f\"\\nWâ‚‚ = {w2} ({problem_type} Gradients):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for t in timesteps_examples:\n",
    "        magnitude = w2 ** t\n",
    "        print(f\"  After {t:2d} timesteps: {magnitude:.2e}\")\n",
    "    \n",
    "    if w2 < 1:\n",
    "        print(f\"  â†’ Gradient becomes â‰ˆ0, can't learn long-term!\")\n",
    "    else:\n",
    "        print(f\"  â†’ Gradient explodes, can't converge!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Visualize the Impact on Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_optimization_landscape():\n",
    "    \"\"\"\n",
    "    Show how exploding/vanishing gradients affect optimization.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Create a simple loss landscape\n",
    "    x = np.linspace(-2, 2, 100)\n",
    "    loss = x**2  # Simple quadratic loss\n",
    "    \n",
    "    # Exploding gradients - large steps\n",
    "    ax1.plot(x, loss, 'b-', linewidth=3, label='Loss Function')\n",
    "    \n",
    "    # Simulate gradient descent with large steps\n",
    "    positions_explode = [-1.5, 1.8, -1.9, 1.7, -1.6, 2.0]  # Bouncing around\n",
    "    for i, pos in enumerate(positions_explode):\n",
    "        ax1.plot(pos, pos**2, 'ro', markersize=12)\n",
    "        if i < len(positions_explode) - 1:\n",
    "            ax1.annotate('', xy=(positions_explode[i+1], positions_explode[i+1]**2),\n",
    "                        xytext=(pos, pos**2),\n",
    "                        arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n",
    "    \n",
    "    ax1.plot(0, 0, 'g*', markersize=20, label='Optimal')\n",
    "    ax1.set_xlabel('Parameter Value', fontweight='bold', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontweight='bold', fontsize=12)\n",
    "    ax1.set_title('EXPLODING Gradients\\n(Steps Too Large)', \n",
    "                  fontweight='bold', fontsize=13, color='red')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.text(0, 3.5, 'ðŸ’¥ Bouncing around!\\nCannot converge', \n",
    "            ha='center', fontsize=11, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))\n",
    "    \n",
    "    # Vanishing gradients - tiny steps\n",
    "    ax2.plot(x, loss, 'b-', linewidth=3, label='Loss Function')\n",
    "    \n",
    "    # Simulate gradient descent with tiny steps\n",
    "    positions_vanish = [-1.5, -1.48, -1.46, -1.44, -1.42, -1.40]  # Very slow\n",
    "    for i, pos in enumerate(positions_vanish):\n",
    "        ax2.plot(pos, pos**2, 'ro', markersize=12)\n",
    "        if i < len(positions_vanish) - 1:\n",
    "            ax2.annotate('', xy=(positions_vanish[i+1], positions_vanish[i+1]**2),\n",
    "                        xytext=(pos, pos**2),\n",
    "                        arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n",
    "    \n",
    "    ax2.plot(0, 0, 'g*', markersize=20, label='Optimal')\n",
    "    ax2.set_xlabel('Parameter Value', fontweight='bold', fontsize=12)\n",
    "    ax2.set_ylabel('Loss', fontweight='bold', fontsize=12)\n",
    "    ax2.set_title('VANISHING Gradients\\n(Steps Too Small)', \n",
    "                  fontweight='bold', fontsize=13, color='blue')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.text(0, 3.5, 'ðŸ‘» Moving too slowly!\\nNever reaches optimum', \n",
    "            ha='center', fontsize=11, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_optimization_landscape()\n",
    "\n",
    "print(\"\\nâš ï¸ Both problems prevent effective training!\")\n",
    "print(\"   â€¢ Exploding: Takes huge steps, bounces around\")\n",
    "print(\"   â€¢ Vanishing: Takes tiny steps, gets stuck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Solutions to the Gradient Problem\n",
    "\n",
    "### 9.1 Gradient Clipping (Partial Solution)\n",
    "\n",
    "A simple technique to prevent exploding gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_gradient_clipping():\n",
    "    \"\"\"\n",
    "    Show how gradient clipping helps with exploding gradients.\n",
    "    \"\"\"\n",
    "    # Simulate gradients\n",
    "    np.random.seed(42)\n",
    "    original_gradients = np.random.randn(100) * 10  # Some very large gradients\n",
    "    \n",
    "    # Apply clipping\n",
    "    clip_value = 5.0\n",
    "    clipped_gradients = np.clip(original_gradients, -clip_value, clip_value)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Original gradients\n",
    "    ax1.hist(original_gradients, bins=30, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "    ax1.axvline(-clip_value, color='orange', linestyle='--', linewidth=3, label='Clip threshold')\n",
    "    ax1.axvline(clip_value, color='orange', linestyle='--', linewidth=3)\n",
    "    ax1.set_xlabel('Gradient Value', fontweight='bold')\n",
    "    ax1.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax1.set_title('Original Gradients (Some Exploding!)', fontweight='bold', fontsize=13)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Clipped gradients\n",
    "    ax2.hist(clipped_gradients, bins=30, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "    ax2.axvline(-clip_value, color='green', linestyle='--', linewidth=3, label='Clip threshold')\n",
    "    ax2.axvline(clip_value, color='green', linestyle='--', linewidth=3)\n",
    "    ax2.set_xlabel('Gradient Value', fontweight='bold')\n",
    "    ax2.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax2.set_title('Clipped Gradients (Controlled!)', fontweight='bold', fontsize=13)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Original gradient range: [{original_gradients.min():.2f}, {original_gradients.max():.2f}]\")\n",
    "    print(f\"Clipped gradient range:  [{clipped_gradients.min():.2f}, {clipped_gradients.max():.2f}]\")\n",
    "    print(f\"\\nâœ“ Clipping prevents exploding gradients!\")\n",
    "    print(f\"âš ï¸ But doesn't solve vanishing gradients...\")\n",
    "\n",
    "demonstrate_gradient_clipping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Better Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different weight initializations\n",
    "def compare_initializations():\n",
    "    timesteps = np.arange(1, 31)\n",
    "    \n",
    "    # Different initialization strategies\n",
    "    init_strategies = {\n",
    "        'Random [-1, 1]': np.random.uniform(-1, 1),\n",
    "        'Xavier': np.sqrt(1.0 / 8),  # sqrt(1/hidden_size)\n",
    "        'He': np.sqrt(2.0 / 8),\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for name, w2 in init_strategies.items():\n",
    "        magnitude = np.abs(w2) ** timesteps\n",
    "        plt.semilogy(timesteps, magnitude, linewidth=2.5, label=f'{name} (Wâ‚‚={w2:.3f})')\n",
    "    \n",
    "    plt.axhline(y=1, color='green', linestyle='--', linewidth=2, label='Ideal (=1)')\n",
    "    plt.xlabel('Number of Time Steps', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Gradient Magnitude (log scale)', fontsize=12, fontweight='bold')\n",
    "    plt.title('Effect of Weight Initialization on Gradient Flow', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ“ Better initialization helps, but doesn't fully solve the problem\")\n",
    "\n",
    "compare_initializations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 The Real Solution: LSTM & GRU\n",
    "\n",
    "The **real solution** to vanishing/exploding gradients is to use:\n",
    "\n",
    "1. **LSTM (Long Short-Term Memory)** - Introduced in 1997\n",
    "2. **GRU (Gated Recurrent Unit)** - Introduced in 2014\n",
    "\n",
    "These architectures use **gates** to control information flow and prevent gradients from vanishing!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Real-World Applications\n",
    "\n",
    "### Where are RNNs (and their variants) used?\n",
    "\n",
    "1. **Natural Language Processing**\n",
    "   - Machine translation\n",
    "   - Text generation\n",
    "   - Sentiment analysis\n",
    "\n",
    "2. **Time Series Prediction**\n",
    "   - Stock prices\n",
    "   - Weather forecasting\n",
    "   - Sales forecasting\n",
    "\n",
    "3. **Speech Recognition**\n",
    "   - Converting speech to text\n",
    "   - Voice assistants\n",
    "\n",
    "4. **Video Analysis**\n",
    "   - Action recognition\n",
    "   - Video captioning\n",
    "\n",
    "5. **Music Generation**\n",
    "   - Composing melodies\n",
    "   - Style transfer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Extended Example: Multi-Day Stock Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a longer sequence\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a more complex stock pattern\n",
    "def generate_stock_sequence(length=10):\n",
    "    \"\"\"\n",
    "    Generate a stock price sequence following StatLand rules with some noise.\n",
    "    \"\"\"\n",
    "    prices = [np.random.choice([0, 0.5, 1])]\n",
    "    \n",
    "    for i in range(1, length):\n",
    "        prev_price = prices[-1]\n",
    "        \n",
    "        if i >= 2:\n",
    "            prev_prev_price = prices[-2]\n",
    "        else:\n",
    "            prev_prev_price = prev_price\n",
    "        \n",
    "        # Apply StatLand rules with some randomness\n",
    "        if prev_prev_price == 0 and prev_price == 0:\n",
    "            next_price = 0 if np.random.rand() < 0.8 else 0.5\n",
    "        elif prev_prev_price == 0 and prev_price == 0.5:\n",
    "            next_price = 1 if np.random.rand() < 0.8 else 0.5\n",
    "        elif prev_prev_price == 1 and prev_price == 0.5:\n",
    "            next_price = 0 if np.random.rand() < 0.8 else 0.5\n",
    "        elif prev_prev_price == 1 and prev_price == 1:\n",
    "            next_price = 1 if np.random.rand() < 0.8 else 0.5\n",
    "        else:\n",
    "            next_price = np.random.choice([0, 0.5, 1])\n",
    "        \n",
    "        prices.append(next_price)\n",
    "    \n",
    "    return prices\n",
    "\n",
    "# Generate sequences\n",
    "sequence_length = 15\n",
    "stock_sequence = generate_stock_sequence(sequence_length)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 6))\n",
    "days = np.arange(1, len(stock_sequence) + 1)\n",
    "\n",
    "plt.plot(days, stock_sequence, 'o-', linewidth=3, markersize=10, color='#3498db')\n",
    "plt.xlabel('Day', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Stock Price (Normalized)', fontsize=12, fontweight='bold')\n",
    "plt.title('Extended Stock Price Sequence in StatLand', fontsize=14, fontweight='bold')\n",
    "plt.yticks([0, 0.5, 1], ['Low (0)', 'Medium (0.5)', 'High (1)'])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations for patterns\n",
    "for i in range(1, len(stock_sequence)):\n",
    "    if i >= 2:\n",
    "        if stock_sequence[i-2] == 0 and stock_sequence[i-1] == 0 and stock_sequence[i] == 0:\n",
    "            plt.annotate('Lâ†’Lâ†’L', xy=(i, stock_sequence[i]), \n",
    "                        xytext=(i, stock_sequence[i]-0.15),\n",
    "                        fontsize=8, ha='center', color='blue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated sequence of length {len(stock_sequence)}:\")\n",
    "print(stock_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test RNN on Longer Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our trained RNN to make predictions on this longer sequence\n",
    "def sliding_window_predictions(sequence, window_size=2):\n",
    "    \"\"\"\n",
    "    Make predictions using a sliding window approach.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(len(sequence) - window_size):\n",
    "        window = sequence[i:i+window_size]\n",
    "        pred = rnn.predict(window)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Make predictions\n",
    "window_size = 2\n",
    "predictions = sliding_window_predictions(stock_sequence, window_size)\n",
    "actual = stock_sequence[window_size:]\n",
    "\n",
    "# Visualize predictions vs actual\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Predictions vs Actual\n",
    "days = np.arange(window_size + 1, len(stock_sequence) + 1)\n",
    "ax1.plot(days, actual, 'o-', linewidth=3, markersize=10, \n",
    "         label='Actual', color='#2ecc71', alpha=0.8)\n",
    "ax1.plot(days, predictions, 's--', linewidth=2, markersize=8, \n",
    "         label='Predicted', color='#e74c3c', alpha=0.8)\n",
    "ax1.set_xlabel('Day', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Stock Price', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('RNN Predictions on Extended Sequence', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Prediction Error\n",
    "errors = [abs(p - a) for p, a in zip(predictions, actual)]\n",
    "ax2.bar(days, errors, color='#e74c3c', alpha=0.6, edgecolor='black')\n",
    "ax2.axhline(y=0.25, color='orange', linestyle='--', linewidth=2, \n",
    "            label='Acceptable error threshold')\n",
    "ax2.set_xlabel('Day', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Absolute Error', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Prediction Errors', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate metrics\n",
    "mae = np.mean(errors)\n",
    "mse = np.mean([e**2 for e in errors])\n",
    "accurate_predictions = sum([1 for e in errors if e < 0.25])\n",
    "accuracy = accurate_predictions / len(errors) * 100\n",
    "\n",
    "print(f\"\\nðŸ“Š Performance Metrics:\")\n",
    "print(f\"   Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"   Mean Squared Error:  {mse:.4f}\")\n",
    "print(f\"   Accuracy (< 0.25 error): {accuracy:.1f}%\")\n",
    "print(f\"   Correct predictions: {accurate_predictions}/{len(errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Key Takeaways\n",
    "\n",
    "### âœ… What We Learned\n",
    "\n",
    "1. **RNNs are designed for sequential data**\n",
    "   - Handle variable-length inputs\n",
    "   - Use feedback loops for memory\n",
    "   - Share weights across time steps\n",
    "\n",
    "2. **Unrolling reveals the structure**\n",
    "   - Makes it easier to understand information flow\n",
    "   - Each time step is a copy of the same network\n",
    "   - Shared parameters across all copies\n",
    "\n",
    "3. **The Vanishing/Exploding Gradient Problem**\n",
    "   - Major limitation of vanilla RNNs\n",
    "   - Prevents learning long-term dependencies\n",
    "   - Solved by LSTM and GRU architectures\n",
    "\n",
    "4. **Implementation Approaches**\n",
    "   - From scratch: Understand the fundamentals\n",
    "   - With libraries: Practical applications\n",
    "\n",
    "### ðŸŽ¯ Next Steps\n",
    "\n",
    "1. **Learn about LSTM networks** - The modern solution to gradient problems\n",
    "2. **Explore GRU networks** - Simpler alternative to LSTMs\n",
    "3. **Study Transformers** - State-of-the-art for many sequence tasks\n",
    "4. **Apply to real datasets** - Try time series, NLP, or other sequential data\n",
    "\n",
    "### ðŸ“š Additional Resources\n",
    "\n",
    "- StatQuest Videos: Excellent visual explanations\n",
    "- \"Understanding LSTM Networks\" by Christopher Olah\n",
    "- PyTorch/TensorFlow documentation for practical implementations\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've completed a comprehensive tutorial on Recurrent Neural Networks! You now understand:\n",
    "- How RNNs work internally\n",
    "- Why they're useful for sequential data\n",
    "- Their limitations and solutions\n",
    "- How to implement them from scratch and with libraries\n",
    "\n",
    "**Keep learning and Quest on! ðŸš€**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Interactive RNN Playground\n",
    "\n",
    "Try experimenting with different parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_playground(hidden_size=8, learning_rate=0.05, epochs=200):\n",
    "    \"\"\"\n",
    "    Interactive function to experiment with RNN parameters.\n",
    "    \n",
    "    Try different values:\n",
    "    - hidden_size: 2, 4, 8, 16, 32\n",
    "    - learning_rate: 0.001, 0.01, 0.05, 0.1\n",
    "    - epochs: 50, 100, 200, 500\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸŽ® RNN Playground\")\n",
    "    print(f\"   Hidden size: {hidden_size}\")\n",
    "    print(f\"   Learning rate: {learning_rate}\")\n",
    "    print(f\"   Epochs: {epochs}\\n\")\n",
    "    \n",
    "    # Train RNN with given parameters\n",
    "    test_rnn = SimpleRNN(input_size=1, hidden_size=hidden_size, \n",
    "                        output_size=1, learning_rate=learning_rate)\n",
    "    losses = test_rnn.train(X_train, y_train, epochs=epochs, verbose=False)\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Training curve\n",
    "    ax1.plot(losses, linewidth=2, color='#3498db')\n",
    "    ax1.set_xlabel('Epoch', fontweight='bold')\n",
    "    ax1.set_ylabel('Loss', fontweight='bold')\n",
    "    ax1.set_title(f'Training Loss (Final: {losses[-1]:.6f})', fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Predictions\n",
    "    predictions = [test_rnn.predict(x) for x in X_train]\n",
    "    expected = [y[-1] for y in y_train]\n",
    "    \n",
    "    x_pos = np.arange(len(X_train))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax2.bar(x_pos - width/2, expected, width, label='Expected', \n",
    "           color='#2ecc71', alpha=0.8)\n",
    "    ax2.bar(x_pos + width/2, predictions, width, label='Predicted', \n",
    "           color='#e74c3c', alpha=0.8)\n",
    "    \n",
    "    ax2.set_xlabel('Scenario', fontweight='bold')\n",
    "    ax2.set_ylabel('Value', fontweight='bold')\n",
    "    ax2.set_title('Predictions vs Expected', fontweight='bold')\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(['Lâ†’L', 'Lâ†’M', 'Hâ†’M', 'Hâ†’H'])\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    errors = [abs(p - e) for p, e in zip(predictions, expected)]\n",
    "    accuracy = sum([1 for e in errors if e < 0.25]) / len(errors) * 100\n",
    "    \n",
    "    print(f\"\\nâœ“ Accuracy: {accuracy:.1f}%\")\n",
    "    print(f\"âœ“ Avg Error: {np.mean(errors):.4f}\")\n",
    "    \n",
    "    return test_rnn, losses\n",
    "\n",
    "# Try it out!\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Try experimenting with different parameters:\")\n",
    "print(\"  rnn_playground(hidden_size=16, learning_rate=0.01, epochs=300)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example run\n",
    "rnn_playground(hidden_size=8, learning_rate=0.05, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸŽ“ End of Tutorial\n",
    "\n",
    "**Thank you for learning about Recurrent Neural Networks!**\n",
    "\n",
    "Remember:\n",
    "- RNNs are great for sequential data\n",
    "- They use feedback loops to create memory\n",
    "- Vanilla RNNs have gradient problems\n",
    "- LSTM and GRU solve these problems\n",
    "\n",
    "**Keep learning, keep building, and Quest on! ðŸš€**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
