{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Networks: Inside the Black Box\n",
        "## A Complete Tutorial from Concept to Code\n",
        "\n",
        "**Based on StatQuest Neural Networks Tutorial**\n",
        "\n",
        "---\n",
        "\n",
        "### Learning Objectives:\n",
        "1. Understand what neural networks are and how they work\n",
        "2. Learn about activation functions (Softplus, ReLU, Sigmoid)\n",
        "3. Build intuition with real-world examples\n",
        "4. Implement neural networks using libraries (TensorFlow/Keras)\n",
        "5. Implement neural networks from scratch (NumPy)\n",
        "6. Visualize neural network operations\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Slide 1: What is a Neural Network?\n",
        "\n",
        "### Key Concept:\n",
        "**Neural networks are NOT black boxes - they are \"Big Fancy Squiggle Fitting Machines\"!**\n",
        "\n",
        "### Components:\n",
        "1. **Nodes** (neurons)\n",
        "2. **Connections** (synapses)\n",
        "3. **Weights** (parameters that multiply inputs)\n",
        "4. **Biases** (parameters that shift results)\n",
        "5. **Activation Functions** (curved/bent lines that create non-linearity)\n",
        "\n",
        "### Architecture:\n",
        "- **Input Layer**: Where we feed data\n",
        "- **Hidden Layer(s)**: Where transformations happen\n",
        "- **Output Layer**: Where we get predictions\n",
        "\n",
        "### The Magic:\n",
        "Neural networks can fit complex patterns (squiggles) to data that simple straight lines cannot!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (uncomment if needed)\n",
        "# !pip install numpy matplotlib tensorflow scikit-learn\n",
        "\n",
        "# Import all necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import FancyBboxPatch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\u2713 All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Slide 2: Real-World Problem - Drug Dosage Effectiveness\n",
        "\n",
        "### The Dataset:\n",
        "We tested a drug at different dosages and measured effectiveness:\n",
        "\n",
        "| Dosage Level | Normalized Dosage | Effective? |\n",
        "|--------------|-------------------|------------|\n",
        "| Low          | 0.0 - 0.3        | No (0)     |\n",
        "| Medium       | 0.4 - 0.6        | Yes (1)    |\n",
        "| High         | 0.7 - 1.0        | No (0)     |\n",
        "\n",
        "### The Challenge:\n",
        "\u274c A straight line cannot fit this data accurately!  \n",
        "\u2713 We need a **curved line (squiggle)** to make good predictions\n",
        "\n",
        "### Goal:\n",
        "Build a neural network that predicts whether a given dosage will be effective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Slide 2 Code: Create the drug dosage dataset\n",
        "\n",
        "# Generate synthetic data based on the problem description\n",
        "np.random.seed(42)\n",
        "\n",
        "# Low dosage (0-0.3): Not effective\n",
        "low_dosage = np.random.uniform(0.0, 0.3, 15)\n",
        "low_effectiveness = np.random.uniform(0.0, 0.2, 15)  # Close to 0\n",
        "\n",
        "# Medium dosage (0.4-0.6): Effective\n",
        "medium_dosage = np.random.uniform(0.4, 0.6, 15)\n",
        "medium_effectiveness = np.random.uniform(0.8, 1.0, 15)  # Close to 1\n",
        "\n",
        "# High dosage (0.7-1.0): Not effective\n",
        "high_dosage = np.random.uniform(0.7, 1.0, 15)\n",
        "high_effectiveness = np.random.uniform(0.0, 0.2, 15)  # Close to 0\n",
        "\n",
        "# Combine all data\n",
        "X_data = np.concatenate([low_dosage, medium_dosage, high_dosage])\n",
        "y_data = np.concatenate([low_effectiveness, medium_effectiveness, high_effectiveness])\n",
        "\n",
        "# Visualize the data\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_data, y_data, c='blue', s=100, alpha=0.6, edgecolors='black', linewidth=1.5)\n",
        "plt.xlabel('Dosage (normalized 0-1)', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Effectiveness (0=No, 1=Yes)', fontsize=12, fontweight='bold')\n",
        "plt.title('Drug Dosage Effectiveness Dataset\\n(Notice: No straight line can fit this!)', \n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.ylim(-0.1, 1.1)\n",
        "plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Decision Boundary')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Dataset created:\")\n",
        "print(f\"  - Total samples: {len(X_data)}\")\n",
        "print(f\"  - Dosage range: [{X_data.min():.2f}, {X_data.max():.2f}]\")\n",
        "print(f\"  - Effectiveness range: [{y_data.min():.2f}, {y_data.max():.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Slide 3: Activation Functions - The Building Blocks\n",
        "\n",
        "### What are Activation Functions?\n",
        "Activation functions are **curved or bent lines** that allow neural networks to learn complex patterns.\n",
        "\n",
        "### Three Common Activation Functions:\n",
        "\n",
        "#### 1. **Softplus** (used in our example)\n",
        "- Formula: \\\\( f(x) = \\\\ln(1 + e^x) \\\\)\n",
        "- Smooth, differentiable approximation of ReLU\n",
        "- Never returns exactly 0\n",
        "\n",
        "#### 2. **ReLU** (Rectified Linear Unit)\n",
        "- Formula: \\\\( f(x) = \\\\max(0, x) \\\\)\n",
        "- Most popular in practice\n",
        "- Simple: returns 0 for negative inputs, x for positive inputs\n",
        "\n",
        "#### 3. **Sigmoid**\n",
        "- Formula: \\\\( f(x) = \\\\frac{1}{1 + e^{-x}} \\\\)\n",
        "- S-shaped curve\n",
        "- Outputs between 0 and 1\n",
        "- Often used in output layers for binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Slide 3 Code: Implement and visualize activation functions\n",
        "\n",
        "# Define activation functions\n",
        "def softplus(x):\n",
        "    \"\"\"Softplus activation: ln(1 + e^x)\"\"\"\n",
        "    return np.log(1 + np.exp(np.clip(x, -500, 500)))  # Clip to prevent overflow\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"ReLU activation: max(0, x)\"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation: 1 / (1 + e^(-x))\"\"\"\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip to prevent overflow\n",
        "\n",
        "# Create x values for plotting\n",
        "x = np.linspace(-5, 5, 1000)\n",
        "\n",
        "# Calculate activation function outputs\n",
        "y_softplus = softplus(x)\n",
        "y_relu = relu(x)\n",
        "y_sigmoid = sigmoid(x)\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Softplus\n",
        "axes[0].plot(x, y_softplus, 'b-', linewidth=3, label='Softplus')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].set_xlabel('Input (x)', fontweight='bold')\n",
        "axes[0].set_ylabel('Output f(x)', fontweight='bold')\n",
        "axes[0].set_title('Softplus Activation\\nf(x) = ln(1 + e^x)', fontweight='bold')\n",
        "axes[0].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
        "axes[0].axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
        "axes[0].legend()\n",
        "\n",
        "# ReLU\n",
        "axes[1].plot(x, y_relu, 'r-', linewidth=3, label='ReLU')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_xlabel('Input (x)', fontweight='bold')\n",
        "axes[1].set_ylabel('Output f(x)', fontweight='bold')\n",
        "axes[1].set_title('ReLU Activation\\nf(x) = max(0, x)', fontweight='bold')\n",
        "axes[1].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
        "axes[1].axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
        "axes[1].legend()\n",
        "\n",
        "# Sigmoid\n",
        "axes[2].plot(x, y_sigmoid, 'g-', linewidth=3, label='Sigmoid')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "axes[2].set_xlabel('Input (x)', fontweight='bold')\n",
        "axes[2].set_ylabel('Output f(x)', fontweight='bold')\n",
        "axes[2].set_title('Sigmoid Activation\\nf(x) = 1/(1 + e^(-x))', fontweight='bold')\n",
        "axes[2].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
        "axes[2].axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
        "axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Key Properties:\")\n",
        "print(\"  Softplus: Smooth, always positive, approximates ReLU\")\n",
        "print(\"  ReLU: Simple, fast, most popular in practice\")\n",
        "print(\"  Sigmoid: S-shaped, outputs between 0 and 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Slide 4: How Neural Networks Create Squiggles\n",
        "\n",
        "### The Process (Step-by-Step):\n",
        "\n",
        "1. **Start with identical activation functions** in the hidden layer\n",
        "\n",
        "2. **Transform each activation function:**\n",
        "   - **Slice**: Use only a portion of the curve (via weights and biases)\n",
        "   - **Flip**: Negative weights flip the curve\n",
        "   - **Stretch**: Multiply y-values to change amplitude\n",
        "   - **Shift**: Add bias to move the curve up/down\n",
        "\n",
        "3. **Add transformed curves together** to create new shapes\n",
        "\n",
        "4. **Final adjustment** to fit the data\n",
        "\n",
        "### The Neural Network from the Video:\n",
        "```\n",
        "Input (dosage)\n",
        "    \u2193\n",
        "Hidden Layer:\n",
        "  Node 1: dosage \u00d7 (-34.4) + 2.14 \u2192 softplus() \u2192 \u00d7 (-1.3)\n",
        "  Node 2: dosage \u00d7 (-2.52) + 1.29 \u2192 softplus() \u2192 \u00d7 2.28\n",
        "    \u2193\n",
        "Add both nodes + (-0.58) \u2192 Output (effectiveness)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Slide 4 Code: Demonstrate how neural networks create squiggles\n",
        "\n",
        "# Parameters from the StatQuest video\n",
        "# Hidden Layer Node 1\n",
        "w1_hidden1 = -34.4  # weight from input to hidden node 1\n",
        "b1_hidden1 = 2.14   # bias for hidden node 1\n",
        "w2_hidden1 = -1.3   # weight from hidden node 1 to output\n",
        "\n",
        "# Hidden Layer Node 2\n",
        "w1_hidden2 = -2.52  # weight from input to hidden node 2\n",
        "b1_hidden2 = 1.29   # bias for hidden node 2\n",
        "w2_hidden2 = 2.28   # weight from hidden node 2 to output\n",
        "\n",
        "# Output bias\n",
        "b_output = -0.58\n",
        "\n",
        "# Create dosage range for visualization\n",
        "dosage_range = np.linspace(0, 1, 1000)\n",
        "\n",
        "# STEP 1: Calculate hidden layer node 1 outputs\n",
        "z1 = dosage_range * w1_hidden1 + b1_hidden1  # Linear transformation\n",
        "a1_before_scale = softplus(z1)                 # Apply activation\n",
        "a1 = a1_before_scale * w2_hidden1             # Scale by output weight\n",
        "\n",
        "# STEP 2: Calculate hidden layer node 2 outputs\n",
        "z2 = dosage_range * w1_hidden2 + b1_hidden2  # Linear transformation\n",
        "a2_before_scale = softplus(z2)                 # Apply activation\n",
        "a2 = a2_before_scale * w2_hidden2             # Scale by output weight\n",
        "\n",
        "# STEP 3: Combine both nodes\n",
        "combined = a1 + a2\n",
        "\n",
        "# STEP 4: Add output bias to get final prediction\n",
        "final_output = combined + b_output\n",
        "\n",
        "# Visualize the transformation process\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "\n",
        "# Row 1: Node 1 transformation\n",
        "axes[0, 0].plot(dosage_range, a1_before_scale, 'b-', linewidth=2)\n",
        "axes[0, 0].set_title('Node 1: After Softplus\\n(before scaling)', fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Dosage')\n",
        "axes[0, 0].set_ylabel('Activation')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 1].plot(dosage_range, a1, 'b-', linewidth=2)\n",
        "axes[0, 1].set_title(f'Node 1: After Scaling\\n(\u00d7 {w2_hidden1})', fontweight='bold', color='blue')\n",
        "axes[0, 1].set_xlabel('Dosage')\n",
        "axes[0, 1].set_ylabel('Scaled Activation')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 2].plot(dosage_range, a2_before_scale, 'orange', linewidth=2)\n",
        "axes[0, 2].set_title('Node 2: After Softplus\\n(before scaling)', fontweight='bold')\n",
        "axes[0, 2].set_xlabel('Dosage')\n",
        "axes[0, 2].set_ylabel('Activation')\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# Row 2: Node 2 transformation and combination\n",
        "axes[1, 0].plot(dosage_range, a2, 'orange', linewidth=2)\n",
        "axes[1, 0].set_title(f'Node 2: After Scaling\\n(\u00d7 {w2_hidden2})', fontweight='bold', color='orange')\n",
        "axes[1, 0].set_xlabel('Dosage')\n",
        "axes[1, 0].set_ylabel('Scaled Activation')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 1].plot(dosage_range, a1, 'b-', linewidth=2, label='Node 1', alpha=0.6)\n",
        "axes[1, 1].plot(dosage_range, a2, 'orange', linewidth=2, label='Node 2', alpha=0.6)\n",
        "axes[1, 1].plot(dosage_range, combined, 'purple', linewidth=3, label='Combined')\n",
        "axes[1, 1].set_title('Adding Both Nodes', fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Dosage')\n",
        "axes[1, 1].set_ylabel('Output')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 2].scatter(X_data, y_data, c='gray', s=50, alpha=0.5, label='Data', zorder=1)\n",
        "axes[1, 2].plot(dosage_range, final_output, 'green', linewidth=4, label='Final Squiggle', zorder=2)\n",
        "axes[1, 2].set_title('Final Output: The Green Squiggle!', fontweight='bold', color='green', fontsize=12)\n",
        "axes[1, 2].set_xlabel('Dosage')\n",
        "axes[1, 2].set_ylabel('Effectiveness')\n",
        "axes[1, 2].legend()\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "axes[1, 2].set_ylim(-0.1, 1.1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83c\udf89 This is how neural networks create squiggles from curved activation functions!\")\n",
        "print(\"\\nKey Insight:\")\n",
        "print(\"  - Each node transforms the activation function differently\")\n",
        "print(\"  - When added together, they create complex patterns\")\n",
        "print(\"  - This allows fitting to non-linear data!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Slide 5: Making Predictions with Neural Networks\n",
        "\n",
        "### Using the Trained Network:\n",
        "\n",
        "Once we have the trained weights and biases, making predictions is straightforward!\n",
        "\n",
        "### Example Prediction:\n",
        "**Question:** Will a dosage of 0.5 be effective?\n",
        "\n",
        "**Step-by-step calculation:**\n",
        "1. Node 1: `0.5 \u00d7 (-34.4) + 2.14 = -15.06` \u2192 `softplus(-15.06) = 0.00` \u2192 `0.00 \u00d7 (-1.3) = 0.00`\n",
        "2. Node 2: `0.5 \u00d7 (-2.52) + 1.29 = 0.03` \u2192 `softplus(0.03) = 0.74` \u2192 `0.74 \u00d7 2.28 = 1.69`\n",
        "3. Output: `0.00 + 1.69 + (-0.58) = 1.11` \u2192 **Effective!** (closer to 1 than 0)\n",
        "\n",
        "### Decision Rule:\n",
        "- Output > 0.5 \u2192 **Effective**\n",
        "- Output \u2264 0.5 \u2192 **Not Effective**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Slide 5 Code: Make predictions with the neural network\n",
        "\n",
        "def predict_effectiveness(dosage):\n",
        "    \"\"\"\n",
        "    Predict drug effectiveness for a given dosage using our neural network.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    dosage : float\n",
        "        Drug dosage (normalized between 0 and 1)\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    prediction : float\n",
        "        Predicted effectiveness (0 = not effective, 1 = effective)\n",
        "    \"\"\"\n",
        "    # Hidden layer node 1\n",
        "    z1 = dosage * w1_hidden1 + b1_hidden1\n",
        "    a1 = softplus(z1) * w2_hidden1\n",
        "    \n",
        "    # Hidden layer node 2\n",
        "    z2 = dosage * w1_hidden2 + b1_hidden2\n",
        "    a2 = softplus(z2) * w2_hidden2\n",
        "    \n",
        "    # Output\n",
        "    output = a1 + a2 + b_output\n",
        "    \n",
        "    return output\n",
        "\n",
        "# Test predictions at different dosages\n",
        "test_dosages = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" \"*15 + \"PREDICTION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Dosage':<15} {'Prediction':<15} {'Decision':<20}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for dosage in test_dosages:\n",
        "    pred = predict_effectiveness(dosage)\n",
        "    decision = \"\u2713 EFFECTIVE\" if pred > 0.5 else \"\u2717 NOT EFFECTIVE\"\n",
        "    print(f\"{dosage:<15.2f} {pred:<15.3f} {decision:<20}\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Visualize predictions\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Plot the neural network prediction curve\n",
        "dosage_fine = np.linspace(0, 1, 1000)\n",
        "predictions = [predict_effectiveness(d) for d in dosage_fine]\n",
        "\n",
        "ax.plot(dosage_fine, predictions, 'green', linewidth=4, label='NN Prediction', zorder=3)\n",
        "ax.scatter(X_data, y_data, c='lightblue', s=100, alpha=0.6, \n",
        "           edgecolors='black', linewidth=1, label='Training Data', zorder=2)\n",
        "\n",
        "# Mark test predictions\n",
        "test_predictions = [predict_effectiveness(d) for d in test_dosages]\n",
        "ax.scatter(test_dosages, test_predictions, c='red', s=200, marker='*', \n",
        "           edgecolors='black', linewidth=2, label='Test Predictions', zorder=4)\n",
        "\n",
        "# Add decision boundary\n",
        "ax.axhline(y=0.5, color='red', linestyle='--', linewidth=2, alpha=0.5, label='Decision Boundary')\n",
        "ax.fill_between(dosage_fine, 0.5, 1.1, alpha=0.1, color='green', label='Effective Zone')\n",
        "ax.fill_between(dosage_fine, -0.1, 0.5, alpha=0.1, color='red', label='Not Effective Zone')\n",
        "\n",
        "ax.set_xlabel('Dosage (normalized)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Predicted Effectiveness', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Neural Network Predictions for Drug Effectiveness', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='upper right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_ylim(-0.1, 1.2)\n",
        "ax.set_xlim(-0.05, 1.05)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\u2713 Neural network successfully predicts drug effectiveness!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Slide 6: Implementation with Libraries (TensorFlow/Keras)\n",
        "\n",
        "### Why Use Libraries?\n",
        "- **Faster development**: Pre-built optimized functions\n",
        "- **GPU acceleration**: Automatic GPU utilization\n",
        "- **Automatic differentiation**: No manual gradient calculation\n",
        "- **Production-ready**: Battle-tested code\n",
        "\n",
        "### Our Network in Keras:\n",
        "```python\n",
        "model = Sequential([\n",
        "    Dense(2, activation='softplus', input_shape=(1,)),  # Hidden layer with 2 nodes\n",
        "    Dense(1)                                             # Output layer\n",
        "])\n",
        "```\n",
        "\n",
        "### Training Process:\n",
        "1. Define the model architecture\n",
        "2. Compile with loss function and optimizer\n",
        "3. Fit to training data\n",
        "4. Evaluate and predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Slide 6 Code: Build neural network with TensorFlow/Keras\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Prepare data\n",
        "X_train = X_data.reshape(-1, 1)  # Reshape for Keras (samples, features)\n",
        "y_train = y_data.reshape(-1, 1)  # Reshape for Keras (samples, outputs)\n",
        "\n",
        "print(\"Building Neural Network with Keras...\\n\")\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Dense(2, activation='softplus', input_shape=(1,), name='hidden_layer'),\n",
        "    Dense(1, activation='linear', name='output_layer')\n",
        "], name='Drug_Effectiveness_NN')\n",
        "\n",
        "# Display model architecture\n",
        "print(\"Model Architecture:\")\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.01),\n",
        "    loss='mean_squared_error',\n",
        "    metrics=['mae']  # Mean Absolute Error\n",
        ")\n",
        "\n",
        "print(\"\\nTraining the neural network...\")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, \n",
        "    y_train,\n",
        "    epochs=500,\n",
        "    batch_size=5,\n",
        "    verbose=0,  # Silent training\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "print(\"\u2713 Training complete!\\n\")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(X_train, y_train, verbose=0)\n",
        "print(f\"Final Training Loss: {loss:.4f}\")\n",
        "print(f\"Final Mean Absolute Error: {mae:.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "X_test_range = np.linspace(0, 1, 1000).reshape(-1, 1)\n",
        "predictions_keras = model.predict(X_test_range, verbose=0)\n",
        "\n",
        "# Visualize results\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Training history\n",
        "ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "ax1.set_xlabel('Epoch', fontweight='bold')\n",
        "ax1.set_ylabel('Loss (MSE)', fontweight='bold')\n",
        "ax1.set_title('Training History', fontweight='bold', fontsize=12)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Predictions\n",
        "ax2.scatter(X_data, y_data, c='blue', s=100, alpha=0.6, \n",
        "            edgecolors='black', linewidth=1.5, label='Training Data', zorder=2)\n",
        "ax2.plot(X_test_range, predictions_keras, 'green', linewidth=3, \n",
        "         label='Keras NN Prediction', zorder=3)\n",
        "ax2.axhline(y=0.5, color='red', linestyle='--', linewidth=1.5, alpha=0.5)\n",
        "ax2.set_xlabel('Dosage', fontweight='bold')\n",
        "ax2.set_ylabel('Effectiveness', fontweight='bold')\n",
        "ax2.set_title('Keras Neural Network Fit', fontweight='bold', fontsize=12)\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_ylim(-0.1, 1.1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83c\udf89 Successfully trained neural network using Keras!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Slide 7: Implementation from Scratch (NumPy)\n",
        "\n",
        "### Why Build from Scratch?\n",
        "- **Deep understanding**: Learn exactly how neural networks work\n",
        "- **Debugging skills**: Better at troubleshooting issues\n",
        "- **Customization**: Complete control over every aspect\n",
        "- **Interview prep**: Common technical interview topic\n",
        "\n",
        "### Key Components to Implement:\n",
        "1. **Forward Propagation**: Pass data through the network\n",
        "2. **Loss Calculation**: Measure prediction error\n",
        "3. **Backward Propagation**: Calculate gradients\n",
        "4. **Weight Update**: Adjust parameters to reduce error\n",
        "\n",
        "### Mathematical Foundation:\n",
        "- **Forward pass**: \\\\( \\\\hat{y} = f(W_2 \\\\cdot \\\\sigma(W_1 \\\\cdot x + b_1) + b_2) \\\\)\n",
        "- **Loss**: \\\\( L = \\\\frac{1}{n} \\\\sum (y - \\\\hat{y})^2 \\\\)\n",
        "- **Backpropagation**: Chain rule to compute \\\\( \\\\frac{\\\\partial L}{\\\\partial W} \\\\) and \\\\( \\\\frac{\\\\partial L}{\\\\partial b} \\\\)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Slide 7 Code: Neural Network from Scratch using NumPy\n",
        "\n",
        "class NeuralNetworkFromScratch:\n",
        "    \"\"\"\n",
        "    A simple neural network with:\n",
        "    - 1 input node\n",
        "    - 2 hidden nodes (softplus activation)\n",
        "    - 1 output node (linear activation)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        \"\"\"Initialize the neural network with random weights and biases.\"\"\"\n",
        "        self.lr = learning_rate\n",
        "        \n",
        "        # Initialize weights and biases with small random values\n",
        "        np.random.seed(42)\n",
        "        self.W1 = np.random.randn(1, 2) * 0.5  # Input to hidden (1x2)\n",
        "        self.b1 = np.random.randn(1, 2) * 0.5  # Hidden layer bias (1x2)\n",
        "        self.W2 = np.random.randn(2, 1) * 0.5  # Hidden to output (2x1)\n",
        "        self.b2 = np.random.randn(1, 1) * 0.5  # Output bias (1x1)\n",
        "        \n",
        "        # Store training history\n",
        "        self.loss_history = []\n",
        "    \n",
        "    def softplus(self, x):\n",
        "        \"\"\"Softplus activation function.\"\"\"\n",
        "        return np.log(1 + np.exp(np.clip(x, -500, 500)))\n",
        "    \n",
        "    def softplus_derivative(self, x):\n",
        "        \"\"\"Derivative of softplus (sigmoid function).\"\"\"\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : numpy array, shape (n_samples, 1)\n",
        "            Input data\n",
        "        \n",
        "        Returns:\n",
        "        --------\n",
        "        output : numpy array, shape (n_samples, 1)\n",
        "            Network predictions\n",
        "        \"\"\"\n",
        "        # Hidden layer\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1  # Linear combination\n",
        "        self.a1 = self.softplus(self.z1)        # Activation\n",
        "        \n",
        "        # Output layer\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2  # Linear combination\n",
        "        self.output = self.z2                          # No activation (linear)\n",
        "        \n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        \"\"\"\n",
        "        Backward propagation (compute gradients).\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : numpy array, shape (n_samples, 1)\n",
        "            Input data\n",
        "        y : numpy array, shape (n_samples, 1)\n",
        "            True labels\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        \n",
        "        # Output layer gradients\n",
        "        d_loss = 2 * (self.output - y) / n_samples  # Derivative of MSE loss\n",
        "        d_W2 = np.dot(self.a1.T, d_loss)             # Gradient for W2\n",
        "        d_b2 = np.sum(d_loss, axis=0, keepdims=True) # Gradient for b2\n",
        "        \n",
        "        # Hidden layer gradients\n",
        "        d_a1 = np.dot(d_loss, self.W2.T)                        # Backprop to hidden layer\n",
        "        d_z1 = d_a1 * self.softplus_derivative(self.z1)        # Apply activation derivative\n",
        "        d_W1 = np.dot(X.T, d_z1)                                 # Gradient for W1\n",
        "        d_b1 = np.sum(d_z1, axis=0, keepdims=True)              # Gradient for b1\n",
        "        \n",
        "        # Update weights and biases using gradient descent\n",
        "        self.W2 -= self.lr * d_W2\n",
        "        self.b2 -= self.lr * d_b2\n",
        "        self.W1 -= self.lr * d_W1\n",
        "        self.b1 -= self.lr * d_b1\n",
        "    \n",
        "    def train(self, X, y, epochs=1000, verbose=True):\n",
        "        \"\"\"\n",
        "        Train the neural network.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : numpy array, shape (n_samples, 1)\n",
        "            Training data\n",
        "        y : numpy array, shape (n_samples, 1)\n",
        "            True labels\n",
        "        epochs : int\n",
        "            Number of training iterations\n",
        "        verbose : bool\n",
        "            Print training progress\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            # Forward propagation\n",
        "            predictions = self.forward(X)\n",
        "            \n",
        "            # Calculate loss (Mean Squared Error)\n",
        "            loss = np.mean((predictions - y) ** 2)\n",
        "            self.loss_history.append(loss)\n",
        "            \n",
        "            # Backward propagation\n",
        "            self.backward(X, y)\n",
        "            \n",
        "            # Print progress\n",
        "            if verbose and (epoch % 100 == 0 or epoch == epochs - 1):\n",
        "                print(f\"Epoch {epoch:4d}/{epochs}: Loss = {loss:.6f}\")\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions on new data.\"\"\"\n",
        "        return self.forward(X)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\" \"*15 + \"TRAINING FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create and train the neural network\n",
        "nn_scratch = NeuralNetworkFromScratch(learning_rate=0.05)\n",
        "\n",
        "# Prepare data\n",
        "X_train_scratch = X_data.reshape(-1, 1)\n",
        "y_train_scratch = y_data.reshape(-1, 1)\n",
        "\n",
        "# Train the network\n",
        "nn_scratch.train(X_train_scratch, y_train_scratch, epochs=1000, verbose=True)\n",
        "\n",
        "print(\"\\n\u2713 Training complete!\")\n",
        "print(\"\\nFinal Parameters:\")\n",
        "print(f\"  W1 (input to hidden): \\n{nn_scratch.W1}\")\n",
        "print(f\"  b1 (hidden bias): \\n{nn_scratch.b1}\")\n",
        "print(f\"  W2 (hidden to output): \\n{nn_scratch.W2}\")\n",
        "print(f\"  b2 (output bias): \\n{nn_scratch.b2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the from-scratch implementation results\n",
        "\n",
        "# Generate predictions\n",
        "X_test_scratch = np.linspace(0, 1, 1000).reshape(-1, 1)\n",
        "predictions_scratch = nn_scratch.predict(X_test_scratch)\n",
        "\n",
        "# Create comprehensive visualization\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# Plot 1: Loss curve\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "ax1.plot(nn_scratch.loss_history, 'purple', linewidth=2)\n",
        "ax1.set_xlabel('Epoch', fontweight='bold')\n",
        "ax1.set_ylabel('Loss (MSE)', fontweight='bold')\n",
        "ax1.set_title('Training Loss Over Time', fontweight='bold', fontsize=12)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_yscale('log')\n",
        "\n",
        "# Plot 2: Predictions vs True Data\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "ax2.scatter(X_data, y_data, c='blue', s=100, alpha=0.6, \n",
        "            edgecolors='black', linewidth=1.5, label='True Data', zorder=2)\n",
        "ax2.plot(X_test_scratch, predictions_scratch, 'red', linewidth=3, \n",
        "         label='From-Scratch Prediction', zorder=3)\n",
        "ax2.axhline(y=0.5, color='gray', linestyle='--', linewidth=1.5, alpha=0.5)\n",
        "ax2.set_xlabel('Dosage', fontweight='bold')\n",
        "ax2.set_ylabel('Effectiveness', fontweight='bold')\n",
        "ax2.set_title('Neural Network Fit (From Scratch)', fontweight='bold', fontsize=12)\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_ylim(-0.1, 1.1)\n",
        "\n",
        "# Plot 3: Hidden Layer Activations\n",
        "ax3 = fig.add_subplot(gs[1, 0])\n",
        "# Forward pass to get activations\n",
        "_ = nn_scratch.forward(X_test_scratch)\n",
        "ax3.plot(X_test_scratch, nn_scratch.a1[:, 0], 'blue', linewidth=2, label='Hidden Node 1', alpha=0.7)\n",
        "ax3.plot(X_test_scratch, nn_scratch.a1[:, 1], 'orange', linewidth=2, label='Hidden Node 2', alpha=0.7)\n",
        "ax3.set_xlabel('Dosage', fontweight='bold')\n",
        "ax3.set_ylabel('Activation Value', fontweight='bold')\n",
        "ax3.set_title('Hidden Layer Activations', fontweight='bold', fontsize=12)\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Comparison with manual calculation\n",
        "ax4 = fig.add_subplot(gs[1, 1])\n",
        "# Use the original StatQuest parameters for comparison\n",
        "manual_predictions = np.array([predict_effectiveness(x[0]) for x in X_test_scratch])\n",
        "ax4.plot(X_test_scratch, manual_predictions, 'green', linewidth=3, \n",
        "         label='StatQuest Manual', alpha=0.7, linestyle='--')\n",
        "ax4.plot(X_test_scratch, predictions_scratch, 'red', linewidth=2, \n",
        "         label='From-Scratch Trained', alpha=0.7)\n",
        "ax4.scatter(X_data, y_data, c='blue', s=50, alpha=0.4, \n",
        "            edgecolors='black', linewidth=1, label='Data', zorder=1)\n",
        "ax4.set_xlabel('Dosage', fontweight='bold')\n",
        "ax4.set_ylabel('Effectiveness', fontweight='bold')\n",
        "ax4.set_title('Comparison: Manual vs Trained', fontweight='bold', fontsize=12)\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "ax4.set_ylim(-0.1, 1.2)\n",
        "\n",
        "plt.suptitle('From-Scratch Neural Network: Complete Analysis', \n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83c\udf89 Successfully implemented and trained neural network from scratch!\")\n",
        "print(\"\\nKey Achievement:\")\n",
        "print(\"  - Implemented forward propagation\")\n",
        "print(\"  - Implemented backward propagation (backprop)\")\n",
        "print(\"  - Trained using gradient descent\")\n",
        "print(\"  - Achieved good fit to the data!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Slide 8: Understanding Backpropagation\n",
        "\n",
        "### What is Backpropagation?\n",
        "**Backpropagation** is the algorithm used to train neural networks by computing gradients efficiently.\n",
        "\n",
        "### The Process:\n",
        "1. **Forward Pass**: Calculate predictions\n",
        "2. **Calculate Loss**: Measure error between predictions and true values\n",
        "3. **Backward Pass**: Calculate gradients using the chain rule\n",
        "4. **Update Weights**: Adjust parameters to reduce loss\n",
        "\n",
        "### Mathematical Foundation:\n",
        "For each weight \\\\(w\\\\), we want to know: \"How does changing \\\\(w\\\\) affect the loss?\"\n",
        "\n",
        "This is the partial derivative: \\\\( \\\\frac{\\\\partial L}{\\\\partial w} \\\\)\n",
        "\n",
        "**Chain Rule** allows us to compute this by breaking it into steps:\n",
        "\\\\[ \\\\frac{\\\\partial L}{\\\\partial w} = \\\\frac{\\\\partial L}{\\\\partial \\\\hat{y}} \\\\times \\\\frac{\\\\partial \\\\hat{y}}{\\\\partial a} \\\\times \\\\frac{\\\\partial a}{\\\\partial z} \\\\times \\\\frac{\\\\partial z}{\\\\partial w} \\\\]\n",
        "\n",
        "### Gradient Descent Update:\n",
        "\\\\[ w_{\\\\text{new}} = w_{\\\\text{old}} - \\\\alpha \\\\frac{\\\\partial L}{\\\\partial w} \\\\]\n",
        "\n",
        "where \\\\(\\\\alpha\\\\) is the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Slide 8 Code: Visualize backpropagation process\n",
        "\n",
        "def visualize_gradient_descent():\n",
        "    \"\"\"\n",
        "    Visualize how gradient descent updates weights to minimize loss.\n",
        "    \"\"\"\n",
        "    # Simple 1D example: minimize (w - 3)^2\n",
        "    def loss_function(w):\n",
        "        return (w - 3) ** 2\n",
        "    \n",
        "    def gradient(w):\n",
        "        return 2 * (w - 3)\n",
        "    \n",
        "    # Initialize weight and learning rate\n",
        "    w = 0.0\n",
        "    learning_rate = 0.1\n",
        "    \n",
        "    # Track optimization path\n",
        "    w_history = [w]\n",
        "    loss_history = [loss_function(w)]\n",
        "    \n",
        "    # Gradient descent iterations\n",
        "    for i in range(20):\n",
        "        grad = gradient(w)\n",
        "        w = w - learning_rate * grad\n",
        "        w_history.append(w)\n",
        "        loss_history.append(loss_function(w))\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Plot 1: Loss surface with gradient descent path\n",
        "    w_range = np.linspace(-1, 5, 300)\n",
        "    loss_range = loss_function(w_range)\n",
        "    \n",
        "    ax1.plot(w_range, loss_range, 'b-', linewidth=2, label='Loss Function')\n",
        "    ax1.plot(w_history, loss_history, 'ro-', markersize=8, linewidth=2, \n",
        "             label='Gradient Descent Path', alpha=0.7)\n",
        "    ax1.plot(w_history[0], loss_history[0], 'go', markersize=15, \n",
        "             label='Start', zorder=5)\n",
        "    ax1.plot(w_history[-1], loss_history[-1], 'r*', markersize=20, \n",
        "             label='End (Optimum)', zorder=5)\n",
        "    \n",
        "    # Add arrows to show direction\n",
        "    for i in range(0, len(w_history)-1, 2):\n",
        "        ax1.annotate('', xy=(w_history[i+1], loss_history[i+1]), \n",
        "                     xytext=(w_history[i], loss_history[i]),\n",
        "                     arrowprops=dict(arrowstyle='->', color='red', lw=1.5, alpha=0.6))\n",
        "    \n",
        "    ax1.set_xlabel('Weight (w)', fontweight='bold', fontsize=12)\n",
        "    ax1.set_ylabel('Loss', fontweight='bold', fontsize=12)\n",
        "    ax1.set_title('Gradient Descent Optimization Path', fontweight='bold', fontsize=13)\n",
        "    ax1.legend(loc='upper right')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Loss over iterations\n",
        "    ax2.plot(loss_history, 'purple', linewidth=3, marker='o', markersize=6)\n",
        "    ax2.set_xlabel('Iteration', fontweight='bold', fontsize=12)\n",
        "    ax2.set_ylabel('Loss', fontweight='bold', fontsize=12)\n",
        "    ax2.set_title('Loss Reduction Over Iterations', fontweight='bold', fontsize=13)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_yscale('log')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nGradient Descent Summary:\")\n",
        "    print(f\"  Starting weight: {w_history[0]:.4f}\")\n",
        "    print(f\"  Final weight: {w_history[-1]:.4f}\")\n",
        "    print(f\"  Starting loss: {loss_history[0]:.4f}\")\n",
        "    print(f\"  Final loss: {loss_history[-1]:.6f}\")\n",
        "    print(f\"  Iterations: {len(w_history) - 1}\")\n",
        "\n",
        "# Visualize gradient descent\n",
        "visualize_gradient_descent()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Key Insights:\")\n",
        "print(\"=\"*60)\n",
        "print(\"1. Gradients tell us the direction to move weights\")\n",
        "print(\"2. Learning rate controls the step size\")\n",
        "print(\"3. We iteratively move towards the minimum loss\")\n",
        "print(\"4. This process is called 'training' the neural network\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Slide 9: Comparing Implementations\n",
        "\n",
        "### Three Approaches:\n",
        "\n",
        "| Aspect | Manual (StatQuest) | Library (Keras) | From Scratch (NumPy) |\n",
        "|--------|-------------------|-----------------|----------------------|\n",
        "| **Difficulty** | Easy (given params) | Very Easy | Hard |\n",
        "| **Flexibility** | None | Medium | Complete |\n",
        "| **Speed** | Fast | Very Fast (GPU) | Slow |\n",
        "| **Learning Value** | High (concepts) | Low | Very High |\n",
        "| **Production Use** | No | Yes | No |\n",
        "| **Understanding** | Conceptual | Black box | Deep |\n",
        "\n",
        "### When to Use Each:\n",
        "- **Manual**: Learning concepts, understanding mechanics\n",
        "- **Library**: Production systems, rapid prototyping, research\n",
        "- **From Scratch**: Education, interviews, custom algorithms\n",
        "\n",
        "### Best Practice:\n",
        "1. Learn from scratch first (understand fundamentals)\n",
        "2. Use libraries for real applications (efficiency)\n",
        "3. Keep manual calculations for debugging (verification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Slide 9 Code: Compare all three implementations\n",
        "\n",
        "# Generate test data\n",
        "X_compare = np.linspace(0, 1, 500).reshape(-1, 1)\n",
        "\n",
        "# Get predictions from all three methods\n",
        "pred_manual = np.array([predict_effectiveness(x[0]) for x in X_compare])\n",
        "pred_keras = model.predict(X_compare, verbose=0).flatten()\n",
        "pred_scratch = nn_scratch.predict(X_compare).flatten()\n",
        "\n",
        "# Create comprehensive comparison visualization\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# Plot 1: All predictions together\n",
        "ax1 = fig.add_subplot(gs[0, :])\n",
        "ax1.scatter(X_data, y_data, c='lightblue', s=100, alpha=0.6, \n",
        "            edgecolors='black', linewidth=1.5, label='Training Data', zorder=1)\n",
        "ax1.plot(X_compare, pred_manual, 'green', linewidth=3, \n",
        "         label='Manual (StatQuest)', linestyle='--', alpha=0.8)\n",
        "ax1.plot(X_compare, pred_keras, 'red', linewidth=2.5, \n",
        "         label='Keras (Library)', alpha=0.7)\n",
        "ax1.plot(X_compare, pred_scratch, 'purple', linewidth=2, \n",
        "         label='NumPy (From Scratch)', alpha=0.7, linestyle=':')\n",
        "ax1.axhline(y=0.5, color='gray', linestyle='--', linewidth=1.5, alpha=0.5)\n",
        "ax1.set_xlabel('Dosage', fontweight='bold', fontsize=12)\n",
        "ax1.set_ylabel('Effectiveness', fontweight='bold', fontsize=12)\n",
        "ax1.set_title('Comparison of All Three Implementations', fontweight='bold', fontsize=14)\n",
        "ax1.legend(loc='upper right', fontsize=11)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_ylim(-0.1, 1.2)\n",
        "\n",
        "# Plot 2: Prediction differences from manual\n",
        "ax2 = fig.add_subplot(gs[1, 0])\n",
        "diff_keras = np.abs(pred_keras - pred_manual)\n",
        "diff_scratch = np.abs(pred_scratch - pred_manual)\n",
        "ax2.plot(X_compare, diff_keras, 'red', linewidth=2, label='Keras vs Manual')\n",
        "ax2.plot(X_compare, diff_scratch, 'purple', linewidth=2, label='Scratch vs Manual')\n",
        "ax2.set_xlabel('Dosage', fontweight='bold')\n",
        "ax2.set_ylabel('Absolute Difference', fontweight='bold')\n",
        "ax2.set_title('Prediction Differences from Manual Method', fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Performance metrics\n",
        "ax3 = fig.add_subplot(gs[1, 1])\n",
        "\n",
        "# Calculate MSE for each method on training data\n",
        "X_train_flat = X_data.reshape(-1, 1)\n",
        "pred_manual_train = np.array([predict_effectiveness(x[0]) for x in X_train_flat])\n",
        "pred_keras_train = model.predict(X_train_flat, verbose=0).flatten()\n",
        "pred_scratch_train = nn_scratch.predict(X_train_flat).flatten()\n",
        "\n",
        "mse_manual = np.mean((pred_manual_train - y_data) ** 2)\n",
        "mse_keras = np.mean((pred_keras_train - y_data) ** 2)\n",
        "mse_scratch = np.mean((pred_scratch_train - y_data) ** 2)\n",
        "\n",
        "methods = ['Manual\\n(StatQuest)', 'Keras\\n(Library)', 'NumPy\\n(Scratch)']\n",
        "mse_values = [mse_manual, mse_keras, mse_scratch]\n",
        "colors = ['green', 'red', 'purple']\n",
        "\n",
        "bars = ax3.bar(methods, mse_values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "ax3.set_ylabel('Mean Squared Error', fontweight='bold', fontsize=11)\n",
        "ax3.set_title('Training Performance Comparison', fontweight='bold', fontsize=12)\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, value in zip(bars, mse_values):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{value:.4f}',\n",
        "             ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.suptitle('Neural Network Implementation Comparison', \n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.show()\n",
        "\n",
        "# Print detailed comparison\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" \"*20 + \"IMPLEMENTATION COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Method':<20} {'MSE':<15} {'Max Error':<15} {'Mean Error':<15}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "max_err_manual = np.max(np.abs(pred_manual_train - y_data))\n",
        "max_err_keras = np.max(np.abs(pred_keras_train - y_data))\n",
        "max_err_scratch = np.max(np.abs(pred_scratch_train - y_data))\n",
        "\n",
        "mean_err_manual = np.mean(np.abs(pred_manual_train - y_data))\n",
        "mean_err_keras = np.mean(np.abs(pred_keras_train - y_data))\n",
        "mean_err_scratch = np.mean(np.abs(pred_scratch_train - y_data))\n",
        "\n",
        "print(f\"{'Manual (StatQuest)':<20} {mse_manual:<15.6f} {max_err_manual:<15.6f} {mean_err_manual:<15.6f}\")\n",
        "print(f\"{'Keras (Library)':<20} {mse_keras:<15.6f} {max_err_keras:<15.6f} {mean_err_keras:<15.6f}\")\n",
        "print(f\"{'NumPy (Scratch)':<20} {mse_scratch:<15.6f} {max_err_scratch:<15.6f} {mean_err_scratch:<15.6f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n\u2713 All three methods successfully model the drug effectiveness!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Slide 10: Key Takeaways and Next Steps\n",
        "\n",
        "### What We Learned:\n",
        "\n",
        "#### 1. **Neural Networks are Squiggle Fitting Machines**\n",
        "   - They transform simple curves into complex patterns\n",
        "   - Multiple nodes work together to fit non-linear data\n",
        "\n",
        "#### 2. **Key Components**\n",
        "   - **Nodes**: Process information\n",
        "   - **Weights**: Control strength of connections\n",
        "   - **Biases**: Shift outputs\n",
        "   - **Activation Functions**: Introduce non-linearity\n",
        "\n",
        "#### 3. **Training Process**\n",
        "   - Forward propagation: Make predictions\n",
        "   - Calculate loss: Measure error\n",
        "   - Backpropagation: Calculate gradients\n",
        "   - Update weights: Improve predictions\n",
        "\n",
        "#### 4. **Implementation Approaches**\n",
        "   - Libraries (Keras): Fast, production-ready\n",
        "   - From scratch (NumPy): Deep understanding\n",
        "   - Manual calculation: Conceptual learning\n",
        "\n",
        "### Next Steps:\n",
        "1. **Deep Learning**: Networks with many hidden layers\n",
        "2. **Convolutional Neural Networks (CNNs)**: For images\n",
        "3. **Recurrent Neural Networks (RNNs)**: For sequences\n",
        "4. **Advanced optimization**: Adam, RMSprop, etc.\n",
        "5. **Regularization**: Preventing overfitting\n",
        "\n",
        "### Resources:\n",
        "- StatQuest YouTube Channel\n",
        "- Deep Learning Specialization (Coursera)\n",
        "- Neural Networks and Deep Learning (Nielsen)\n",
        "- TensorFlow and PyTorch documentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Slide 10 Code: Final summary visualization\n",
        "\n",
        "# Create a comprehensive final summary\n",
        "fig = plt.figure(figsize=(16, 12))\n",
        "gs = fig.add_gridspec(3, 2, hspace=0.4, wspace=0.3)\n",
        "\n",
        "# Plot 1: Neural Network Architecture Diagram\n",
        "ax1 = fig.add_subplot(gs[0, :])\n",
        "ax1.text(0.5, 0.9, 'Neural Network Architecture', \n",
        "         ha='center', va='top', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Draw network structure\n",
        "# Input node\n",
        "circle1 = plt.Circle((0.2, 0.5), 0.08, color='lightblue', ec='black', linewidth=2, zorder=3)\n",
        "ax1.add_patch(circle1)\n",
        "ax1.text(0.2, 0.5, 'Input\\n(Dosage)', ha='center', va='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "# Hidden layer nodes\n",
        "circle2 = plt.Circle((0.5, 0.65), 0.08, color='lightgreen', ec='black', linewidth=2, zorder=3)\n",
        "circle3 = plt.Circle((0.5, 0.35), 0.08, color='lightgreen', ec='black', linewidth=2, zorder=3)\n",
        "ax1.add_patch(circle2)\n",
        "ax1.add_patch(circle3)\n",
        "ax1.text(0.5, 0.65, 'Hidden\\nNode 1', ha='center', va='center', fontsize=8, fontweight='bold')\n",
        "ax1.text(0.5, 0.35, 'Hidden\\nNode 2', ha='center', va='center', fontsize=8, fontweight='bold')\n",
        "\n",
        "# Output node\n",
        "circle4 = plt.Circle((0.8, 0.5), 0.08, color='lightcoral', ec='black', linewidth=2, zorder=3)\n",
        "ax1.add_patch(circle4)\n",
        "ax1.text(0.8, 0.5, 'Output\\n(Effect.)', ha='center', va='center', fontsize=8, fontweight='bold')\n",
        "\n",
        "# Draw connections\n",
        "connections = [\n",
        "    ((0.2, 0.5), (0.5, 0.65)),\n",
        "    ((0.2, 0.5), (0.5, 0.35)),\n",
        "    ((0.5, 0.65), (0.8, 0.5)),\n",
        "    ((0.5, 0.35), (0.8, 0.5))\n",
        "]\n",
        "for start, end in connections:\n",
        "    ax1.plot([start[0], end[0]], [start[1], end[1]], 'k-', linewidth=2, alpha=0.5, zorder=1)\n",
        "\n",
        "# Add labels\n",
        "ax1.text(0.35, 0.73, 'W, b', ha='center', fontsize=9, style='italic', color='red')\n",
        "ax1.text(0.35, 0.43, 'W, b', ha='center', fontsize=9, style='italic', color='red')\n",
        "ax1.text(0.65, 0.6, 'W', ha='center', fontsize=9, style='italic', color='red')\n",
        "ax1.text(0.65, 0.45, 'W', ha='center', fontsize=9, style='italic', color='red')\n",
        "\n",
        "ax1.set_xlim(0, 1)\n",
        "ax1.set_ylim(0.2, 1)\n",
        "ax1.axis('off')\n",
        "\n",
        "# Plot 2: Activation functions\n",
        "ax2 = fig.add_subplot(gs[1, 0])\n",
        "x_act = np.linspace(-3, 3, 200)\n",
        "ax2.plot(x_act, softplus(x_act), 'b-', linewidth=2, label='Softplus')\n",
        "ax2.plot(x_act, relu(x_act), 'r-', linewidth=2, label='ReLU')\n",
        "ax2.plot(x_act, sigmoid(x_act), 'g-', linewidth=2, label='Sigmoid')\n",
        "ax2.set_xlabel('Input', fontweight='bold')\n",
        "ax2.set_ylabel('Output', fontweight='bold')\n",
        "ax2.set_title('Activation Functions', fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
        "ax2.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
        "\n",
        "# Plot 3: Final fit\n",
        "ax3 = fig.add_subplot(gs[1, 1])\n",
        "ax3.scatter(X_data, y_data, c='blue', s=100, alpha=0.6, \n",
        "            edgecolors='black', linewidth=1.5, label='Data', zorder=2)\n",
        "ax3.plot(X_compare, pred_keras, 'green', linewidth=3, \n",
        "         label='NN Prediction', zorder=3)\n",
        "ax3.axhline(y=0.5, color='red', linestyle='--', linewidth=1.5, alpha=0.5)\n",
        "ax3.fill_between(X_compare.flatten(), 0.5, 1.2, alpha=0.1, color='green')\n",
        "ax3.fill_between(X_compare.flatten(), -0.1, 0.5, alpha=0.1, color='red')\n",
        "ax3.set_xlabel('Dosage', fontweight='bold')\n",
        "ax3.set_ylabel('Effectiveness', fontweight='bold')\n",
        "ax3.set_title('Final Result: Perfect Squiggle Fit!', fontweight='bold')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "ax3.set_ylim(-0.1, 1.2)\n",
        "\n",
        "# Plot 4: Training process\n",
        "ax4 = fig.add_subplot(gs[2, 0])\n",
        "ax4.text(0.5, 0.9, 'Training Process (Backpropagation)', \n",
        "         ha='center', va='top', fontsize=12, fontweight='bold', transform=ax4.transAxes)\n",
        "steps = ['1. Forward\\nPass', '2. Calculate\\nLoss', '3. Backward\\nPass', '4. Update\\nWeights']\n",
        "x_pos = [0.15, 0.38, 0.62, 0.85]\n",
        "for i, (step, x) in enumerate(zip(steps, x_pos)):\n",
        "    rect = FancyBboxPatch((x-0.08, 0.4), 0.16, 0.3, \n",
        "                          boxstyle=\"round,pad=0.01\", \n",
        "                          edgecolor='black', facecolor=plt.cm.viridis(i/4), \n",
        "                          linewidth=2, transform=ax4.transAxes, zorder=2)\n",
        "    ax4.add_patch(rect)\n",
        "    ax4.text(x, 0.55, step, ha='center', va='center', fontsize=9, \n",
        "             fontweight='bold', transform=ax4.transAxes, zorder=3)\n",
        "    if i < len(steps) - 1:\n",
        "        ax4.annotate('', xy=(x_pos[i+1]-0.08, 0.55), xytext=(x+0.08, 0.55),\n",
        "                     arrowprops=dict(arrowstyle='->', lw=2, color='black'),\n",
        "                     transform=ax4.transAxes)\n",
        "ax4.annotate('', xy=(0.15-0.08, 0.45), xytext=(0.85+0.08, 0.65),\n",
        "             arrowprops=dict(arrowstyle='->', lw=2, color='red', linestyle='--'),\n",
        "             transform=ax4.transAxes)\n",
        "ax4.text(0.5, 0.2, 'Repeat until convergence', ha='center', va='center', \n",
        "         fontsize=10, style='italic', color='red', transform=ax4.transAxes)\n",
        "ax4.set_xlim(0, 1)\n",
        "ax4.set_ylim(0, 1)\n",
        "ax4.axis('off')\n",
        "\n",
        "# Plot 5: Key concepts\n",
        "ax5 = fig.add_subplot(gs[2, 1])\n",
        "ax5.text(0.5, 0.95, 'Key Concepts', ha='center', va='top', \n",
        "         fontsize=12, fontweight='bold', transform=ax5.transAxes)\n",
        "\n",
        "concepts = [\n",
        "    '\u2713 Neural Networks = Squiggle Fitters',\n",
        "    '\u2713 Activation Functions = Building Blocks',\n",
        "    '\u2713 Weights & Biases = Learned Parameters',\n",
        "    '\u2713 Backpropagation = Training Algorithm',\n",
        "    '\u2713 Non-linear Problems = Perfect Use Case'\n",
        "]\n",
        "\n",
        "for i, concept in enumerate(concepts):\n",
        "    y_pos = 0.75 - i * 0.12\n",
        "    ax5.text(0.1, y_pos, concept, ha='left', va='center', \n",
        "             fontsize=10, transform=ax5.transAxes,\n",
        "             bbox=dict(boxstyle='round', facecolor='lightyellow', \n",
        "                      edgecolor='black', linewidth=1.5))\n",
        "\n",
        "ax5.set_xlim(0, 1)\n",
        "ax5.set_ylim(0, 1)\n",
        "ax5.axis('off')\n",
        "\n",
        "plt.suptitle('Neural Networks: Complete Summary', \n",
        "             fontsize=18, fontweight='bold', y=0.98)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" \"*15 + \"\ud83c\udf89 CONGRATULATIONS! \ud83c\udf89\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nYou have successfully completed the Neural Networks tutorial!\")\n",
        "print(\"\\nYou now understand:\")\n",
        "print(\"  \u2713 What neural networks are and how they work\")\n",
        "print(\"  \u2713 How activation functions create non-linear patterns\")\n",
        "print(\"  \u2713 How to implement NNs with libraries (Keras)\")\n",
        "print(\"  \u2713 How to implement NNs from scratch (NumPy)\")\n",
        "print(\"  \u2713 How backpropagation trains neural networks\")\n",
        "print(\"\\nNext steps: Explore deep learning, CNNs, and RNNs!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n\ud83d\udca1 Remember: Neural Networks are just Big Fancy Squiggle Fitting Machines!\")\n",
        "print(\"\\n\ud83d\udcda Keep learning and Quest On!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus: Interactive Exploration\n",
        "\n",
        "### Experiment with Parameters!\n",
        "\n",
        "Try modifying these parameters to see how they affect the neural network:\n",
        "\n",
        "1. **Number of hidden nodes**: Change from 2 to 3, 4, or more\n",
        "2. **Activation functions**: Try ReLU or sigmoid instead of softplus\n",
        "3. **Learning rate**: Increase or decrease to see effect on training\n",
        "4. **Network depth**: Add more hidden layers\n",
        "5. **Dataset**: Create different patterns of data\n",
        "\n",
        "Use the code cells below to experiment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bonus Code: Experiment with different architectures\n",
        "\n",
        "# Try different numbers of hidden nodes\n",
        "def experiment_with_architecture(n_hidden_nodes=2, activation='softplus', learning_rate=0.01, epochs=500):\n",
        "    \"\"\"\n",
        "    Experiment with different neural network architectures.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    n_hidden_nodes : int\n",
        "        Number of nodes in hidden layer\n",
        "    activation : str\n",
        "        Activation function ('softplus', 'relu', or 'sigmoid')\n",
        "    learning_rate : float\n",
        "        Learning rate for training\n",
        "    epochs : int\n",
        "        Number of training epochs\n",
        "    \"\"\"\n",
        "    print(f\"\\nExperiment: {n_hidden_nodes} hidden nodes, {activation} activation, lr={learning_rate}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Create model\n",
        "    model_exp = Sequential([\n",
        "        Dense(n_hidden_nodes, activation=activation, input_shape=(1,)),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    \n",
        "    model_exp.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='mean_squared_error'\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    history_exp = model_exp.fit(\n",
        "        X_data.reshape(-1, 1), \n",
        "        y_data.reshape(-1, 1),\n",
        "        epochs=epochs,\n",
        "        batch_size=5,\n",
        "        verbose=0\n",
        "    )\n",
        "    \n",
        "    # Evaluate\n",
        "    final_loss = history_exp.history['loss'][-1]\n",
        "    print(f\"Final Loss: {final_loss:.6f}\")\n",
        "    \n",
        "    # Visualize\n",
        "    X_test = np.linspace(0, 1, 1000).reshape(-1, 1)\n",
        "    predictions = model_exp.predict(X_test, verbose=0)\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Plot predictions\n",
        "    ax1.scatter(X_data, y_data, c='blue', s=100, alpha=0.6, \n",
        "                edgecolors='black', linewidth=1.5, label='Data')\n",
        "    ax1.plot(X_test, predictions, 'red', linewidth=3, label='Prediction')\n",
        "    ax1.axhline(y=0.5, color='gray', linestyle='--', linewidth=1.5, alpha=0.5)\n",
        "    ax1.set_xlabel('Dosage', fontweight='bold')\n",
        "    ax1.set_ylabel('Effectiveness', fontweight='bold')\n",
        "    ax1.set_title(f'Fit: {n_hidden_nodes} nodes, {activation}', fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_ylim(-0.1, 1.1)\n",
        "    \n",
        "    # Plot training history\n",
        "    ax2.plot(history_exp.history['loss'], 'purple', linewidth=2)\n",
        "    ax2.set_xlabel('Epoch', fontweight='bold')\n",
        "    ax2.set_ylabel('Loss', fontweight='bold')\n",
        "    ax2.set_title('Training History', fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_yscale('log')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return model_exp, final_loss\n",
        "\n",
        "# Example experiments - uncomment to run!\n",
        "print(\"Run these experiments to see how architecture affects performance:\\n\")\n",
        "\n",
        "# Experiment 1: Different hidden layer sizes\n",
        "model_2, loss_2 = experiment_with_architecture(n_hidden_nodes=2, activation='softplus')\n",
        "\n",
        "# Uncomment to try more:\n",
        "# model_4, loss_4 = experiment_with_architecture(n_hidden_nodes=4, activation='softplus')\n",
        "# model_relu, loss_relu = experiment_with_architecture(n_hidden_nodes=2, activation='relu')\n",
        "# model_sigmoid, loss_sigmoid = experiment_with_architecture(n_hidden_nodes=2, activation='sigmoid')\n",
        "\n",
        "print(\"\\n\u2713 Experiment complete! Try changing the parameters above to explore more.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and References\n",
        "\n",
        "### What We Covered:\n",
        "This tutorial provided a complete introduction to neural networks, covering:\n",
        "- Fundamental concepts and terminology\n",
        "- Real-world application (drug dosage effectiveness)\n",
        "- Activation functions and their role\n",
        "- How neural networks create complex patterns\n",
        "- Implementation using TensorFlow/Keras\n",
        "- Implementation from scratch using NumPy\n",
        "- Backpropagation and training process\n",
        "- Practical comparisons and experimentation\n",
        "\n",
        "### Additional Resources:\n",
        "- **StatQuest YouTube**: Original neural networks series\n",
        "- **Deep Learning Book** (Goodfellow, Bengio, Courville)\n",
        "- **Neural Networks and Deep Learning** (Michael Nielsen)\n",
        "- **TensorFlow Documentation**: https://www.tensorflow.org/\n",
        "- **PyTorch Documentation**: https://pytorch.org/\n",
        "\n",
        "### Practice Exercises:\n",
        "1. Modify the dataset to have different patterns\n",
        "2. Implement a neural network with multiple hidden layers\n",
        "3. Try different activation functions and compare results\n",
        "4. Apply neural networks to a real-world dataset (e.g., Iris, MNIST)\n",
        "5. Implement additional features like dropout or batch normalization\n",
        "\n",
        "---\n",
        "\n",
        "**Remember**: Neural networks are powerful tools, but they're not magic.  \n",
        "Understanding how they work makes you a better machine learning practitioner!\n",
        "\n",
        "**Quest On!** \ud83d\ude80"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}