{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs): A Complete Tutorial\n",
    "\n",
    "## From Theory to Practice - Understanding Image Classification\n",
    "\n",
    "**Author:** Based on StatQuest with Josh Starmer  \n",
    "**Date:** November 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Why CNNs? Problems with Regular Neural Networks](#why-cnns)\n",
    "3. [Core Concepts of CNNs](#core-concepts)\n",
    "4. [Step-by-Step: How CNNs Work](#how-cnns-work)\n",
    "5. [Building CNNs from Scratch](#from-scratch)\n",
    "6. [Real-World Example: MNIST Digit Classification](#mnist-example)\n",
    "7. [Building CNNs with Keras/TensorFlow](#with-keras)\n",
    "8. [Visualizing CNN Features](#visualization)\n",
    "9. [Summary and Best Practices](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a id='introduction'></a>\n",
    "\n",
    "Imagine you're playing tic-tac-toe and your computer needs to recognize whether you drew an **X** or an **O**. How does it do this?\n",
    "\n",
    "The answer: **Convolutional Neural Networks (CNNs)**!\n",
    "\n",
    "CNNs are specialized neural networks designed for processing grid-like data, especially images. They've revolutionized:\n",
    "- Image classification\n",
    "- Object detection\n",
    "- Face recognition\n",
    "- Medical image analysis\n",
    "- Self-driving cars\n",
    "- And much more!\n",
    "\n",
    "Let's dive in and understand how they work! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.colors import ListedColormap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why CNNs? Problems with Regular Neural Networks <a id='why-cnns'></a>\n",
    "\n",
    "Let's start by understanding why we need CNNs. Regular (fully connected) neural networks have three major problems when dealing with images:\n",
    "\n",
    "### Problem 1: Too Many Parameters\n",
    "- A small 6√ó6 image = 36 input nodes\n",
    "- Each hidden layer node needs 36 weights\n",
    "- A 100√ó100 image = 10,000 weights per node!\n",
    "- **Doesn't scale well** ‚ùå\n",
    "\n",
    "### Problem 2: Not Shift-Invariant\n",
    "- If you shift an image by 1 pixel, the network might fail\n",
    "- **Poor generalization** ‚ùå\n",
    "\n",
    "### Problem 3: Ignores Spatial Relationships\n",
    "- Pixels near each other are usually correlated\n",
    "- Regular networks treat each pixel independently\n",
    "- **Misses important patterns** ‚ùå\n",
    "\n",
    "### CNNs Solve These Problems! ‚úÖ\n",
    "1. **Reduce parameters** through weight sharing\n",
    "2. **Tolerate small shifts** through convolution\n",
    "3. **Exploit correlations** by looking at pixel neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple example: Letter O and Letter X\n",
    "def create_letter_o():\n",
    "    \"\"\"Create a 6x6 image of letter O\"\"\"\n",
    "    letter_o = np.array([\n",
    "        [0, 1, 1, 1, 1, 0],\n",
    "        [1, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 1],\n",
    "        [0, 1, 1, 1, 1, 0]\n",
    "    ])\n",
    "    return letter_o\n",
    "\n",
    "def create_letter_x():\n",
    "    \"\"\"Create a 6x6 image of letter X\"\"\"\n",
    "    letter_x = np.array([\n",
    "        [1, 0, 0, 0, 0, 1],\n",
    "        [0, 1, 0, 0, 1, 0],\n",
    "        [0, 0, 1, 1, 0, 0],\n",
    "        [0, 0, 1, 1, 0, 0],\n",
    "        [0, 1, 0, 0, 1, 0],\n",
    "        [1, 0, 0, 0, 0, 1]\n",
    "    ])\n",
    "    return letter_x\n",
    "\n",
    "# Create and visualize\n",
    "letter_o = create_letter_o()\n",
    "letter_x = create_letter_x()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(letter_o, cmap='gray_r', interpolation='nearest')\n",
    "axes[0].set_title('Letter O (6√ó6 pixels)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, which='both', color='blue', linewidth=0.5)\n",
    "axes[0].set_xticks(np.arange(-0.5, 6, 1))\n",
    "axes[0].set_yticks(np.arange(-0.5, 6, 1))\n",
    "axes[0].tick_params(labelbottom=False, labelleft=False)\n",
    "\n",
    "axes[1].imshow(letter_x, cmap='gray_r', interpolation='nearest')\n",
    "axes[1].set_title('Letter X (6√ó6 pixels)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, which='both', color='blue', linewidth=0.5)\n",
    "axes[1].set_xticks(np.arange(-0.5, 6, 1))\n",
    "axes[1].set_yticks(np.arange(-0.5, 6, 1))\n",
    "axes[1].tick_params(labelbottom=False, labelleft=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLetter O array:\")\n",
    "print(letter_o)\n",
    "print(\"\\nLetter X array:\")\n",
    "print(letter_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Concepts of CNNs <a id='core-concepts'></a>\n",
    "\n",
    "CNNs have three main building blocks:\n",
    "\n",
    "### 3.1 Convolution (Filtering)\n",
    "- Apply a small **filter** (usually 3√ó3) to the image\n",
    "- The filter slides across the image\n",
    "- At each position, compute the **dot product**\n",
    "- Creates a **feature map**\n",
    "\n",
    "### 3.2 Activation Function (ReLU)\n",
    "- Apply ReLU: `max(0, x)`\n",
    "- Converts negative values to 0\n",
    "- Keeps positive values unchanged\n",
    "- Introduces non-linearity\n",
    "\n",
    "### 3.3 Pooling\n",
    "- **Max Pooling**: Select maximum value in each region\n",
    "- **Average Pooling**: Calculate average value\n",
    "- Reduces dimensionality\n",
    "- Makes network more robust to small shifts\n",
    "\n",
    "Let's visualize each step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step-by-Step: How CNNs Work <a id='how-cnns-work'></a>\n",
    "\n",
    "Let's walk through the complete process of classifying the letter O!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Convolution - Applying a Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve2d(image, kernel, bias=0, stride=1):\n",
    "    \"\"\"\n",
    "    Perform 2D convolution on an image with a kernel.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : numpy array\n",
    "        Input image\n",
    "    kernel : numpy array\n",
    "        Convolution filter/kernel\n",
    "    bias : float\n",
    "        Bias term to add\n",
    "    stride : int\n",
    "        Step size for sliding the kernel\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    feature_map : numpy array\n",
    "        Output feature map\n",
    "    \"\"\"\n",
    "    # Get dimensions\n",
    "    image_height, image_width = image.shape\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "    \n",
    "    # Calculate output dimensions\n",
    "    output_height = (image_height - kernel_height) // stride + 1\n",
    "    output_width = (image_width - kernel_width) // stride + 1\n",
    "    \n",
    "    # Initialize feature map\n",
    "    feature_map = np.zeros((output_height, output_width))\n",
    "    \n",
    "    # Perform convolution\n",
    "    for i in range(output_height):\n",
    "        for j in range(output_width):\n",
    "            # Extract the region\n",
    "            region = image[i*stride:i*stride+kernel_height, \n",
    "                          j*stride:j*stride+kernel_width]\n",
    "            \n",
    "            # Compute dot product (element-wise multiplication and sum)\n",
    "            feature_map[i, j] = np.sum(region * kernel) + bias\n",
    "    \n",
    "    return feature_map\n",
    "\n",
    "# Create a simple edge detection filter\n",
    "filter_kernel = np.array([\n",
    "    [1, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 1]\n",
    "])\n",
    "\n",
    "bias = -2\n",
    "\n",
    "# Apply convolution to letter O\n",
    "feature_map_o = convolve2d(letter_o, filter_kernel, bias=bias)\n",
    "\n",
    "print(\"Filter (3√ó3):\")\n",
    "print(filter_kernel)\n",
    "print(f\"\\nBias: {bias}\")\n",
    "print(f\"\\nFeature Map shape: {feature_map_o.shape}\")\n",
    "print(\"\\nFeature Map:\")\n",
    "print(feature_map_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the convolution process\n",
    "def visualize_convolution_step(image, kernel, position=(0, 0), bias=0):\n",
    "    \"\"\"\n",
    "    Visualize a single step of convolution\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(image, cmap='gray_r', interpolation='nearest')\n",
    "    axes[0].set_title('Input Image', fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(True, which='both', color='blue', linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    # Highlight the region\n",
    "    i, j = position\n",
    "    rect = Rectangle((j-0.5, i-0.5), 3, 3, linewidth=3, \n",
    "                     edgecolor='red', facecolor='none')\n",
    "    axes[0].add_patch(rect)\n",
    "    \n",
    "    # Show filter\n",
    "    axes[1].imshow(kernel, cmap='RdBu_r', interpolation='nearest', vmin=-1, vmax=1)\n",
    "    axes[1].set_title('Filter (Kernel)', fontsize=12, fontweight='bold')\n",
    "    for (x, y), value in np.ndenumerate(kernel):\n",
    "        axes[1].text(y, x, f'{value}', ha='center', va='center', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, which='both', color='gray', linewidth=0.5)\n",
    "    \n",
    "    # Show region being processed\n",
    "    region = image[i:i+3, j:j+3]\n",
    "    axes[2].imshow(region, cmap='gray_r', interpolation='nearest')\n",
    "    axes[2].set_title('Image Region', fontsize=12, fontweight='bold')\n",
    "    for (x, y), value in np.ndenumerate(region):\n",
    "        axes[2].text(y, x, f'{int(value)}', ha='center', va='center', \n",
    "                    fontsize=14, fontweight='bold', \n",
    "                    color='white' if value > 0.5 else 'black')\n",
    "    axes[2].grid(True, which='both', color='blue', linewidth=0.5)\n",
    "    \n",
    "    # Calculate and show result\n",
    "    dot_product = np.sum(region * kernel)\n",
    "    result = dot_product + bias\n",
    "    \n",
    "    axes[3].text(0.5, 0.6, 'Calculation:', ha='center', va='center', \n",
    "                fontsize=14, fontweight='bold', transform=axes[3].transAxes)\n",
    "    axes[3].text(0.5, 0.45, f'Dot Product = {dot_product:.0f}', ha='center', va='center',\n",
    "                fontsize=12, transform=axes[3].transAxes)\n",
    "    axes[3].text(0.5, 0.35, f'+ Bias ({bias}) = {result:.0f}', ha='center', va='center',\n",
    "                fontsize=12, transform=axes[3].transAxes)\n",
    "    axes[3].text(0.5, 0.2, f'Result: {result:.0f}', ha='center', va='center',\n",
    "                fontsize=16, fontweight='bold', \n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8),\n",
    "                transform=axes[3].transAxes)\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return result\n",
    "\n",
    "# Visualize convolution at position (0, 0)\n",
    "print(\"Convolution Step at Position (0, 0):\")\n",
    "visualize_convolution_step(letter_o, filter_kernel, position=(0, 0), bias=bias)\n",
    "plt.show()\n",
    "\n",
    "# Visualize convolution at position (1, 1)\n",
    "print(\"\\nConvolution Step at Position (1, 1):\")\n",
    "visualize_convolution_step(letter_o, filter_kernel, position=(1, 1), bias=bias)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the complete feature map\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Input\n",
    "axes[0].imshow(letter_o, cmap='gray_r', interpolation='nearest')\n",
    "axes[0].set_title('Input: Letter O\\n(6√ó6)', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, which='both', color='blue', linewidth=0.5)\n",
    "\n",
    "# Filter\n",
    "axes[1].imshow(filter_kernel, cmap='RdBu_r', interpolation='nearest')\n",
    "axes[1].set_title('Filter\\n(3√ó3)', fontsize=12, fontweight='bold')\n",
    "for (i, j), val in np.ndenumerate(filter_kernel):\n",
    "    axes[1].text(j, i, f'{val}', ha='center', va='center', \n",
    "                fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, which='both', color='gray', linewidth=0.5)\n",
    "\n",
    "# Feature Map\n",
    "im = axes[2].imshow(feature_map_o, cmap='RdYlGn', interpolation='nearest')\n",
    "axes[2].set_title('Feature Map\\n(4√ó4)', fontsize=12, fontweight='bold')\n",
    "for (i, j), val in np.ndenumerate(feature_map_o):\n",
    "    axes[2].text(j, i, f'{val:.0f}', ha='center', va='center', \n",
    "                fontsize=10, fontweight='bold')\n",
    "axes[2].grid(True, which='both', color='gray', linewidth=0.5)\n",
    "plt.colorbar(im, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Convolution complete! Feature map created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Activation Function (ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU (Rectified Linear Unit) activation function\n",
    "    Returns max(0, x) element-wise\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Apply ReLU to feature map\n",
    "activated_map = relu(feature_map_o)\n",
    "\n",
    "# Visualize ReLU activation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Before ReLU\n",
    "im1 = axes[0].imshow(feature_map_o, cmap='RdYlGn', interpolation='nearest')\n",
    "axes[0].set_title('Before ReLU\\n(Feature Map)', fontsize=12, fontweight='bold')\n",
    "for (i, j), val in np.ndenumerate(feature_map_o):\n",
    "    color = 'white' if val < 0 else 'black'\n",
    "    axes[0].text(j, i, f'{val:.0f}', ha='center', va='center', \n",
    "                fontsize=10, fontweight='bold', color=color)\n",
    "axes[0].grid(True, which='both', color='gray', linewidth=0.5)\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# ReLU function visualization\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = relu(x)\n",
    "axes[1].plot(x, y, linewidth=3, color='red')\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "axes[1].axvline(x=0, color='black', linestyle='--', alpha=0.3)\n",
    "axes[1].set_xlabel('Input', fontsize=11)\n",
    "axes[1].set_ylabel('Output', fontsize=11)\n",
    "axes[1].set_title('ReLU Function\\nmax(0, x)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].text(2, 3, 'Positive values\\nstay the same', fontsize=10, \n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "axes[1].text(-3, 0.5, 'Negative values\\nbecome 0', fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))\n",
    "\n",
    "# After ReLU\n",
    "im2 = axes[2].imshow(activated_map, cmap='YlGn', interpolation='nearest')\n",
    "axes[2].set_title('After ReLU\\n(Activated Map)', fontsize=12, fontweight='bold')\n",
    "for (i, j), val in np.ndenumerate(activated_map):\n",
    "    axes[2].text(j, i, f'{val:.0f}', ha='center', va='center', \n",
    "                fontsize=10, fontweight='bold')\n",
    "axes[2].grid(True, which='both', color='gray', linewidth=0.5)\n",
    "plt.colorbar(im2, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Map (before ReLU):\")\n",
    "print(feature_map_o)\n",
    "print(\"\\nActivated Map (after ReLU):\")\n",
    "print(activated_map)\n",
    "print(\"\\n‚úÖ ReLU activation applied! Negative values set to 0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Pooling (Max Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool2d(feature_map, pool_size=2, stride=2):\n",
    "    \"\"\"\n",
    "    Perform 2D max pooling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_map : numpy array\n",
    "        Input feature map\n",
    "    pool_size : int\n",
    "        Size of pooling window\n",
    "    stride : int\n",
    "        Step size for sliding the pooling window\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pooled : numpy array\n",
    "        Pooled output\n",
    "    \"\"\"\n",
    "    height, width = feature_map.shape\n",
    "    \n",
    "    # Calculate output dimensions\n",
    "    out_height = (height - pool_size) // stride + 1\n",
    "    out_width = (width - pool_size) // stride + 1\n",
    "    \n",
    "    pooled = np.zeros((out_height, out_width))\n",
    "    \n",
    "    for i in range(out_height):\n",
    "        for j in range(out_width):\n",
    "            # Extract region\n",
    "            region = feature_map[i*stride:i*stride+pool_size,\n",
    "                                j*stride:j*stride+pool_size]\n",
    "            # Take maximum\n",
    "            pooled[i, j] = np.max(region)\n",
    "    \n",
    "    return pooled\n",
    "\n",
    "# Apply max pooling\n",
    "pooled_map = max_pool2d(activated_map, pool_size=2, stride=2)\n",
    "\n",
    "# Visualize max pooling\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Before pooling\n",
    "axes[0].imshow(activated_map, cmap='YlGn', interpolation='nearest')\n",
    "axes[0].set_title('Before Pooling\\n(4√ó4 Activated Map)', fontsize=12, fontweight='bold')\n",
    "for (i, j), val in np.ndenumerate(activated_map):\n",
    "    axes[0].text(j, i, f'{val:.0f}', ha='center', va='center', \n",
    "                fontsize=10, fontweight='bold')\n",
    "\n",
    "# Draw pooling regions\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        rect = Rectangle((j*2-0.5, i*2-0.5), 2, 2, \n",
    "                        linewidth=3, edgecolor='red', facecolor='none')\n",
    "        axes[0].add_patch(rect)\n",
    "axes[0].grid(True, which='both', color='gray', linewidth=0.5)\n",
    "\n",
    "# Pooling operation visualization\n",
    "axes[1].text(0.5, 0.7, 'Max Pooling Operation', ha='center', va='center',\n",
    "            fontsize=14, fontweight='bold', transform=axes[1].transAxes)\n",
    "axes[1].text(0.5, 0.55, 'Pool Size: 2√ó2', ha='center', va='center',\n",
    "            fontsize=12, transform=axes[1].transAxes)\n",
    "axes[1].text(0.5, 0.45, 'Stride: 2', ha='center', va='center',\n",
    "            fontsize=12, transform=axes[1].transAxes)\n",
    "axes[1].text(0.5, 0.3, 'Selects maximum value\\nfrom each 2√ó2 region', \n",
    "            ha='center', va='center', fontsize=11,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8),\n",
    "            transform=axes[1].transAxes)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# After pooling\n",
    "im = axes[2].imshow(pooled_map, cmap='YlGn', interpolation='nearest')\n",
    "axes[2].set_title('After Pooling\\n(2√ó2 Pooled Map)', fontsize=12, fontweight='bold')\n",
    "for (i, j), val in np.ndenumerate(pooled_map):\n",
    "    axes[2].text(j, i, f'{val:.0f}', ha='center', va='center', \n",
    "                fontsize=12, fontweight='bold')\n",
    "axes[2].grid(True, which='both', color='gray', linewidth=0.5)\n",
    "plt.colorbar(im, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Activated Map (4√ó4):\")\n",
    "print(activated_map)\n",
    "print(\"\\nPooled Map (2√ó2):\")\n",
    "print(pooled_map)\n",
    "print(\"\\n‚úÖ Max pooling complete! Dimensionality reduced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Flatten and Feed to Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the pooled map\n",
    "flattened_input = pooled_map.flatten()\n",
    "\n",
    "print(\"Pooled Map (2D):\")\n",
    "print(pooled_map)\n",
    "print(f\"\\nFlattened Input (1D): {flattened_input}\")\n",
    "print(f\"Number of inputs to neural network: {len(flattened_input)}\")\n",
    "\n",
    "# Visualize the flattening process\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# 2D pooled map\n",
    "axes[0].imshow(pooled_map, cmap='YlGn', interpolation='nearest')\n",
    "axes[0].set_title('Pooled Map (2√ó2)', fontsize=12, fontweight='bold')\n",
    "for (i, j), val in np.ndenumerate(pooled_map):\n",
    "    axes[0].text(j, i, f'{val:.0f}', ha='center', va='center', \n",
    "                fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, which='both', color='gray', linewidth=0.5)\n",
    "\n",
    "# 1D flattened\n",
    "axes[1].barh(range(len(flattened_input)), flattened_input, color='green', alpha=0.7)\n",
    "axes[1].set_yticks(range(len(flattened_input)))\n",
    "axes[1].set_yticklabels([f'Input {i+1}' for i in range(len(flattened_input))])\n",
    "axes[1].set_xlabel('Value', fontsize=11)\n",
    "axes[1].set_title('Flattened Input Vector', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "for i, v in enumerate(flattened_input):\n",
    "    axes[1].text(v + 0.1, i, f'{v:.0f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Flattening complete! Ready for neural network.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete CNN Pipeline Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cnn_pipeline(image, filter_kernel, bias=-2, title=\"CNN Pipeline\"):\n",
    "    \"\"\"\n",
    "    Visualize the complete CNN pipeline\n",
    "    \"\"\"\n",
    "    # Step 1: Convolution\n",
    "    feature_map = convolve2d(image, filter_kernel, bias=bias)\n",
    "    \n",
    "    # Step 2: ReLU\n",
    "    activated = relu(feature_map)\n",
    "    \n",
    "    # Step 3: Max Pooling\n",
    "    pooled = max_pool2d(activated, pool_size=2, stride=2)\n",
    "    \n",
    "    # Step 4: Flatten\n",
    "    flattened = pooled.flatten()\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = plt.figure(figsize=(18, 4))\n",
    "    gs = fig.add_gridspec(1, 6, width_ratios=[1, 1, 1, 1, 1, 1.2])\n",
    "    \n",
    "    # Input\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    ax1.imshow(image, cmap='gray_r', interpolation='nearest')\n",
    "    ax1.set_title('1. Input\\n(6√ó6)', fontsize=10, fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Filter\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    ax2.imshow(filter_kernel, cmap='RdBu_r', interpolation='nearest')\n",
    "    ax2.set_title('2. Filter\\n(3√ó3)', fontsize=10, fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Feature Map\n",
    "    ax3 = fig.add_subplot(gs[2])\n",
    "    ax3.imshow(feature_map, cmap='RdYlGn', interpolation='nearest')\n",
    "    ax3.set_title('3. Convolution\\n(4√ó4)', fontsize=10, fontweight='bold')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    # Activated\n",
    "    ax4 = fig.add_subplot(gs[3])\n",
    "    ax4.imshow(activated, cmap='YlGn', interpolation='nearest')\n",
    "    ax4.set_title('4. ReLU\\n(4√ó4)', fontsize=10, fontweight='bold')\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    # Pooled\n",
    "    ax5 = fig.add_subplot(gs[4])\n",
    "    ax5.imshow(pooled, cmap='YlGn', interpolation='nearest')\n",
    "    ax5.set_title('5. Max Pool\\n(2√ó2)', fontsize=10, fontweight='bold')\n",
    "    for (i, j), val in np.ndenumerate(pooled):\n",
    "        ax5.text(j, i, f'{val:.0f}', ha='center', va='center', \n",
    "                fontsize=10, fontweight='bold')\n",
    "    ax5.axis('off')\n",
    "    \n",
    "    # Flattened\n",
    "    ax6 = fig.add_subplot(gs[5])\n",
    "    ax6.barh(range(len(flattened)), flattened, color='green', alpha=0.7)\n",
    "    ax6.set_yticks(range(len(flattened)))\n",
    "    ax6.set_yticklabels([f'{i+1}' for i in range(len(flattened))], fontsize=8)\n",
    "    ax6.set_title('6. Flatten\\n(4 inputs)', fontsize=10, fontweight='bold')\n",
    "    ax6.set_xlabel('Value', fontsize=9)\n",
    "    for i, v in enumerate(flattened):\n",
    "        ax6.text(v + 0.1, i, f'{v:.0f}', va='center', fontsize=8, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return flattened\n",
    "\n",
    "# Visualize for Letter O\n",
    "print(\"Complete CNN Pipeline for Letter O:\")\n",
    "output_o = visualize_cnn_pipeline(letter_o, filter_kernel, bias=-2, \n",
    "                                  title=\"CNN Pipeline: Letter O ‚Üí Neural Network Input\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize for Letter X\n",
    "print(\"\\nComplete CNN Pipeline for Letter X:\")\n",
    "output_x = visualize_cnn_pipeline(letter_x, filter_kernel, bias=-2,\n",
    "                                  title=\"CNN Pipeline: Letter X ‚Üí Neural Network Input\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Complete CNN pipeline visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building CNNs from Scratch <a id='from-scratch'></a>\n",
    "\n",
    "Now let's build a complete CNN from scratch to classify O's and X's!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    \"\"\"\n",
    "    A simple CNN from scratch for binary classification (O vs X)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape=(6, 6), filter_size=3, num_hidden=4):\n",
    "        \"\"\"\n",
    "        Initialize CNN with random weights\n",
    "        \"\"\"\n",
    "        # CNN parameters\n",
    "        self.filter = np.random.randn(filter_size, filter_size) * 0.1\n",
    "        self.bias_conv = np.random.randn() * 0.1\n",
    "        \n",
    "        # Calculate sizes after conv and pooling\n",
    "        conv_size = input_shape[0] - filter_size + 1\n",
    "        pool_size = conv_size // 2\n",
    "        self.flattened_size = pool_size * pool_size\n",
    "        \n",
    "        # Neural network parameters\n",
    "        self.w_hidden = np.random.randn(self.flattened_size, num_hidden) * 0.1\n",
    "        self.b_hidden = np.random.randn(num_hidden) * 0.1\n",
    "        \n",
    "        self.w_output = np.random.randn(num_hidden, 2) * 0.1  # 2 outputs: O and X\n",
    "        self.b_output = np.random.randn(2) * 0.1\n",
    "        \n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        \"\"\"\n",
    "        # 1. Convolution\n",
    "        self.feature_map = convolve2d(image, self.filter, bias=self.bias_conv)\n",
    "        \n",
    "        # 2. ReLU activation\n",
    "        self.activated = relu(self.feature_map)\n",
    "        \n",
    "        # 3. Max pooling\n",
    "        self.pooled = max_pool2d(self.activated, pool_size=2, stride=2)\n",
    "        \n",
    "        # 4. Flatten\n",
    "        self.flattened = self.pooled.flatten()\n",
    "        \n",
    "        # 5. Hidden layer\n",
    "        self.hidden_input = np.dot(self.flattened, self.w_hidden) + self.b_hidden\n",
    "        self.hidden_output = relu(self.hidden_input)\n",
    "        \n",
    "        # 6. Output layer\n",
    "        self.output_input = np.dot(self.hidden_output, self.w_output) + self.b_output\n",
    "        \n",
    "        # 7. Softmax for probabilities\n",
    "        exp_scores = np.exp(self.output_input - np.max(self.output_input))\n",
    "        self.output = exp_scores / np.sum(exp_scores)\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def predict(self, image):\n",
    "        \"\"\"\n",
    "        Predict class for an image\n",
    "        \"\"\"\n",
    "        output = self.forward(image)\n",
    "        return np.argmax(output), output\n",
    "\n",
    "# Create and test the CNN\n",
    "cnn = SimpleCNN()\n",
    "\n",
    "print(\"CNN Architecture:\")\n",
    "print(f\"  Input: 6√ó6 image\")\n",
    "print(f\"  Filter: 3√ó3 ({self.filter.size} parameters)\")\n",
    "print(f\"  After convolution: 4√ó4 feature map\")\n",
    "print(f\"  After ReLU: 4√ó4 activated map\")\n",
    "print(f\"  After pooling: 2√ó2 pooled map\")\n",
    "print(f\"  Flattened: {cnn.flattened_size} inputs\")\n",
    "print(f\"  Hidden layer: 4 neurons\")\n",
    "print(f\"  Output: 2 classes (O, X)\")\n",
    "\n",
    "# Test predictions (before training)\n",
    "print(\"\\n--- Predictions Before Training ---\")\n",
    "pred_o, prob_o = cnn.predict(letter_o)\n",
    "print(f\"Letter O: Predicted class = {pred_o} ({'O' if pred_o == 0 else 'X'})\")\n",
    "print(f\"  Probabilities: O={prob_o[0]:.3f}, X={prob_o[1]:.3f}\")\n",
    "\n",
    "pred_x, prob_x = cnn.predict(letter_x)\n",
    "print(f\"Letter X: Predicted class = {pred_x} ({'O' if pred_x == 0 else 'X'})\")\n",
    "print(f\"  Probabilities: O={prob_x[0]:.3f}, X={prob_x[1]:.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ CNN created and tested (untrained).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN from Scratch\n",
    "\n",
    "Now let's implement backpropagation to train our CNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableCNN(SimpleCNN):\n",
    "    \"\"\"\n",
    "    Trainable CNN with backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape=(6, 6), filter_size=3, num_hidden=4, learning_rate=0.01):\n",
    "        super().__init__(input_shape, filter_size, num_hidden)\n",
    "        self.lr = learning_rate\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def train_step(self, image, target):\n",
    "        \"\"\"\n",
    "        One training step with backpropagation\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        output = self.forward(image)\n",
    "        \n",
    "        # Calculate loss (cross-entropy)\n",
    "        loss = -np.log(output[target] + 1e-8)\n",
    "        \n",
    "        # Backward pass (simplified - only updating output and hidden weights)\n",
    "        # Gradient of loss w.r.t output\n",
    "        d_output = output.copy()\n",
    "        d_output[target] -= 1\n",
    "        \n",
    "        # Gradients for output layer\n",
    "        d_w_output = np.outer(self.hidden_output, d_output)\n",
    "        d_b_output = d_output\n",
    "        \n",
    "        # Gradients for hidden layer\n",
    "        d_hidden = np.dot(d_output, self.w_output.T)\n",
    "        d_hidden[self.hidden_input <= 0] = 0  # ReLU gradient\n",
    "        \n",
    "        d_w_hidden = np.outer(self.flattened, d_hidden)\n",
    "        d_b_hidden = d_hidden\n",
    "        \n",
    "        # Update weights\n",
    "        self.w_output -= self.lr * d_w_output\n",
    "        self.b_output -= self.lr * d_b_output\n",
    "        self.w_hidden -= self.lr * d_w_hidden\n",
    "        self.b_hidden -= self.lr * d_b_hidden\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train(self, images, targets, epochs=1000, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the CNN\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for image, target in zip(images, targets):\n",
    "                loss = self.train_step(image, target)\n",
    "                total_loss += loss\n",
    "            \n",
    "            avg_loss = total_loss / len(images)\n",
    "            self.loss_history.append(avg_loss)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Create training data (with some variations)\n",
    "def create_shifted_o(shift_right=0, shift_down=0):\n",
    "    \"\"\"Create a shifted version of O\"\"\"\n",
    "    img = np.zeros((6, 6))\n",
    "    base_o = create_letter_o()\n",
    "    # Simple shift (keeping it within bounds)\n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            new_i = min(5, max(0, i + shift_down))\n",
    "            new_j = min(5, max(0, j + shift_right))\n",
    "            if 0 <= i-shift_down < 6 and 0 <= j-shift_right < 6:\n",
    "                img[new_i, new_j] = base_o[i-shift_down, j-shift_right]\n",
    "    return img\n",
    "\n",
    "def create_shifted_x(shift_right=0, shift_down=0):\n",
    "    \"\"\"Create a shifted version of X\"\"\"\n",
    "    img = np.zeros((6, 6))\n",
    "    base_x = create_letter_x()\n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            new_i = min(5, max(0, i + shift_down))\n",
    "            new_j = min(5, max(0, j + shift_right))\n",
    "            if 0 <= i-shift_down < 6 and 0 <= j-shift_right < 6:\n",
    "                img[new_i, new_j] = base_x[i-shift_down, j-shift_right]\n",
    "    return img\n",
    "\n",
    "# Create training set\n",
    "train_images = [\n",
    "    create_letter_o(),\n",
    "    create_shifted_o(shift_right=1),\n",
    "    create_letter_x(),\n",
    "    create_shifted_x(shift_right=1),\n",
    "]\n",
    "\n",
    "train_targets = [0, 0, 1, 1]  # 0 for O, 1 for X\n",
    "\n",
    "# Train the CNN\n",
    "print(\"Training CNN from scratch...\\n\")\n",
    "cnn_trained = TrainableCNN(learning_rate=0.1)\n",
    "cnn_trained.train(train_images, train_targets, epochs=500, verbose=True)\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(cnn_trained.loss_history, linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test the trained CNN\n",
    "print(\"\\n--- Predictions After Training ---\")\n",
    "test_images = [\n",
    "    (create_letter_o(), \"Letter O\"),\n",
    "    (create_letter_x(), \"Letter X\"),\n",
    "    (create_shifted_x(shift_right=1), \"Letter X (shifted)\"),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (image, label) in enumerate(test_images):\n",
    "    pred, prob = cnn_trained.predict(image)\n",
    "    \n",
    "    axes[idx].imshow(image, cmap='gray_r', interpolation='nearest')\n",
    "    axes[idx].set_title(f\"{label}\\nPredicted: {'O' if pred == 0 else 'X'}\\nProb: O={prob[0]:.3f}, X={prob[1]:.3f}\",\n",
    "                       fontsize=11, fontweight='bold')\n",
    "    axes[idx].grid(True, which='both', color='blue', linewidth=0.5, alpha=0.3)\n",
    "    axes[idx].tick_params(labelbottom=False, labelleft=False)\n",
    "    \n",
    "    print(f\"{label}: Predicted class = {pred} ({'O' if pred == 0 else 'X'})\")\n",
    "    print(f\"  Probabilities: O={prob[0]:.3f}, X={prob[1]:.3f}\\n\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ CNN successfully trained and tested from scratch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-World Example: MNIST Digit Classification <a id='mnist-example'></a>\n",
    "\n",
    "Now let's apply CNNs to a real-world problem: recognizing handwritten digits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "print(\"Loading MNIST dataset...\")\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape for CNN (add channel dimension)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "print(f\"Training set: {x_train.shape}\")\n",
    "print(f\"Test set: {x_test.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
    "\n",
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(x_train[i].squeeze(), cmap='gray')\n",
    "    ax.set_title(f'Label: {y_train[i]}', fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample MNIST Digits', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ MNIST dataset loaded and visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Building CNNs with Keras/TensorFlow <a id='with-keras'></a>\n",
    "\n",
    "Now let's build a proper CNN using Keras!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    \"\"\"\n",
    "    Create a CNN model for MNIST classification\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # First convolutional block\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', name='conv2'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "        \n",
    "        # Third convolutional block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', name='conv3'),\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        layers.Flatten(name='flatten'),\n",
    "        layers.Dense(64, activation='relu', name='dense1'),\n",
    "        layers.Dropout(0.5, name='dropout'),\n",
    "        layers.Dense(10, activation='softmax', name='output')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_cnn_model()\n",
    "\n",
    "# Display model architecture\n",
    "print(\"CNN Model Architecture:\")\n",
    "print(\"=\"*70)\n",
    "model.summary()\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ CNN model created and compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training CNN on MNIST...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test samples\n",
    "predictions = model.predict(x_test[:20], verbose=0)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(x_test[i].squeeze(), cmap='gray')\n",
    "    \n",
    "    true_label = y_test[i]\n",
    "    pred_label = predicted_classes[i]\n",
    "    confidence = predictions[i][pred_label] * 100\n",
    "    \n",
    "    color = 'green' if true_label == pred_label else 'red'\n",
    "    ax.set_title(f'True: {true_label}, Pred: {pred_label}\\nConf: {confidence:.1f}%',\n",
    "                fontsize=10, fontweight='bold', color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('CNN Predictions on Test Set', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Predictions visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing CNN Features <a id='visualization'></a>\n",
    "\n",
    "Let's peek inside the CNN to see what it's learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that outputs intermediate layer activations\n",
    "layer_outputs = [layer.output for layer in model.layers[:6]]  # First 6 layers\n",
    "activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "# Get activations for a sample image\n",
    "sample_image = x_test[0:1]  # First test image\n",
    "activations = activation_model.predict(sample_image, verbose=0)\n",
    "\n",
    "# Visualize the original image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(sample_image.squeeze(), cmap='gray')\n",
    "plt.title(f'Input Image (Label: {y_test[0]})', fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Visualizing features for digit: {y_test[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first convolutional layer filters\n",
    "layer_names = ['conv1', 'pool1', 'conv2', 'pool2', 'conv3']\n",
    "\n",
    "for layer_name, activation in zip(layer_names, activations):\n",
    "    if 'conv' in layer_name:\n",
    "        n_features = min(16, activation.shape[-1])  # Show up to 16 feature maps\n",
    "        size = activation.shape[1]\n",
    "        \n",
    "        fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "        \n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            if i < n_features:\n",
    "                ax.imshow(activation[0, :, :, i], cmap='viridis')\n",
    "                ax.set_title(f'Filter {i+1}', fontsize=10, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.suptitle(f'{layer_name.upper()} Feature Maps ({size}√ó{size})', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "print(\"\\n‚úÖ Feature maps visualized!\")\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Early layers detect edges and simple patterns\")\n",
    "print(\"- Deeper layers detect more complex features\")\n",
    "print(\"- Each filter specializes in different aspects of the image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned filters from the first convolutional layer\n",
    "first_layer = model.layers[0]\n",
    "filters, biases = first_layer.get_weights()\n",
    "\n",
    "# Normalize filters for visualization\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters_normalized = (filters - f_min) / (f_max - f_min)\n",
    "\n",
    "# Plot the first 16 filters\n",
    "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < 16:\n",
    "        # Get the filter\n",
    "        filt = filters_normalized[:, :, 0, i]\n",
    "        ax.imshow(filt, cmap='gray')\n",
    "        ax.set_title(f'Filter {i+1}', fontsize=10, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Learned Filters in First Convolutional Layer (3√ó3)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Learned filters visualized!\")\n",
    "print(\"These filters are automatically learned during training to detect useful features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Best Practices <a id='summary'></a>\n",
    "\n",
    "### Key Takeaways üéØ\n",
    "\n",
    "**Why CNNs?**\n",
    "1. **Reduce parameters** through weight sharing (same filter applied everywhere)\n",
    "2. **Handle shifts** through convolution and pooling\n",
    "3. **Exploit spatial relationships** by looking at local neighborhoods\n",
    "\n",
    "**CNN Architecture**\n",
    "1. **Convolution**: Apply filters to detect features\n",
    "2. **Activation (ReLU)**: Introduce non-linearity\n",
    "3. **Pooling**: Reduce dimensionality and improve robustness\n",
    "4. **Flatten**: Convert to 1D for dense layers\n",
    "5. **Dense layers**: Final classification\n",
    "\n",
    "**Best Practices**\n",
    "- Start with small filters (3√ó3 is common)\n",
    "- Use multiple convolutional layers to learn hierarchical features\n",
    "- Apply pooling to reduce computational cost\n",
    "- Use data augmentation for better generalization\n",
    "- Monitor both training and validation metrics\n",
    "- Use dropout to prevent overfitting\n",
    "\n",
    "**Common Applications**\n",
    "- Image classification\n",
    "- Object detection\n",
    "- Face recognition\n",
    "- Medical image analysis\n",
    "- Self-driving cars\n",
    "- Video analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison: Regular NN vs CNN\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARISON: Regular Neural Network vs CNN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nRegular Neural Network for 28√ó28 images:\")\n",
    "print(\"  - Input: 784 nodes (28√ó28 flattened)\")\n",
    "print(\"  - Hidden layer with 128 nodes: 784 √ó 128 = 100,352 weights\")\n",
    "print(\"  - Total: ~100,000+ parameters\")\n",
    "print(\"  - Problems: Too many parameters, no spatial awareness, not shift-invariant\")\n",
    "\n",
    "print(\"\\nConvolutional Neural Network:\")\n",
    "print(\"  - Conv1: 32 filters (3√ó3) = 32 √ó 9 = 288 weights\")\n",
    "print(\"  - Conv2: 64 filters (3√ó3) √ó 32 = 18,432 weights\")\n",
    "print(\"  - Conv3: 64 filters (3√ó3) √ó 64 = 36,864 weights\")\n",
    "print(f\"  - Total: {model.count_params():,} parameters\")\n",
    "print(\"  - Benefits: Fewer parameters, spatial awareness, shift-invariant!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ CNN is much more efficient and effective for images!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! üéâ You've completed a comprehensive tutorial on Convolutional Neural Networks!\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Why CNNs are better than regular neural networks for images\n",
    "- ‚úÖ The three core operations: Convolution, Activation, and Pooling\n",
    "- ‚úÖ How to build CNNs from scratch with NumPy\n",
    "- ‚úÖ How to build CNNs with Keras/TensorFlow\n",
    "- ‚úÖ How to visualize what CNNs learn\n",
    "- ‚úÖ How to apply CNNs to real-world problems (MNIST)\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with different architectures\n",
    "- Try transfer learning with pre-trained models (VGG, ResNet, etc.)\n",
    "- Apply CNNs to your own image datasets\n",
    "- Explore advanced topics: batch normalization, residual connections, attention mechanisms\n",
    "\n",
    "**BAM!** üí• You're now ready to build powerful image classification systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
